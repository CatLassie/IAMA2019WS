{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this exercise in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says \"YOUR ANSWER HERE\" or `YOUR CODE HERE` and remove the `raise NotImplementedError()` lines. Please add your name and student ID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Peter Rjabcsenko\"\n",
    "STUDENT_ID = \"1228563\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yP7m75WmHFh"
   },
   "source": [
    "# Assignment 2 - Automatic Drum Transcription\n",
    "## Intelligent Audio and Music Analysis\n",
    "### WS 2019\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Welcome to the second assignment. \n",
    "In this exercise you will build four different algorithms to extract drum note onsets from audio.\n",
    "\n",
    "## 0.1. Naming notebooks\n",
    "\n",
    "Set your name and matriculation number in the title of the notebook above, replacing the placeholders. The title should be then e.g. \"Assignment 2 - 12345678 Hans Meiser\".\n",
    "\n",
    "## 0.2. Download and extract the dataset\n",
    "Again, you will need a dataset consisting of audio files and annotations for this exercise. Download the .zip file from TUWEL and unpack it into the folder where you run this jupyter notebook from. The folder should have the \"data\" folder as a subfolder. Alternatively, set the DATA_PATH constant accordingly. Please make sure that you do not use backward slashes if working on Windows. Either use normal forward slashes in path names or use the functions provided by the os.path module.\n",
    "\n",
    "If you use Google Colab, upload the data to the Google Colab folder in Google Drive.\n",
    "\n",
    "## 0.3. Other requirements\n",
    "Make sure your python(3) installation contains the following packages:\n",
    "* cython\n",
    "* numpy\n",
    "* scipy\n",
    "* sklearn\n",
    "* torch (PyTorch)\n",
    "* madmom\n",
    "\n",
    "In Google Colab you can use the following code block to install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ixdPxBM8LPc"
   },
   "outputs": [],
   "source": [
    "! pip uninstall madmom magenta mido\n",
    "! pip install mido numpy scipy madmom torch matplotlib sklearn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXd6fg_-8kCy"
   },
   "source": [
    "## 0.4. Fill in the missing code blocks\n",
    "Read and work through the whole notebook from top to bottom and fill in missing code marked with TODO comments.\n",
    "The comments along the code provide many hints, so it is recommended to also read through the provided code pieces, since they will help solve the exercise.\n",
    "\n",
    "Execute *ALL* code cells below from top to bottom after each other, by either using `Ctrl-Enter`, `Shift-Enter`, or the GUI buttons above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsNqXfl1mHFi"
   },
   "source": [
    "# 1. Dependencies, imports, and global variables\n",
    "First we will import all required libraries and define globally used parameters for spectrogram calculation and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d67a3FaNmHFj"
   },
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# filters, might be useful for separate and detect\n",
    "from scipy.signal import butter, freqz\n",
    "from scipy.ndimage.filters import maximum_filter, uniform_filter\n",
    "\n",
    "# classifier for segment and classify method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.audio.spectrogram import LogarithmicFilteredSpectrogram\n",
    "from madmom.audio import Signal\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "from madmom.evaluation import OnsetEvaluation, OnsetSumEvaluation\n",
    "from madmom.features import CNNOnsetProcessor\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "# plotting library for visualization for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'pgf.rcfonts': False})\n",
    "\n",
    "COLAB_DRIVE_BASE = \"/content/g-drive\"\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if in colab, mount gdrive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  print('trying to mount google drive...')\n",
    "  drive.mount(COLAB_DRIVE_BASE, force_remount=True)\n",
    "\n",
    "#\n",
    "# some global parameter settings we will need along the way\n",
    "#\n",
    "EPSILON = np.finfo(np.float32).eps  # small epsilon needed sometimes for computational stability (div by zeros)\n",
    "\n",
    "SETTINGS = {  # settings for spectrogram (feature) calculation\n",
    "    'fps': 100,  # frames per second of our resulting spectrograms\n",
    "    'fmin': 30,  # minimum frequency\n",
    "    'fmax': 15000,  # maximum frequency of spectrogram\n",
    "    'frame_size': 2048,  # frame size for spectrogram\n",
    "    'sample_rate': 44100,  # input sample rate - input audio will be resampled to this sample rate.\n",
    "    'num_bands': 12,  # bands per octave (freq. factor 2)\n",
    "    'num_channels': 1,  # input audio will be converted to mono\n",
    "    'norm_filters': True,  # normalize triangular filters for log/log spectrogram to have equal area\n",
    "}\n",
    "\n",
    "# drum label names\n",
    "# all arrays and lists containing instruments will always follow this index system, 0:KD (kick/bass drum),\n",
    "# 1:SD (snare drum), 2: HH (hi-hat).\n",
    "names_3_map = ['KD', 'SD', 'HH']\n",
    "num_3_drum_notes = len(names_3_map)\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "  PATH = os.path.join(COLAB_DRIVE_BASE, 'My Drive/Colab Notebooks')\n",
    "\n",
    "DATA_PATH = os.path.join(PATH, 'data/drums_simple')  # change this value if you copied the dataset somewhere else!\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'annotations')\n",
    "SAMPLE_ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'sample_annotations')\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, 'samples')\n",
    "CACHE_PATH = os.path.join(DATA_PATH, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "MODEL_PATH = os.path.join(DATA_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\n",
    "# some info about our data\n",
    "NUM_KITS = 4  # we have 4 different drum kits\n",
    "NUM_TRACKS = 4  # and 4 tracks per kit\n",
    "FPS = SETTINGS['fps']  # shorthand to the FPS we use for our spectrogram\n",
    "RANK = num_3_drum_notes  # we use three instruments\n",
    "\n",
    "# turn on / off plotting (for debugging)\n",
    "plot = False\n",
    "plot_len = 400\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 12345\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q26tbpN0mHFm"
   },
   "source": [
    "# 2. Helper functions\n",
    "The next block defines some helper functions for audio file handling, spectrogram calculation, and annotation processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ko4xjkHDmHFm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# some helper functions to handle data from our example dataset\n",
    "#\n",
    "def step_diff(array, step):\n",
    "    \"\"\"\n",
    "    Calculates a 1st order difference between rows step values .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.array\n",
    "        Input array to calculate the 1st order difference.\n",
    "\n",
    "    step : int\n",
    "        Number of steps for offset for difference calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    difference : np.array\n",
    "        Array containing the 1st order difference. Note that the number of rows will be steps less than the input's.\n",
    "    \"\"\"\n",
    "    a = array[step:]\n",
    "    b = array[:-step]\n",
    "    return b-a\n",
    "\n",
    "\n",
    "def load_audio(audio_file_list):\n",
    "    \"\"\"\n",
    "    Load audio from the given files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_file_list : list\n",
    "        List with audio filenames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    audio_list : list\n",
    "        List containing the actual audio.\n",
    "\n",
    "    \"\"\"\n",
    "    audio_list = []\n",
    "    for audio_file in audio_file_list:\n",
    "        signal = Signal(audio_file, **SETTINGS)\n",
    "        audio_list.append(signal)\n",
    "    return audio_list\n",
    "\n",
    "\n",
    "def compute_feature(file, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute (spectrogram) feature for the given audio file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Audio file name.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments used for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature : numpy array\n",
    "        Computed feature\n",
    "\n",
    "    \"\"\"\n",
    "    # create (filtered) spectrogram\n",
    "    return LogarithmicFilteredSpectrogram(file, **kwargs)\n",
    "\n",
    "\n",
    "def create_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "            print('successfully loaded cached file:', cache_file)\n",
    "        else:\n",
    "            feat = compute_feature(audio_file, **kwargs)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "                print('successfully stored cache for file:', audio_file)\n",
    "        feature_list.append(feat)\n",
    "    return feature_list\n",
    "\n",
    "\n",
    "def load_annotations(files):\n",
    "    \"\"\"\n",
    "    Load annotations from files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with annotation filenames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    annotation_list : list\n",
    "        List with annotations.\n",
    "\n",
    "    \"\"\"\n",
    "    annotation_list = []\n",
    "    for annotation_file in files:\n",
    "        annotation = madmom.io.load_notes(annotation_file)\n",
    "        annotation_list.append(annotation)\n",
    "    return annotation_list\n",
    "\n",
    "\n",
    "def compute_target_array_from_times(times, fps, num_frames, num_targets):\n",
    "    \"\"\"\n",
    "    creates a numpy array with targets for neural network training\n",
    "    :param times: list\n",
    "    list of annotations for which the target should be 1. times in seconds.\n",
    "    :param fps:\n",
    "    sampling frequency of target array\n",
    "    :param num_frames:\n",
    "    total number of frames (all entries in times must fit into the total number of frames).\n",
    "    :param num_targets:\n",
    "    total number of targets (all entries in times must fit into the total number of labels).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(times) > 0 and np.max(times, 0)[0] * fps > num_frames:\n",
    "        print(\"Maximum time is larger than number of samples - cutting times.\")\n",
    "    if len(times) > 0 and np.max(times, 0)[1] >= num_targets:\n",
    "        print(\"Maximum label index is larger than num_targets - cutting labels.\")\n",
    "\n",
    "    new_targets = np.zeros((num_frames, num_targets))\n",
    "    for entry_nr, time_entry in enumerate(times):\n",
    "        cur_time = time_entry[0]\n",
    "        time_idx = int(cur_time*fps)\n",
    "        inst_idx = int(time_entry[1])\n",
    "        if 0 <= inst_idx < num_targets:\n",
    "            if time_idx < num_frames:\n",
    "                new_targets[time_idx, inst_idx] = 1\n",
    "\n",
    "    return new_targets\n",
    "\n",
    "\n",
    "def create_targets(annotation_list, feature_list, fps=FPS, num_classes=3):\n",
    "    \"\"\"\n",
    "    Create targets for the given annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_list : list\n",
    "        List with annotations\n",
    "    feature_list : list\n",
    "        List with features (needed to determine length)\n",
    "    fps : float\n",
    "        Frames per second\n",
    "    num_classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    target_list : list\n",
    "        List with targets for NN training.\n",
    "\n",
    "    \"\"\"\n",
    "    target_list = []\n",
    "    for annotation, feature in zip(annotation_list, feature_list):\n",
    "        target = compute_target_array_from_times(annotation, fps, len(feature), num_classes)\n",
    "        target_list.append(target)\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def plot_peak_picking(onset_function, pre_avg = 0.05, post_avg = 0.05, pre_max = 0.02, post_max = 0.02, combine = 0.02,\n",
    "                      thresh = 0.2, smooth = 0.0, plot_frames=1000):\n",
    "    \"\"\"\n",
    "    helper function which visualizes the peak picking parameters, use to adapt peak picking settings if necessary.\n",
    "    :param onset_function:\n",
    "    :param pre_avg:\n",
    "    :param post_avg:\n",
    "    :param pre_max:\n",
    "    :param post_max:\n",
    "    :param combine:\n",
    "    :param thresh:\n",
    "    :param smooth:\n",
    "    :param plot_frames:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # plot example to investigate peak picking\n",
    "    peak_picker = OnsetPeakPickingProcessor(threshold=thresh, smooth=smooth, pre_avg=pre_avg,\n",
    "                                               post_avg=post_avg, pre_max=pre_max, post_max=post_max,\n",
    "                                               combine=combine, fps=FPS)\n",
    "    inst_det = [peak_picker.process(onset_function[:, inst]) for inst in range(3)]\n",
    "\n",
    "    for inst in range(3):\n",
    "        plt.subplot(3, 1, inst + 1)\n",
    "        activations = onset_function[:plot_frames, inst]\n",
    "\n",
    "        avg_length = (pre_avg + post_avg) * FPS + 1\n",
    "        avg_origin = int(np.floor((pre_avg - post_avg) * FPS / 2))\n",
    "        avg_filter_size = avg_length\n",
    "        max_length = (pre_max + post_max) * FPS + 1\n",
    "        max_filter_size = max_length\n",
    "        max_origin = int(np.floor((pre_max - post_max) * FPS / 2))\n",
    "\n",
    "        mov_avg = uniform_filter(activations, avg_filter_size, mode='constant', origin=avg_origin)\n",
    "        mov_max = maximum_filter(activations, max_filter_size, mode='constant', origin=max_origin)\n",
    "\n",
    "        select = inst_det[inst] * FPS < plot_frames\n",
    "        peaks = inst_det[inst][select] * FPS\n",
    "        plt.plot(activations)\n",
    "        plt.plot(peaks, onset_function[np.asarray(peaks, dtype=int), inst], 'ro')\n",
    "        plt.plot(mov_avg, 'g')\n",
    "        plt.plot(mov_max, 'y')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_activation_functions(spectrogram, activation_functions, templates=None):\n",
    "    \"\"\"\n",
    "    helper function that visualizes spectrogram alongside detected activation functions.\n",
    "    use to debug your methods.\n",
    "    :param spectrogram:\n",
    "    :param activation_functions:\n",
    "    :param templates:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if templates is None:\n",
    "        num_plots = 2\n",
    "    else:\n",
    "        num_plots = 3\n",
    "    plt.figure()\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    if templates is not None:\n",
    "        plt.subplot(num_plots, 1, 2)\n",
    "        plt.imshow(templates, aspect='auto', origin='lower')\n",
    "    plt.subplot(num_plots, 1, num_plots)\n",
    "    plt.plot(activation_functions[:, 0])\n",
    "    plt.plot(activation_functions[:, 1] + 1)\n",
    "    plt.plot(activation_functions[:, 2] + 2)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgnI9hwomHFo"
   },
   "source": [
    "# 3. Dataset\n",
    "The next code block will load all the data from the example dataset we will use for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzvaNXQ3mHFp"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# load our example dataset\n",
    "#\n",
    "\n",
    "# load audio and calculate features\n",
    "audio_files = search_files(AUDIO_PATH, '.wav')\n",
    "audio_files += search_files(AUDIO_PATH, '.flac')\n",
    "audio = load_audio(audio_files)\n",
    "features = create_features(audio_files, **SETTINGS)\n",
    "\n",
    "sample_files = search_files(SAMPLES_PATH, '.wav')\n",
    "sample_files += search_files(SAMPLES_PATH, '.flac')\n",
    "sample_audio = load_audio(sample_files)\n",
    "sample_features = create_features(sample_files, **SETTINGS)\n",
    "\n",
    "\n",
    "# load annotations and create targets\n",
    "annotation_files = search_files(ANNOTATIONS_PATH, '.txt')\n",
    "annotations = load_annotations(annotation_files)\n",
    "targets = create_targets(annotations, features)\n",
    "\n",
    "sample_annotation_files = search_files(SAMPLE_ANNOTATIONS_PATH, '.txt')\n",
    "sample_annotations = load_annotations(sample_annotation_files)\n",
    "sample_targets = create_targets(sample_annotations, features)\n",
    "sample_times = [[0, 8], [11, 19], [21, 29]]  # these are the times within which the onsets for each instrumt\n",
    "                                              # are in the audio (in spec seconds) detailed annotations exist too!\n",
    "\n",
    "fs = SETTINGS['sample_rate']\n",
    "test_audio = audio[:(NUM_TRACKS * NUM_KITS)]\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXQL9tm3mHFr"
   },
   "source": [
    "# 4. Separate and detect approach\n",
    "The next code block contains the first method which requiers some code to be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JY9FDhn-mHFs"
   },
   "outputs": [],
   "source": [
    "def separate_and_detect():\n",
    "    \"\"\"\n",
    "    this function runs the main loop over our dataset using a simple separate and detect approach\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To separate the individual drum instruments you can either use bandpass filters, see \n",
    "        https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "    You can look at the spectrogram to decide where you want to place the cutoff-frequencies.\n",
    "    For the kits used in our dataset, these frequencies might work ok: \n",
    "    low 0-100 Hz\n",
    "    high 10k - 22k Hz\n",
    "    mid  100-10k Hz\n",
    "        \n",
    "    or simply calculate a spectrogram and only use the relevant frequency bands, i.e. set the rest to 0 \n",
    "    (recommended, will usually work better). \n",
    "    The following frequency bands should work with our dataset: \n",
    "    low: bands 0-4\n",
    "    mid: bands 6-30\n",
    "    high: bands 50-end\n",
    "    \n",
    "    Note: this doesn't work too well, aim for around 50% f-measure; don't spend to much time on this approach.\n",
    "    \"\"\"\n",
    "\n",
    "    # peak picking settings, use these settings, only play around with them once you have a working system.\n",
    "    peak_picking_sep = OnsetPeakPickingProcessor(threshold=0.15, smooth=0.0, combine=0.04, delay=0.0, fps=100,\n",
    "                                                 pitch_offset=0, pre_max=0.02, post_max=0.02)\n",
    "\n",
    "    results_sep = [None for _ in range(len(test_audio))]\n",
    "    # iterate over tracks\n",
    "    for idx, data in enumerate(test_audio):\n",
    "        inst_eval = [None, None, None]\n",
    "        # for each instrument\n",
    "        for inst in range(3):\n",
    "            # get the spectrogram for the current file\n",
    "            filt_spec = np.copy(features[idx])\n",
    "\n",
    "            # fiter the signal for current instrument\n",
    "            # TODO\n",
    "\n",
    "            # calculate a simple onset detection function\n",
    "            # e.g. use the step_diff function to calculate the spectral diff, like discussed\n",
    "            # in the onset detection part (sum of abs. diffs)\n",
    "            # TODO\n",
    "\n",
    "            onset_activations = np.zeros((100, ))  # TODO replace with onset activations!\n",
    "\n",
    "            # peack picking and onset evaluation\n",
    "            filt_detections = peak_picking_sep.process(onset_activations)\n",
    "            inst_eval[inst] = OnsetEvaluation(filt_detections,\n",
    "                                              [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                              window=0.05, combine=0.025)\n",
    "\n",
    "        # evaluation over all isntruments for the current track\n",
    "        results_sep[idx] = OnsetSumEvaluation(inst_eval)\n",
    "\n",
    "    # evaluation over all tracks, ouput results\n",
    "    overall_results_sep = OnsetSumEvaluation(results_sep)\n",
    "    print('Separate and Detect f-measure: %f' % overall_results_sep.fmeasure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T713_h9LmHFu"
   },
   "source": [
    "## 4.1. Run method\n",
    "Once the TODO blocks are implemented, run the method in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGXyGhDfmHFu"
   },
   "outputs": [],
   "source": [
    "separate_and_detect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltMrygxbmHFw"
   },
   "source": [
    "# 5. Segment and classify approach\n",
    "In the next code block, implement the missing code marked with \"TODO\" to create a simple segment and classify approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agEOC0z4mHFx"
   },
   "outputs": [],
   "source": [
    "def segment_and_classify():\n",
    "    \"\"\"\n",
    "    this function runs the main loop over our dataset using a simple segment and classify approach\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # train the classifier\n",
    "    #\n",
    "    # we first have to train a classifier which is able to classify the different instruments and also\n",
    "    # combined onsets of them. Since we only have single instrument onsets as sample, we first build a dictionary\n",
    "    # of instrument hit combinations.\n",
    "    # We will then use a simple KNN-classifier; you can experiment with other classifiers after the KNN version \n",
    "    # works, if you want (e.g. try SVMs from sklearn)\n",
    "\n",
    "    # the hits are separated by about one second:\n",
    "    hits_len = int(1*fs)\n",
    "\n",
    "    # list of our features and labels for the classes used to train our classifier\n",
    "    knn_feats = []\n",
    "    knn_labels = []\n",
    "\n",
    "    # onset combinations we want to use. 1..use instrument in combo 0..don't use\n",
    "    # indices are: KD, SD, HH\n",
    "    combos = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]\n",
    "    # iterate over kits and create combinations:\n",
    "    for kit_idx in range(NUM_KITS):\n",
    "        cur_audio = sample_audio[kit_idx]\n",
    "        cur_annot = np.asarray(sample_annotations[kit_idx][:, 0]*fs, dtype=int)\n",
    "        # create combinations:\n",
    "        # there are four onsetes per instrument, with different loudness.\n",
    "        # we will only combine onsetes with the same loudness, but use\n",
    "        # ever loudness value for our classifier.\n",
    "        for onset_idx in range(4):\n",
    "            kd = cur_annot[onset_idx]\n",
    "            sd = cur_annot[onset_idx + 4]\n",
    "            hh = cur_annot[onset_idx + 8]\n",
    "\n",
    "            for combo_idx, combo in enumerate(combos):\n",
    "                # add audio to create combined onset\n",
    "                combo_audio = cur_audio[kd:(kd+hits_len)] * combo[0] +\\\n",
    "                              cur_audio[sd:(sd+hits_len)] * combo[1] +\\\n",
    "                              cur_audio[hh:(hh+hits_len)] * combo[2]\n",
    "\n",
    "                # calculate features (mean spectrogram), add features and add lable\n",
    "                cur_mean = np.sum(compute_feature(combo_audio, **SETTINGS), axis=0)\n",
    "                cur_mean = cur_mean / np.max(cur_mean)\n",
    "                knn_feats.append(cur_mean)\n",
    "                knn_labels.append(combo_idx)\n",
    "\n",
    "    # for the no instrument class (7) use noise with five different levels of volume:\n",
    "    for idx in range(5):\n",
    "        factor = idx*0.2+0.1\n",
    "        combo_audio = np.random.randint(low=int(-32768*factor), high=int(32767*factor),\n",
    "                                        size=(hits_len,), dtype=np.int16)\n",
    "\n",
    "        cur_mean = np.sum(compute_feature(combo_audio, **SETTINGS), axis=0)\n",
    "        cur_mean = cur_mean / np.max(cur_mean)\n",
    "        knn_feats.append(cur_mean)\n",
    "        knn_labels.append(7)\n",
    "\n",
    "    # Train classifier\n",
    "    # Use a KNeighborsClassifier from sklearn (e.g. with 5 neighbours)\n",
    "    # TODO\n",
    "\n",
    "    # initialize an onset detector (use the one from madmom: CNNOnsetProcessor)\n",
    "    # TODO\n",
    "\n",
    "    # and a peak picking method (use the one from madmom: OnsetPeakPickingProcessor)\n",
    "    # TODO\n",
    "\n",
    "    # results list\n",
    "    results_class = [None for _ in range(len(test_audio))]\n",
    "    # iterate over dataset\n",
    "    for idx, data in enumerate(test_audio):\n",
    "        # Detect onsets, using your onset detector of choice.\n",
    "        # TODO\n",
    "        onsets = []  # TODO replace with list of onset positions\n",
    "\n",
    "        # Calculate features for onsets.\n",
    "        onset_feats = [None for _ in range(len(onsets))]\n",
    "        for onsets_idx, onset in enumerate(onsets):\n",
    "            # TODO\n",
    "            onset_feats[onsets_idx] = np.zeros((10,))  # TODO replace with features (mean spectrogram) for the current\n",
    "                                                       # TODO do it as it is done when creating the training data\n",
    "\n",
    "        # Predict class labels for onsets using the trained KNN classifier.\n",
    "        # TODO\n",
    "\n",
    "        # Translate labels back to instrument combinations.\n",
    "        # TODO\n",
    "\n",
    "        # Finally, fill this list with sub lists for each instrument with the corresponding onset times.\n",
    "        # inst_det = [[<times for KD>], [<times for SD>], [<times for HH>]]\n",
    "        # TODO\n",
    "        inst_det = []  # TODO replace this empty list\n",
    "\n",
    "        # Evaluate onsets for this track.\n",
    "        inst_eval = [OnsetEvaluation(inst_det[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                     window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_class[idx] = OnsetSumEvaluation(inst_eval)\n",
    "\n",
    "    # Evaluate over all tracks and print results.\n",
    "    overall_results_class = OnsetSumEvaluation(results_class)\n",
    "    print('Segment and Classify f-measure: %f' % overall_results_class.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGCmqR4ZmHF0"
   },
   "source": [
    "## 5.1. Run method\n",
    "Once the TODO blocks are implemented, run the method in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6isq-TRmHF0"
   },
   "outputs": [],
   "source": [
    "segment_and_classify()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia_DvqCFmHF2"
   },
   "source": [
    "# 6. NMF based methods\n",
    "For the NMF based methods we will use one function that compares (i) a simple NMF method with random initialization, (ii) NMF with template initialization, (iii) fixed bases NMF, and (iv) semi-adaptive bases NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQclXVsgmHF3"
   },
   "source": [
    "## 6.1. NMF base class\n",
    "Implement a class that performs NMF. Fill in the missing code blocks marked with \"TODO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8Gu42MAmHF3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# implement a class which performs NMF\n",
    "class NMFBase:\n",
    "    \"\"\"\n",
    "    class that is used to perform NMF variants\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, B_init, G_init, updateB=True, verbose=False, beta=None):\n",
    "        \"\"\"\n",
    "        :param X: input spectrogram\n",
    "        :param B_init: initialization for base matrix B\n",
    "        :param G_init: initialization for activations matrix G\n",
    "        :param updateB: update matrix B (true) or not (false)\n",
    "        :param verbose: provide more output for debugging\n",
    "        :param beta: use a semi-adaptive update of B using the provided beta\n",
    "                     (None = don't use semi adaptive updates). udpateB has to be True for this.\n",
    "        \"\"\"\n",
    "        self.B = np.copy(B_init)\n",
    "        self.B_init = np.copy(B_init)\n",
    "        self.G = np.copy(G_init)\n",
    "        self.X = X\n",
    "        self.X_hat = np.zeros(X.shape)\n",
    "        self.updateB = updateB\n",
    "        self.verbose = verbose\n",
    "        self.alpha = 1.0\n",
    "        self.beta = beta\n",
    "\n",
    "    # Calculates loss according to generalized Kullback-Leibler divergence between X and X_hat.\n",
    "    def loss(self):\n",
    "        # Implement the generalized Kullback-Leibler divergence as provided in the slides of the lecture.\n",
    "        # Use the classe's fields' current values as inputs (self.B, self.G, ...).\n",
    "        # Use numpy functions np.dot(...), np.devide(), etc. for the calculation!\n",
    "        # TODO\n",
    "        return np.zeros((1,))  # TODO replace this with the real loss\n",
    "\n",
    "    def update(self):\n",
    "        # Implement the update for B and G as discussed in the slides of the lecture.\n",
    "        # Make sure you consider the options self.updateB (fixed bases) and self.beta (semi-adaptive bases).\n",
    "        # Use the classes fields current values as inputs and update them in-place (self.B, self.G, ...).\n",
    "        # Also note the extra slide from the lecture providing implementation hints for the update calculation.\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    # call this method from outside the class to run the NMF algorithm\n",
    "    def optimize(self, tol=1e-4, max_iter=100):\n",
    "        n_iter = 0\n",
    "        errors = []\n",
    "        start_time = time.time()\n",
    "        error_at_init = self.loss()\n",
    "        previous_error = error_at_init\n",
    "        while n_iter < max_iter:\n",
    "            if self.beta is not None:\n",
    "                self.alpha = (n_iter / max_iter) ** self.beta\n",
    "            \n",
    "            self.update()\n",
    "            error = self.loss()\n",
    "            errors.append(error)\n",
    "            n_iter += 1\n",
    "\n",
    "            # test convergence criterion every 10 iterations\n",
    "            if tol > 0 and n_iter % 10 == 0:\n",
    "                if self.verbose:\n",
    "                    iter_time = time.time()\n",
    "                    print(\"Epoch %02d reached after %.3f seconds, error: %f\" %\n",
    "                          (n_iter, iter_time - start_time, error))\n",
    "\n",
    "                if (previous_error - error) / error_at_init < tol:\n",
    "                    break\n",
    "                previous_error = error\n",
    "\n",
    "        # do not print if we have already printed in the convergence test\n",
    "        if self.verbose and (tol == 0 or n_iter % 10 != 0):\n",
    "            end_time = time.time()\n",
    "            print(\"Epoch %02d reached after %.3f seconds.\" %\n",
    "                  (n_iter, end_time - start_time))\n",
    "\n",
    "            if plot:\n",
    "                # plot NMF loss development along with X, X_hat, B and G for debugging\n",
    "                max_plot_frames = 2000\n",
    "                plt.figure()\n",
    "                plt.subplot(5, 1, 1)\n",
    "                plt.imshow(self.X[:, :max_plot_frames], origin='lower', aspect='auto')\n",
    "                plt.subplot(5, 1, 2)\n",
    "                plt.imshow(self.X_hat[:, :max_plot_frames], origin='lower', aspect='auto')\n",
    "                plt.subplot(5, 1, 3)\n",
    "                plt.plot(self.G[:, :max_plot_frames].T / np.max(self.G[:, :5000].T, axis=0) + [0, 1, 2])\n",
    "                plt.xlim([0, max_plot_frames])\n",
    "                plt.subplot(5, 1, 4)\n",
    "                plt.plot(self.B / np.max(self.B, axis=0) + [0, 1, 2])\n",
    "                plt.subplot(5, 1, 5)\n",
    "                plt.plot(errors)\n",
    "                plt.xlim([0, max_iter])\n",
    "                plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnshvEn1mHF5"
   },
   "source": [
    "## 6.2 NMF experiment function\n",
    "Similar as with the first two approaches, we now implement a function that runs the different NMF approaches on our test dataset. Fill in the missing code blocks marked with \"TODO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZafKX5pmHF6"
   },
   "outputs": [],
   "source": [
    "# run nmf variants\n",
    "def nmf():\n",
    "    name = 'NMF'\n",
    "\n",
    "    # peak picking for NMF, values should work, only change if you have a working method.\n",
    "    nmf_peak_picking = OnsetPeakPickingProcessor(threshold=0.2, smooth=0.0, pre_avg=0.05,\n",
    "                                                 post_avg=0.05, pre_max=0.02, post_max=0.02,\n",
    "                                                 combine=0.02, fps=FPS)\n",
    "\n",
    "    # base-templates from train data which can be used for initialization of B for fixed and semi-adaptive approaches\n",
    "    # extract mean spectrograms from training samples...\n",
    "    means = [[None, None, None] for _ in range(NUM_KITS)]\n",
    "    for cur_kit_idx in range(NUM_KITS):\n",
    "        sample_feat = sample_features[cur_kit_idx]\n",
    "        cur_means = [np.sum(sample_feat[slice(sample_times[idx][0] * FPS, sample_times[idx][1] * FPS), :], axis=0) for\n",
    "                     idx in range(len(sample_times))]\n",
    "        means[cur_kit_idx] = [cur_mean / np.max(cur_mean) for cur_mean in cur_means]\n",
    "\n",
    "    # result lists to hold results for individual tracks for each method\n",
    "    results_mnf_rand = [None for _ in range(len(test_audio))]\n",
    "    results_mnf_init = [None for _ in range(len(test_audio))]\n",
    "    results_mnf_fixed = [None for _ in range(len(test_audio))]\n",
    "    results_mnf_semi_adaptive = [None for _ in range(len(test_audio))]\n",
    "\n",
    "    # iterate over songs...\n",
    "    for idx, data in enumerate(test_audio):\n",
    "        print('%s, track %d' % (name, idx))\n",
    "        cur_kit_idx = idx % NUM_KITS\n",
    "\n",
    "        # plain NMF, random initalization of B and G\n",
    "        # create a NMFBase object and initialize it accordingly. \n",
    "        # run the optimize method of the NMFBase object.\n",
    "        # normalize the activation output G, (divide by its maximum) for consistent peak picking!\n",
    "        # TODO\n",
    "        nmf_trans_rand = np.zeros((1,))  # TODO replace this with the activation functions provided by NMF \n",
    "        # for evaluation, the indices of the output are expected to be 0..KD, 1..SD, 2..HH;\n",
    "        # find a simple method to sort the activation functions correctly, you might reuse the KNN\n",
    "        # classifier from the segment and classify method (make it a global variable in that case).\n",
    "\n",
    "        # evaluate results\n",
    "        inst_det_rand = [nmf_peak_picking.process(nmf_trans_rand[:, inst]) for inst in range(3)]\n",
    "        inst_eval_rand = [OnsetEvaluation(inst_det_rand[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                       window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_mnf_rand[idx] = OnsetSumEvaluation(inst_eval_rand)\n",
    "\n",
    "        # plain NMF, initialize B with templates\n",
    "        # create a NMFBase object and initialize it accordingly. \n",
    "        # run the optimize method of the NMFBase object.\n",
    "        # normalize the activation output G, (divide by its maximum) for consistent peak picking!\n",
    "        # TODO\n",
    "        nmf_trans_init = np.zeros((1,))  # TODO replace this with the activation functions provided by NMF \n",
    "\n",
    "        # evaluate results\n",
    "        inst_det_init = [nmf_peak_picking.process(nmf_trans_init[:, inst]) for inst in range(3)]\n",
    "        inst_eval_init = [OnsetEvaluation(inst_det_init[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                       window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_mnf_init[idx] = OnsetSumEvaluation(inst_eval_init)\n",
    "\n",
    "        # perform fixed bases NMF, initialize B with templates\n",
    "        # create a NMFBase object and initialize it accordingly. \n",
    "        # run the optimize method of the NMFBase object.\n",
    "        # normalize the activation output G, (divide by its maximum) for consistent peak picking!\n",
    "        # TODO\n",
    "        nmf_trans_fixed = np.zeros((1,))  # TODO replace this with the activation functions provided by NMF \n",
    "\n",
    "        # evaluate results\n",
    "        inst_det_fixed = [nmf_peak_picking.process(nmf_trans_fixed[:, inst]) for inst in range(3)]\n",
    "        inst_eval_fixed = [OnsetEvaluation(inst_det_fixed[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                     window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_mnf_fixed[idx] = OnsetSumEvaluation(inst_eval_fixed)\n",
    "\n",
    "        # semi adaptive NMF, initialize B with templates\n",
    "        # create a NMFBase object and initialize it accordingly. \n",
    "        # run the optimize method of the NMFBase object.\n",
    "        # normalize the activation output G, (divide by its maximum) for consistent peak picking!\n",
    "        # TODO\n",
    "        nmf_trans_semi_adaptive = np.zeros((1,))  # TODO replace this with the activation functions provided by NMF \n",
    "\n",
    "        # evaluate results\n",
    "        inst_det_semi_adaptive = [nmf_peak_picking.process(nmf_trans_semi_adaptive[:, inst]) for inst in range(3)]\n",
    "        inst_eval_semi_adaptive = [OnsetEvaluation(inst_det_semi_adaptive[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                       window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_mnf_semi_adaptive[idx] = OnsetSumEvaluation(inst_eval_semi_adaptive)\n",
    "\n",
    "    # collect overall results and print output\n",
    "    overall_results_nmf_rand = OnsetSumEvaluation(results_mnf_rand)\n",
    "    print('%s rand-init f-measure: %f' % (name, overall_results_nmf_rand.fmeasure))\n",
    "\n",
    "    overall_results_nmf_init = OnsetSumEvaluation(results_mnf_init)\n",
    "    print('%s template init f-measure: %f' % (name, overall_results_nmf_init.fmeasure))\n",
    "\n",
    "    overall_results_nmf_fixed = OnsetSumEvaluation(results_mnf_fixed)\n",
    "    print('%s fixed f-measure: %f' % (name, overall_results_nmf_fixed.fmeasure))\n",
    "\n",
    "    overall_results_nmf_semi_adaptive = OnsetSumEvaluation(results_mnf_semi_adaptive)\n",
    "    print('%s semi-adapative f-measure: %f' % (name, overall_results_nmf_semi_adaptive.fmeasure))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7_1ISl4mHF8"
   },
   "source": [
    "## 6.3. Run method\n",
    "Once the TODO blocks are implemented, run the method in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdxC-n78mHF8"
   },
   "outputs": [],
   "source": [
    "nmf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbWCKPkVmHF-"
   },
   "source": [
    "# 7. CNN based method\n",
    "In the next couple of code blocks we will implement a neural network based (CNN) ADT approach.\n",
    "Please fill in the missing code marked with \"TODO\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gD7SpyYcmHF_"
   },
   "source": [
    "## 7.1. CNN class\n",
    "Implement a convolutional neural network as shown in the slides of the lecture. Implement the missing parts marked with \"TODO\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoVG9e96mHGA"
   },
   "outputs": [],
   "source": [
    "# transcription base class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #\n",
    "        # In this constructor, create the layers needed to build the network.\n",
    "        # Use the pytorch components nn.Conv2d, nn.BatchNorm2d, nn.Dropout2d, nn.Linear, nn.BatchNorm1d\n",
    "        # The network should have the same architecture as presented in the lecture slides (CNN)\n",
    "        # Note that one convolutional block (yellowish blocks) consists of TWO layers of 3x3 convolutions with\n",
    "        # batch normalization for EACH layer and max pooling and dropout after the convolutional block (after the 2nd\n",
    "        # convolutional layer. The whole network consists of two convolutional blocks, where in the first each layer\n",
    "        # contains 32 filters, and in the second each layer contains 64 filters.\n",
    "        # After that, a dense layer (nn.Linear) with 50 neurons and the output dense layer with 3 neurons follow.\n",
    "        # TODO\n",
    "        # e.g. for the first convolutional layer we will need something like:\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function calculates a forward pass through the network (i.e. calculates the output for given input x).\n",
    "        # Hand x through the layers of the network and calculate the output.\n",
    "        # Don't forget to apply the nonlinearities (activation functions).\n",
    "        # Use ReLU activation function ( torch_func.relu() ) except for the ouput of the network where we need\n",
    "        # sigmoid activations (0-1) for our activation functions ( torch_func.sigmoid() or torch.sigmoid() )\n",
    "        # TODO\n",
    "        # e.g. to calculate the hidden output of the first convolutional layer:\n",
    "        h1 = torch_func.relu(self.conv1_bn(self.conv1(x)))\n",
    "\n",
    "        # Note that you should always apply the batch normalization, max pooling (torch_func.max_pool2d),\n",
    "        # and dropout BEFORE you apply the activation function!!\n",
    "        # TODO\n",
    "        hn = 0  # TODO replace this with the last layer of the network\n",
    "        y = torch_func.sigmoid(hn)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVGc7roQmHGC"
   },
   "source": [
    "## 7.2 CNN helper functions, dataset preparation, and data formatter\n",
    "The next code block defines some helper functions, read through the code---there is nothing to implement in this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdR6iwGRmHGC"
   },
   "outputs": [],
   "source": [
    "def train_nn(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for one epoch of NN training.\n",
    "    Within one epoch, all the data is used once, we use mini-batch gradient descent.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be trained\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param train_loader: Data provider\n",
    "    :param optimizer: Optimizer (Gradient descent update algorithm)\n",
    "    :param epoch: Current epoch number\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout layers for example).\n",
    "    model.train()\n",
    "    # we measure the needed time\n",
    "    t = time.time()\n",
    "    # iterate over training data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device (GPU) if necessary\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data)\n",
    "        # calculate loss\n",
    "        loss = torch_func.binary_cross_entropy(output, target)\n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using our optimizer\n",
    "        optimizer.step()\n",
    "        # print some outputs if we reached our logging intervall\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "def test_nn(args, model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Function wich iterates over test data (eval or test set) and calculates loss.\n",
    "    Here no parameter update is done\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be tested\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param test_loader: Data provider\n",
    "    :return: cumulative test loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    test_loss = 0\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in test_loader:\n",
    "            # move data to device (GPU) if necessasry\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data)\n",
    "            # claculate loss and add it to our cumulative loss\n",
    "            test_loss += torch_func.binary_cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        test_loss, len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def inference_cnn(args, model, device, data, outnum=3):\n",
    "    \"\"\"\n",
    "    Function calculating the actual output of the network, given some input.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The network to be used\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param data: Data for which the output should be calculated\n",
    "    :param outnum: Number of network outputs to reserve output memory\n",
    "    :return: output of network\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    # reserve output memory\n",
    "    in_shape = data.shape\n",
    "    side = int((args.context-1)/2)\n",
    "    outlen = in_shape[0] - 2*side\n",
    "    output = np.zeros((outlen, outnum))\n",
    "    # move input to device if necessary\n",
    "    data = torch.from_numpy(data[None, None, :, :])\n",
    "    data = data.to(device)\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        # iterate over input data\n",
    "        for idx in range(outlen):\n",
    "            # calculate output for input data\n",
    "            output[idx, :] = model(data[:, :, idx:(idx+args.context), :])[0, :]\n",
    "    # compensate for convolutional context and return output\n",
    "    return np.vstack((np.zeros((side, outnum)), output, np.zeros((side, outnum))))\n",
    "\n",
    "\n",
    "# split data into train, valid and test sets.\n",
    "# for a hard but fair evaluation, we can not reuse a track or kit we trained on for validation or testing.\n",
    "# therefore, we split the data in respect to tracks and kits, only using kit 0 and 1 as well as track 0 and 1 for\n",
    "# training; kit 2 and track 2 for validation; and kit 3 and track 3 for testing.\n",
    "# you can also test on the whole set, but you will see, that for the tracks we trained on, the performance is even\n",
    "# better since the model usually overfits a bit on the training data.\n",
    "train_tracks = [0, 1]\n",
    "train_kits = [0, 1]\n",
    "valid_tracks = [2]\n",
    "valid_kits = [2]\n",
    "test_tracks = [3]\n",
    "test_kits = [3]\n",
    "\n",
    "train_idxs = [track * NUM_KITS + kit for kit in train_kits for track in train_tracks]\n",
    "valid_idxs = [track * NUM_KITS + kit for kit in valid_kits for track in valid_tracks]\n",
    "\n",
    "test_idxs = [track * NUM_KITS + kit for kit in test_kits for track in test_tracks]\n",
    "\n",
    "train_feat = [np.asarray(features[cur_idx], dtype='float32') for cur_idx in train_idxs]\n",
    "train_feat.extend([np.asarray(feat, dtype='float32') for feat in sample_features])\n",
    "valid_feat = [np.asarray(features[cur_idx], dtype='float32') for cur_idx in valid_idxs]\n",
    "test_feat = [np.asarray(features[cur_idx], dtype='float32') for cur_idx in test_idxs]\n",
    "\n",
    "train_annot = [annotations[cur_idx] for cur_idx in train_idxs]\n",
    "train_annot.extend(sample_annotations)\n",
    "valid_annot = [annotations[cur_idx] for cur_idx in valid_idxs]\n",
    "test_annot = [annotations[cur_idx] for cur_idx in test_idxs]\n",
    "\n",
    "train_targets = [np.asarray(targets[cur_idx], dtype='float32') for cur_idx in train_idxs]\n",
    "train_targets.extend([np.asarray(targ, dtype='float32') for targ in sample_targets])\n",
    "valid_targets = [np.asarray(targets[cur_idx], dtype='float32') for cur_idx in valid_idxs]\n",
    "test_targets = [np.asarray(targets[cur_idx], dtype='float32') for cur_idx in test_idxs]\n",
    "\n",
    "\n",
    "# class which formats the spectrogram data in the way needed for convolutional neural network training\n",
    "class DrumSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list, context, hop):\n",
    "        \"\"\"\n",
    "        Create spectrogram based drum dataset for CNN training\n",
    "        :param feat_list: list with spectrograms (np.array) for individual tracks\n",
    "        :param targ_list: list with targets (np.array) for individual tracks\n",
    "        :param context: length of context for CNN\n",
    "        :param hop: hop size between two consecutive spectrogram snippets. this is usually 1, especiall for testing\n",
    "                    and inference. For very large datasets it can make sense to increase this, to speed up training.\n",
    "        \"\"\"\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.context = context\n",
    "        self.side = int((context-1)/2)\n",
    "        self.hop_size = hop\n",
    "        # list with snippet count per track\n",
    "        self.snip_cnt = []\n",
    "        # overall snippet count\n",
    "        total_snip_cnt = 0\n",
    "        # calculate overall number of snippets we can get from our data\n",
    "        for feat in feat_list:\n",
    "            cur_len = int(np.ceil((feat.shape[0] - self.side*2) / self.hop_size))\n",
    "            self.snip_cnt.append(cur_len)\n",
    "            total_snip_cnt += cur_len\n",
    "        self.length = int(total_snip_cnt)\n",
    "        super(DrumSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a snipped by index, from the whole dataset\n",
    "        :param index: index of the snipped to be returned\n",
    "        :return: a snipped for CNN training\n",
    "        \"\"\"\n",
    "        # find track which contains snipped with index [index]\n",
    "        overal_pos = 0\n",
    "        for idx, cnt in enumerate(self.snip_cnt):\n",
    "            # check if this track contains the snipped with index [index]\n",
    "            if index < overal_pos+cnt:\n",
    "                break\n",
    "            else:\n",
    "                # if not, add the current tracks snipped count to the overall snipped count already visited\n",
    "                overal_pos += cnt\n",
    "\n",
    "        # calculate the position of the snipped within the track nr. [idx]\n",
    "        position = index-overal_pos\n",
    "        position += self.side\n",
    "\n",
    "        # get snipped and target\n",
    "        sample = self.features[idx][(position-self.side):(position+self.side+1), :]\n",
    "        target = self.targets[idx][position, :]\n",
    "        # convert to PyTorch tensor and return\n",
    "        return torch.from_numpy(sample).unsqueeze_(0), torch.from_numpy(target)\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VT2mCfwlmHGE"
   },
   "source": [
    "## 7.3 CNN experiment function\n",
    "The next code block contains the experiment using the CNN. First we train the CNN, then we use it to transcribe the test split of the dataset.\n",
    "Implement the missing blocks marked with \"TODO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgbXmXBnmHGF"
   },
   "outputs": [],
   "source": [
    "# cnn drum transcription experiment\n",
    "def cnn():\n",
    "    print('Training CNN...')\n",
    "    # peak picking for CNN outputs\n",
    "    cnn_peak_picking = OnsetPeakPickingProcessor(threshold=0.05, smooth=0.0, pre_avg=0.01,\n",
    "                                                 post_avg=0.01, pre_max=0.02, post_max=0.02,\n",
    "                                                 combine=0.02, fps=FPS)\n",
    "    # parameters for NN training\n",
    "    name = 'CNN'\n",
    "    args = Args()\n",
    "    args.batch_size = 64\n",
    "    args.test_batch_size = 1000\n",
    "    args.max_epochs = 1000\n",
    "    args.patience = 4\n",
    "    args.lr = 0.01\n",
    "    args.momentum = 0.5\n",
    "    args.no_cuda = not g_use_cuda\n",
    "    args.seed = 1\n",
    "    args.log_interval = 100\n",
    "    args.context = 25\n",
    "    args.step_size = 1\n",
    "\n",
    "    # setup pytorch\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # create model and optimizer, we use plain SGD with momentum\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(DrumSet(train_feat, train_targets, args.context, args.step_size),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(DrumSet(valid_feat, valid_targets, args.context, 1),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(DrumSet(test_feat, test_targets, args.context, 1),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_test_loss = 9999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_nn(args, model, device, train_loader, optimizer, epoch)\n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        test_loss = test_nn(args, model, device, valid_loader)\n",
    "        # check for early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, CNN_MODEL_NAME + '.model'))\n",
    "            best_test_loss = test_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate CNN...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    test_nn(args, model, device, test_loader)\n",
    "\n",
    "    # calculate actual output for the test data and generate a transcript and evaluate it.\n",
    "    results_cnn = [None for _ in range(len(test_feat))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_feat):\n",
    "        # run the inference method\n",
    "        # TODO\n",
    "\n",
    "        # evaluate results, compare how it was done with NMF\n",
    "        # TODO\n",
    "        results_cnn[test_idx] = 0  # replace with OnsetSumEvaluation\n",
    "\n",
    "    # calculate overall results and print output\n",
    "    overall_results_cnn = OnsetSumEvaluation(results_cnn)\n",
    "    print('%s f-measure: %f' % (name, overall_results_cnn.fmeasure))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrBfxGN7mHGH"
   },
   "source": [
    "## 7.4. Run experiment\n",
    "The next and last code block runs the CNN experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkAlJChEmHGH"
   },
   "outputs": [],
   "source": [
    "cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3e_VLa4nmHGJ"
   },
   "source": [
    "# 8. Results and discussion\n",
    "Briefly note your observations about the evaluation.\n",
    "How did the methods perform compared to each other?\n",
    "What can you say in terms of runtime?\n",
    "What was surprising? Can you give explanations for the differences in performance?\n",
    "How would you expect to perform the individual methods on other, unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0DxitHhmHGK"
   },
   "source": [
    "Your answer goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-crhqvkmHGK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2 - <mat.nr.> <name> - GColab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
