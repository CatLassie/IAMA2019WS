{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 2 Experiment!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies, imports, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# filters, might be useful for separate and detect\n",
    "from scipy.signal import butter, freqz\n",
    "from scipy.ndimage.filters import maximum_filter, uniform_filter\n",
    "\n",
    "# classifier for segment and classify method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.audio.spectrogram import LogarithmicFilteredSpectrogram\n",
    "from madmom.audio import Signal\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "from madmom.evaluation import OnsetEvaluation, OnsetSumEvaluation\n",
    "from madmom.features import CNNOnsetProcessor\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "# plotting library for visualization for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'pgf.rcfonts': False})\n",
    "\n",
    "COLAB_DRIVE_BASE = \"/content/g-drive\"\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if in colab, mount gdrive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  print('trying to mount google drive...')\n",
    "  drive.mount(COLAB_DRIVE_BASE, force_remount=True)\n",
    "\n",
    "#\n",
    "# some global parameter settings we will need along the way\n",
    "#\n",
    "EPSILON = np.finfo(np.float32).eps  # small epsilon needed sometimes for computational stability (div by zeros)\n",
    "\n",
    "SETTINGS = {  # settings for spectrogram (feature) calculation\n",
    "    'fps': 100,  # frames per second of our resulting spectrograms\n",
    "    'fmin': 30,  # minimum frequency\n",
    "    'fmax': 15000,  # maximum frequency of spectrogram\n",
    "    'frame_size': 2048,  # frame size for spectrogram\n",
    "    'sample_rate': 44100,  # input sample rate - input audio will be resampled to this sample rate.\n",
    "    'num_bands': 12,  # bands per octave (freq. factor 2)\n",
    "    'num_channels': 1,  # input audio will be converted to mono\n",
    "    'norm_filters': True,  # normalize triangular filters for log/log spectrogram to have equal area\n",
    "}\n",
    "\n",
    "# drum label names\n",
    "# all arrays and lists containing instruments will always follow this index system, 0:KD (kick/bass drum),\n",
    "# 1:SD (snare drum), 2: HH (hi-hat).\n",
    "names_3_map = ['KD', 'SD', 'HH']\n",
    "num_3_drum_notes = len(names_3_map)\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "  PATH = os.path.join(COLAB_DRIVE_BASE, 'My Drive/Colab Notebooks')\n",
    "\n",
    "DATA_PATH = os.path.join(PATH, 'data/drums_simple')  # change this value if you copied the dataset somewhere else!\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'annotations')\n",
    "SAMPLE_ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'sample_annotations')\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, 'samples')\n",
    "CACHE_PATH = os.path.join(DATA_PATH, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "MODEL_PATH = os.path.join(DATA_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\n",
    "# some info about our data\n",
    "NUM_KITS = 4  # we have 4 different drum kits\n",
    "NUM_TRACKS = 4  # and 4 tracks per kit\n",
    "FPS = SETTINGS['fps']  # shorthand to the FPS we use for our spectrogram\n",
    "RANK = num_3_drum_notes  # we use three instruments\n",
    "\n",
    "# turn on / off plotting (for debugging)\n",
    "plot = False\n",
    "plot_len = 400\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 12345\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# some helper functions to handle data from our example dataset\n",
    "#\n",
    "def step_diff(array, step):\n",
    "    \"\"\"\n",
    "    Calculates a 1st order difference between rows step values .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.array\n",
    "        Input array to calculate the 1st order difference.\n",
    "\n",
    "    step : int\n",
    "        Number of steps for offset for difference calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    difference : np.array\n",
    "        Array containing the 1st order difference. Note that the number of rows will be steps less than the input's.\n",
    "    \"\"\"\n",
    "    a = array[step:]\n",
    "    b = array[:-step]\n",
    "    return b-a\n",
    "\n",
    "\n",
    "def load_audio(audio_file_list):\n",
    "    \"\"\"\n",
    "    Load audio from the given files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_file_list : list\n",
    "        List with audio filenames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    audio_list : list\n",
    "        List containing the actual audio.\n",
    "\n",
    "    \"\"\"\n",
    "    audio_list = []\n",
    "    for audio_file in audio_file_list:\n",
    "        signal = Signal(audio_file, **SETTINGS)\n",
    "        audio_list.append(signal)\n",
    "    return audio_list\n",
    "\n",
    "\n",
    "def compute_feature(file, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute (spectrogram) feature for the given audio file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Audio file name.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments used for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature : numpy array\n",
    "        Computed feature\n",
    "\n",
    "    \"\"\"\n",
    "    # create (filtered) spectrogram\n",
    "    return LogarithmicFilteredSpectrogram(file, **kwargs)\n",
    "\n",
    "\n",
    "def create_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "            print('successfully loaded cached file:', cache_file)\n",
    "        else:\n",
    "            feat = compute_feature(audio_file, **kwargs)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "                print('successfully stored cache for file:', audio_file)\n",
    "        feature_list.append(feat)\n",
    "    return feature_list\n",
    "\n",
    "\n",
    "def load_annotations(files):\n",
    "    \"\"\"\n",
    "    Load annotations from files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with annotation filenames.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    annotation_list : list\n",
    "        List with annotations.\n",
    "\n",
    "    \"\"\"\n",
    "    annotation_list = []\n",
    "    for annotation_file in files:\n",
    "        annotation = madmom.io.load_notes(annotation_file)\n",
    "        annotation_list.append(annotation)\n",
    "    return annotation_list\n",
    "\n",
    "\n",
    "def compute_target_array_from_times(times, fps, num_frames, num_targets):\n",
    "    \"\"\"\n",
    "    creates a numpy array with targets for neural network training\n",
    "    :param times: list\n",
    "    list of annotations for which the target should be 1. times in seconds.\n",
    "    :param fps:\n",
    "    sampling frequency of target array\n",
    "    :param num_frames:\n",
    "    total number of frames (all entries in times must fit into the total number of frames).\n",
    "    :param num_targets:\n",
    "    total number of targets (all entries in times must fit into the total number of labels).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(times) > 0 and np.max(times, 0)[0] * fps > num_frames:\n",
    "        print(\"Maximum time is larger than number of samples - cutting times.\")\n",
    "    if len(times) > 0 and np.max(times, 0)[1] >= num_targets:\n",
    "        print(\"Maximum label index is larger than num_targets - cutting labels.\")\n",
    "\n",
    "    new_targets = np.zeros((num_frames, num_targets))\n",
    "    for entry_nr, time_entry in enumerate(times):\n",
    "        cur_time = time_entry[0]\n",
    "        time_idx = int(cur_time*fps)\n",
    "        inst_idx = int(time_entry[1])\n",
    "        if 0 <= inst_idx < num_targets:\n",
    "            if time_idx < num_frames:\n",
    "                new_targets[time_idx, inst_idx] = 1\n",
    "\n",
    "    return new_targets\n",
    "\n",
    "\n",
    "def create_targets(annotation_list, feature_list, fps=FPS, num_classes=3):\n",
    "    \"\"\"\n",
    "    Create targets for the given annotations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    annotation_list : list\n",
    "        List with annotations\n",
    "    feature_list : list\n",
    "        List with features (needed to determine length)\n",
    "    fps : float\n",
    "        Frames per second\n",
    "    num_classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    target_list : list\n",
    "        List with targets for NN training.\n",
    "\n",
    "    \"\"\"\n",
    "    target_list = []\n",
    "    for annotation, feature in zip(annotation_list, feature_list):\n",
    "        target = compute_target_array_from_times(annotation, fps, len(feature), num_classes)\n",
    "        target_list.append(target)\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def plot_peak_picking(onset_function, pre_avg = 0.05, post_avg = 0.05, pre_max = 0.02, post_max = 0.02, combine = 0.02,\n",
    "                      thresh = 0.2, smooth = 0.0, plot_frames=1000):\n",
    "    \"\"\"\n",
    "    helper function which visualizes the peak picking parameters, use to adapt peak picking settings if necessary.\n",
    "    :param onset_function:\n",
    "    :param pre_avg:\n",
    "    :param post_avg:\n",
    "    :param pre_max:\n",
    "    :param post_max:\n",
    "    :param combine:\n",
    "    :param thresh:\n",
    "    :param smooth:\n",
    "    :param plot_frames:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # plot example to investigate peak picking\n",
    "    peak_picker = OnsetPeakPickingProcessor(threshold=thresh, smooth=smooth, pre_avg=pre_avg,\n",
    "                                               post_avg=post_avg, pre_max=pre_max, post_max=post_max,\n",
    "                                               combine=combine, fps=FPS)\n",
    "    inst_det = [peak_picker.process(onset_function[:, inst]) for inst in range(3)]\n",
    "\n",
    "    for inst in range(3):\n",
    "        plt.subplot(3, 1, inst + 1)\n",
    "        activations = onset_function[:plot_frames, inst]\n",
    "\n",
    "        avg_length = (pre_avg + post_avg) * FPS + 1\n",
    "        avg_origin = int(np.floor((pre_avg - post_avg) * FPS / 2))\n",
    "        avg_filter_size = avg_length\n",
    "        max_length = (pre_max + post_max) * FPS + 1\n",
    "        max_filter_size = max_length\n",
    "        max_origin = int(np.floor((pre_max - post_max) * FPS / 2))\n",
    "\n",
    "        mov_avg = uniform_filter(activations, avg_filter_size, mode='constant', origin=avg_origin)\n",
    "        mov_max = maximum_filter(activations, max_filter_size, mode='constant', origin=max_origin)\n",
    "\n",
    "        select = inst_det[inst] * FPS < plot_frames\n",
    "        peaks = inst_det[inst][select] * FPS\n",
    "        plt.plot(activations)\n",
    "        plt.plot(peaks, onset_function[np.asarray(peaks, dtype=int), inst], 'ro')\n",
    "        plt.plot(mov_avg, 'g')\n",
    "        plt.plot(mov_max, 'y')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_activation_functions(spectrogram, activation_functions, templates=None):\n",
    "    \"\"\"\n",
    "    helper function that visualizes spectrogram alongside detected activation functions.\n",
    "    use to debug your methods.\n",
    "    :param spectrogram:\n",
    "    :param activation_functions:\n",
    "    :param templates:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if templates is None:\n",
    "        num_plots = 2\n",
    "    else:\n",
    "        num_plots = 3\n",
    "    plt.figure()\n",
    "    plt.subplot(num_plots, 1, 1)\n",
    "    plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    if templates is not None:\n",
    "        plt.subplot(num_plots, 1, 2)\n",
    "        plt.imshow(templates, aspect='auto', origin='lower')\n",
    "    plt.subplot(num_plots, 1, num_plots)\n",
    "    plt.plot(activation_functions[:, 0])\n",
    "    plt.plot(activation_functions[:, 1] + 1)\n",
    "    plt.plot(activation_functions[:, 2] + 2)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# load our example dataset\n",
    "#\n",
    "\n",
    "# load audio and calculate features\n",
    "audio_files = search_files(AUDIO_PATH, '.wav')\n",
    "audio_files += search_files(AUDIO_PATH, '.flac')\n",
    "audio = load_audio(audio_files)\n",
    "features = create_features(audio_files, **SETTINGS)\n",
    "\n",
    "sample_files = search_files(SAMPLES_PATH, '.wav')\n",
    "sample_files += search_files(SAMPLES_PATH, '.flac')\n",
    "sample_audio = load_audio(sample_files)\n",
    "sample_features = create_features(sample_files, **SETTINGS)\n",
    "\n",
    "\n",
    "# load annotations and create targets\n",
    "annotation_files = search_files(ANNOTATIONS_PATH, '.txt')\n",
    "annotations = load_annotations(annotation_files)\n",
    "targets = create_targets(annotations, features)\n",
    "\n",
    "sample_annotation_files = search_files(SAMPLE_ANNOTATIONS_PATH, '.txt')\n",
    "sample_annotations = load_annotations(sample_annotation_files)\n",
    "sample_targets = create_targets(sample_annotations, features)\n",
    "sample_times = [[0, 8], [11, 19], [21, 29]]  # these are the times within which the onsets for each instrumt\n",
    "                                              # are in the audio (in spec seconds) detailed annotations exist too!\n",
    "\n",
    "fs = SETTINGS['sample_rate']\n",
    "test_audio = audio[:(NUM_TRACKS * NUM_KITS)]\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Separate and detect approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_and_detect():\n",
    "    \"\"\"\n",
    "    this function runs the main loop over our dataset using a simple separate and detect approach\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To separate the individual drum instruments you can either use bandpass filters, see \n",
    "        https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "    You can look at the spectrogram to decide where you want to place the cutoff-frequencies.\n",
    "    For the kits used in our dataset, these frequencies might work ok: \n",
    "    low 0-100 Hz\n",
    "    high 10k - 22k Hz\n",
    "    mid  100-10k Hz\n",
    "        \n",
    "    or simply calculate a spectrogram and only use the relevant frequency bands, i.e. set the rest to 0 \n",
    "    (recommended, will usually work better). \n",
    "    The following frequency bands should work with our dataset: \n",
    "    low: bands 0-4\n",
    "    mid: bands 6-30\n",
    "    high: bands 50-end\n",
    "    \n",
    "    Note: this doesn't work too well, aim for around 50% f-measure; don't spend to much time on this approach.\n",
    "    \"\"\"\n",
    "\n",
    "    # peak picking settings, use these settings, only play around with them once you have a working system.\n",
    "    ############ NOTE: 2.3 seems like a reasonable value for the threshold ###############################\n",
    "    peak_picking_sep = OnsetPeakPickingProcessor(threshold=2.3, smooth=0.0, combine=0.04, delay=0.0, fps=100,\n",
    "                                                 pitch_offset=0, pre_max=0.02, post_max=0.02)\n",
    "\n",
    "    results_sep = [None for _ in range(len(test_audio))]\n",
    "    # iterate over tracks\n",
    "    for idx, data in enumerate(test_audio):\n",
    "        inst_eval = [None, None, None]\n",
    "        # for each instrument\n",
    "        for inst in range(3):\n",
    "            # get the spectrogram for the current file\n",
    "            filt_spec = np.copy(features[idx])\n",
    "\n",
    "            # fiter the signal for current instrument\n",
    "            ################# INSTRUMENT FILTERING ######################\n",
    "            for i, frame in enumerate(filt_spec):\n",
    "                filt_frame = None\n",
    "                if inst == 0:\n",
    "                    filt_frame = np.concatenate((frame[:5], np.zeros(len(frame[5:]))))\n",
    "                if inst == 1:\n",
    "                    filt_frame = np.concatenate((np.zeros(len(frame[:6])), frame[6:31], np.zeros(len(frame[31:]))))    \n",
    "                if inst == 2:\n",
    "                    filt_frame = np.concatenate((np.zeros(len(frame[:50])), frame[50:]))\n",
    "                    \n",
    "                filt_spec[i] = filt_frame\n",
    "                \n",
    "            # calculate a simple onset detection function\n",
    "            # e.g. use the step_diff function to calculate the spectral diff, like discussed\n",
    "            # in the onset detection part (sum of abs. diffs)\n",
    "            ####################### ODF CALCULATION #########\n",
    "            odf = []\n",
    "            for i, frame in enumerate(filt_spec):\n",
    "                flux_sum = 0\n",
    "                for j, bin in enumerate(frame):\n",
    "                    diff = filt_spec[i][j] - (filt_spec[i-1][j] if i > 0 else 0)\n",
    "                    flux = diff if diff >= 0 else 0\n",
    "                    flux_sum = flux_sum + flux\n",
    "\n",
    "                odf.append(flux_sum)\n",
    "                \n",
    "            ##################### INFO ####################\n",
    "            print('spectrogram:', idx, 'instrument:', inst)\n",
    "            ###############################################\n",
    "\n",
    "            onset_activations = np.array(odf)\n",
    "\n",
    "            # peack picking and onset evaluation\n",
    "            filt_detections = peak_picking_sep.process(onset_activations)\n",
    "            inst_eval[inst] = OnsetEvaluation(filt_detections,\n",
    "                                              [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                              window=0.05, combine=0.025)\n",
    "\n",
    "        # evaluation over all isntruments for the current track\n",
    "        results_sep[idx] = OnsetSumEvaluation(inst_eval)\n",
    "\n",
    "    # evaluation over all tracks, ouput results\n",
    "    overall_results_sep = OnsetSumEvaluation(results_sep)\n",
    "    print('Separate and Detect f-measure: %f' % overall_results_sep.fmeasure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Run method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_and_detect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "######################### ODF CALCULATION flux 1 (uses step_diff) #########\n",
    "'''\n",
    "filt_spec_T = filt_spec.transpose()\n",
    "\n",
    "diffs = []\n",
    "for i, bin in enumerate(filt_spec_T):\n",
    "    diff = step_diff(bin, 1)\n",
    "    diff = [v if v > 0 else 0 for v in diff]\n",
    "    diffs.append(diff)\n",
    "\n",
    "diffs = np.array(diffs)\n",
    "diffs_T = diffs.transpose()\n",
    "\n",
    "odf_1 = []\n",
    "for i, frame in enumerate(diffs_T):\n",
    "    odf_1.append(sum(frame))\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Segment and classify approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_classify():\n",
    "    \"\"\"\n",
    "    this function runs the main loop over our dataset using a simple segment and classify approach\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # train the classifier\n",
    "    #\n",
    "    # we first have to train a classifier which is able to classify the different instruments and also\n",
    "    # combined onsets of them. Since we only have single instrument onsets as sample, we first build a dictionary\n",
    "    # of instrument hit combinations.\n",
    "    # We will then use a simple KNN-classifier; you can experiment with other classifiers after the KNN version \n",
    "    # works, if you want (e.g. try SVMs from sklearn)\n",
    "\n",
    "    # the hits are separated by about one second:\n",
    "    hits_len = int(1*fs)\n",
    "\n",
    "    # list of our features and labels for the classes used to train our classifier\n",
    "    knn_feats = []\n",
    "    knn_labels = []\n",
    "\n",
    "    # onset combinations we want to use. 1..use instrument in combo 0..don't use\n",
    "    # indices are: KD, SD, HH\n",
    "    combos = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]\n",
    "    # iterate over kits and create combinations:\n",
    "    for kit_idx in range(NUM_KITS):\n",
    "        cur_audio = sample_audio[kit_idx]\n",
    "        cur_annot = np.asarray(sample_annotations[kit_idx][:, 0]*fs, dtype=int)\n",
    "        # create combinations:\n",
    "        # there are four onsetes per instrument, with different loudness.\n",
    "        # we will only combine onsetes with the same loudness, but use\n",
    "        # ever loudness value for our classifier.\n",
    "        for onset_idx in range(4):\n",
    "            kd = cur_annot[onset_idx]\n",
    "            sd = cur_annot[onset_idx + 4]\n",
    "            hh = cur_annot[onset_idx + 8]\n",
    "\n",
    "            for combo_idx, combo in enumerate(combos):\n",
    "                # add audio to create combined onset\n",
    "                combo_audio = cur_audio[kd:(kd+hits_len)] * combo[0] +\\\n",
    "                              cur_audio[sd:(sd+hits_len)] * combo[1] +\\\n",
    "                              cur_audio[hh:(hh+hits_len)] * combo[2]\n",
    "\n",
    "                # calculate features (mean spectrogram), add features and add lable\n",
    "                cur_mean = np.sum(compute_feature(combo_audio, **SETTINGS), axis=0)\n",
    "                cur_mean = cur_mean / np.max(cur_mean)\n",
    "                knn_feats.append(cur_mean)\n",
    "                knn_labels.append(combo_idx)\n",
    "\n",
    "    # for the no instrument class (7) use noise with five different levels of volume:\n",
    "    for idx in range(5):\n",
    "        factor = idx*0.2+0.1\n",
    "        combo_audio = np.random.randint(low=int(-32768*factor), high=int(32767*factor),\n",
    "                                        size=(hits_len,), dtype=np.int16)\n",
    "\n",
    "        cur_mean = np.sum(compute_feature(combo_audio, **SETTINGS), axis=0)\n",
    "        cur_mean = cur_mean / np.max(cur_mean)\n",
    "        knn_feats.append(cur_mean)\n",
    "        knn_labels.append(7)\n",
    "\n",
    "    # Train classifier\n",
    "    # Use a KNeighborsClassifier from sklearn (e.g. with 5 neighbours)\n",
    "    # TODO\n",
    "\n",
    "    # initialize an onset detector (use the one from madmom: CNNOnsetProcessor)\n",
    "    # TODO\n",
    "\n",
    "    # and a peak picking method (use the one from madmom: OnsetPeakPickingProcessor)\n",
    "    # TODO\n",
    "\n",
    "    # results list\n",
    "    results_class = [None for _ in range(len(test_audio))]\n",
    "    # iterate over dataset\n",
    "    for idx, data in enumerate(test_audio):\n",
    "        # Detect onsets, using your onset detector of choice.\n",
    "        # TODO\n",
    "        onsets = []  # TODO replace with list of onset positions\n",
    "\n",
    "        # Calculate features for onsets.\n",
    "        onset_feats = [None for _ in range(len(onsets))]\n",
    "        for onsets_idx, onset in enumerate(onsets):\n",
    "            # TODO\n",
    "            onset_feats[onsets_idx] = np.zeros((10,))  # TODO replace with features (mean spectrogram) for the current\n",
    "                                                       # TODO do it as it is done when creating the training data\n",
    "\n",
    "        # Predict class labels for onsets using the trained KNN classifier.\n",
    "        # TODO\n",
    "\n",
    "        # Translate labels back to instrument combinations.\n",
    "        # TODO\n",
    "\n",
    "        # Finally, fill this list with sub lists for each instrument with the corresponding onset times.\n",
    "        # inst_det = [[<times for KD>], [<times for SD>], [<times for HH>]]\n",
    "        # TODO\n",
    "        inst_det = []  # TODO replace this empty list\n",
    "\n",
    "        # Evaluate onsets for this track.\n",
    "        inst_eval = [OnsetEvaluation(inst_det[inst], [event[0] for event in annotations[idx] if event[1] == inst],\n",
    "                                     window=0.05, combine=0.025) for inst in range(3)]\n",
    "        results_class[idx] = OnsetSumEvaluation(inst_eval)\n",
    "\n",
    "    # Evaluate over all tracks and print results.\n",
    "    overall_results_class = OnsetSumEvaluation(results_class)\n",
    "    print('Segment and Classify f-measure: %f' % overall_results_class.fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Run method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_and_classify()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
