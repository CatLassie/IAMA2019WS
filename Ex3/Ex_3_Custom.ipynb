{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 3 Experiment!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2 imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# filters, might be useful for separate and detect\n",
    "from scipy.signal import butter, freqz\n",
    "from scipy.ndimage.filters import maximum_filter, uniform_filter\n",
    "\n",
    "# classifier for segment and classify method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.audio.spectrogram import LogarithmicFilteredSpectrogram\n",
    "from madmom.audio import Signal\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "from madmom.evaluation import OnsetEvaluation, OnsetSumEvaluation\n",
    "from madmom.features import CNNOnsetProcessor\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "\"\"\"\n",
    "# plotting library for visualization for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'pgf.rcfonts': False})\n",
    "\n",
    "COLAB_DRIVE_BASE = \"/content/g-drive\"\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if in colab, mount gdrive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  print('trying to mount google drive...')\n",
    "  drive.mount(COLAB_DRIVE_BASE, force_remount=True)\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "# some global parameter settings we will need along the way\n",
    "#\n",
    "EPSILON = np.finfo(np.float32).eps  # small epsilon needed sometimes for computational stability (div by zeros)\n",
    "\n",
    "\"\"\"\n",
    "SETTINGS = {  # settings for spectrogram (feature) calculation\n",
    "    'fps': 100,  # frames per second of our resulting spectrograms\n",
    "    'fmin': 30,  # minimum frequency\n",
    "    'fmax': 15000,  # maximum frequency of spectrogram\n",
    "    'frame_size': 2048,  # frame size for spectrogram\n",
    "    'sample_rate': 44100,  # input sample rate - input audio will be resampled to this sample rate.\n",
    "    'num_bands': 12,  # bands per octave (freq. factor 2)\n",
    "    'num_channels': 1,  # input audio will be converted to mono\n",
    "    'norm_filters': True,  # normalize triangular filters for log/log spectrogram to have equal area\n",
    "}\n",
    "\n",
    "# drum label names\n",
    "# all arrays and lists containing instruments will always follow this index system, 0:KD (kick/bass drum),\n",
    "# 1:SD (snare drum), 2: HH (hi-hat).\n",
    "names_3_map = ['KD', 'SD', 'HH']\n",
    "num_3_drum_notes = len(names_3_map)\n",
    "\"\"\"\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "if IN_COLAB:\n",
    "  PATH = os.path.join(COLAB_DRIVE_BASE, 'My Drive/Colab Notebooks')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DATA_PATH = os.path.join(PATH, 'data/drums_simple')  # change this value if you copied the dataset somewhere else!\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'annotations')\n",
    "SAMPLE_ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'sample_annotations')\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, 'samples')\n",
    "CACHE_PATH = os.path.join(DATA_PATH, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "MODEL_PATH = os.path.join(DATA_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# some info about our data\n",
    "NUM_KITS = 4  # we have 4 different drum kits\n",
    "NUM_TRACKS = 4  # and 4 tracks per kit\n",
    "FPS = SETTINGS['fps']  # shorthand to the FPS we use for our spectrogram\n",
    "RANK = num_3_drum_notes  # we use three instruments\n",
    "\n",
    "# turn on / off plotting (for debugging)\n",
    "plot = False\n",
    "plot_len = 400\n",
    "\"\"\"\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 1234 #12345\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "######## ADJUST DATA PATHS ACCORDING TO YOUR LOCAL CONFIGURATION ########\n",
    "DATA_PATH_1 = os.path.join(PATH, 'data/part_1')\n",
    "AUDIO_PATH_1 = os.path.join(DATA_PATH_1, 'mp3.zip')\n",
    "ANNOTATIONS_PATH_1 = os.path.join(DATA_PATH_1, 'annotations_final.csv')\n",
    "META_DATA_PATH_1 = os.path.join(DATA_PATH_1, 'clip_info_final.csv')\n",
    "\n",
    "CACHE_PATH_1 = os.path.join(DATA_PATH_1, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH_1):\n",
    "    os.makedirs(CACHE_PATH_1)\n",
    "MODEL_PATH_1 = os.path.join(DATA_PATH_1, 'models')\n",
    "if not os.path.exists(MODEL_PATH_1):\n",
    "    os.makedirs(MODEL_PATH_1)  \n",
    "    \n",
    "CNN_MODEL_NAME = 'cnn_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "replace = np.vectorize(lambda v : v.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio, annotations and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = search_files(AUDIO_PATH_1, '.mp3', recursion_depth=1)\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "# librosa cant load these files for some reason\n",
    "# norine_braun-now_and_zen-08-gently-117-146.mp3\n",
    "del audio_files[10687]\n",
    "# jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3\n",
    "del audio_files[12821]\n",
    "# american_baroque-dances_and_suites_of_rameau_and_couperin-26-loracle_suite_in_d_from_les_fetes_dhebe_rameau-0-29\n",
    "del audio_files[13701]\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "annotations = np.genfromtxt(ANNOTATIONS_PATH_1, dtype=str, delimiter='\\t')\n",
    "meta_data = np.genfromtxt(META_DATA_PATH_1, dtype=str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogMelSpectrogram from Music Auto Tagging (+ caching from Ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        # src = src[(n_sample-n_sample_fit)/2:(n_sample+n_sample_fit)/2]\n",
    "        src = src[int((n_sample-n_sample_fit)/2):int((n_sample+n_sample_fit)/2)]\n",
    "        \n",
    "    #logam = librosa.logamplitude\n",
    "    logam = librosa.core.power_to_db\n",
    "    \n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref_power=1.0)\n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS))\n",
    "    \n",
    "    \"\"\"\n",
    "    stft = librosa.core.stft(y=src, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=N_FFT, n_mels=N_MELS)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    ret = db_mel_spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret\n",
    "\n",
    "def init_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH_1, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "        else:\n",
    "            feat = compute_melgram(audio_file)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "        feature_list.append(feat)\n",
    "        if len(feature_list)%5000 == 0:\n",
    "            print('computed', len(feature_list), 'features...')\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep 50 top tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_50_tags(annotations):\n",
    "    \"\"\"\n",
    "    returns annotations filtered by top 50 most frequent tags\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    anno_values = anno[1:, 1:len(anno[0])-1]\n",
    "    anno_int = np.asarray(replace(anno_values), dtype=int)\n",
    "    anno_sum = anno_int.sum(axis=0)\n",
    "    anno_sorted = np.sort(anno_sum)[::-1]\n",
    "    smallest_tag_value = anno_sorted[49]\n",
    "    \n",
    "    tag_indices = np.where(anno_sum >= smallest_tag_value)\n",
    "    tag_array = [i+1 for i in tag_indices[0]]\n",
    "    cols = [0] + tag_array\n",
    "    cols = cols + [len(annotations[0])-1]\n",
    "    \n",
    "    return anno[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = filter_top_50_tags(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Validation / Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_title_dictionary(meta_data):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> track title\n",
    "    \"\"\"\n",
    "    meta = meta_data.copy()\n",
    "    \n",
    "    filtered_meta = meta[1:, [2,9]]\n",
    "    clean_meta = np.asarray(replace(filtered_meta))\n",
    "\n",
    "    meta_dict = {}\n",
    "    for i, d in enumerate(clean_meta):\n",
    "        meta_dict[d[1]] = d[0]\n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "def compute_target_dictionary(annotations):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> list of annotations\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    filtered_anno = anno[1:, 1:]\n",
    "    clean_anno = np.asarray(replace(filtered_anno))\n",
    "\n",
    "    target_dict = {}\n",
    "    for i, d in enumerate(clean_anno):\n",
    "        target_dict[d[50]] = d[:50].astype(np.float32)\n",
    "    \n",
    "    return target_dict\n",
    "\n",
    "def group_audio(audio_files, meta_dict):\n",
    "    \"\"\"\n",
    "    returns audio grouped by track title (based on dictionary)\n",
    "    \"\"\"\n",
    "    grouped_audio = []\n",
    "    same_track = []\n",
    "    for i, a in enumerate(audio_files):\n",
    "        if i == 0:\n",
    "            same_track.append(a)\n",
    "        else:\n",
    "            previous_title = meta_dict[audio_files[i-1].split(AUDIO_PATH_1+'/')[1]]\n",
    "            current_title = meta_dict[audio_files[i].split(AUDIO_PATH_1+'/')[1]]\n",
    "            if previous_title == current_title:\n",
    "                same_track.append(a)\n",
    "            else:\n",
    "                grouped_audio.append(same_track)\n",
    "                same_track = []\n",
    "                same_track.append(a)\n",
    "\n",
    "    grouped_audio.append(same_track)\n",
    "    return grouped_audio\n",
    "\n",
    "def shuffle_and_split_files(grouped_audio):\n",
    "    \"\"\"\n",
    "    returns approx. 50% as training, 25% as validation, 25% as test data (randomly shuffled)\n",
    "    \"\"\"\n",
    "    half_idx = int(len(grouped_audio)/2)\n",
    "    three_quarter_idx = int(half_idx/2) + half_idx\n",
    "    \n",
    "    grouped_audio_shuffled = random.Random(seed).sample(grouped_audio, len(grouped_audio))\n",
    "\n",
    "    training_audio = grouped_audio_shuffled[:half_idx]\n",
    "    validation_audio = grouped_audio_shuffled[half_idx:three_quarter_idx]\n",
    "    test_audio = grouped_audio_shuffled[three_quarter_idx:]\n",
    "\n",
    "    training_audio = [item for sublist in training_audio for item in sublist]\n",
    "    validation_audio = [item for sublist in validation_audio for item in sublist]\n",
    "    test_audio = [item for sublist in test_audio for item in sublist]\n",
    "    \n",
    "    return np.array(training_audio), np.array(validation_audio), np.array(test_audio)\n",
    "\n",
    "def init_targets(audio_files, target_dict):\n",
    "    targets = list(map(lambda v : target_dict[v.split(AUDIO_PATH_1+'/')[1]], audio_files))\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = compute_title_dictionary(meta_data)\n",
    "target_dict = compute_target_dictionary(top_annotations)\n",
    "grouped_audio = group_audio(audio_files, meta_dict)\n",
    "training_audio, validation_audio, test_audio = shuffle_and_split_files(grouped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = init_features(training_audio)\n",
    "validation_features = init_features(validation_audio)\n",
    "test_features = init_features(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_targets = init_targets(training_audio, target_dict)\n",
    "validation_targets = init_targets(validation_audio, target_dict)\n",
    "test_targets = init_targets(test_audio, target_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription base class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #\n",
    "        # In this constructor, create the layers needed to build the network.\n",
    "        # Use the pytorch components nn.Conv2d, nn.BatchNorm2d, nn.Dropout2d, nn.Linear, nn.BatchNorm1d\n",
    "        # The network should have the same architecture as presented in the lecture slides (CNN)\n",
    "        # Note that one convolutional block (yellowish blocks) consists of TWO layers of 3x3 convolutions with\n",
    "        # batch normalization for EACH layer and max pooling and dropout after the convolutional block (after the 2nd\n",
    "        # convolutional layer. The whole network consists of two convolutional blocks, where in the first each layer\n",
    "        # contains 32 filters, and in the second each layer contains 64 filters.\n",
    "        # After that, a dense layer (nn.Linear) with 50 neurons and the output dense layer with 3 neurons follow.\n",
    "        \n",
    "        # e.g. for the first convolutional layer we will need something like:\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.mp1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.drop1 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(64)\n",
    "        self.mp2 = nn.MaxPool2d(3, stride=2)\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.lin1 = nn.Linear(64*5*19, 50) #64x5x19 are the dimensions of one feature at this point\n",
    "        self.lin1_bn = nn.BatchNorm1d(50)\n",
    "        self.drop3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.lin2 = nn.Linear(50,3)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function calculates a forward pass through the network (i.e. calculates the output for given input x).\n",
    "        # Hand x through the layers of the network and calculate the output.\n",
    "        # Don't forget to apply the nonlinearities (activation functions).\n",
    "        # Use ReLU activation function ( torch_func.relu() ) except for the ouput of the network where we need\n",
    "        # sigmoid activations (0-1) for our activation functions ( torch_func.sigmoid() or torch.sigmoid() )\n",
    "        # e.g. to calculate the hidden output of the first convolutional layer:\n",
    "        \n",
    "        print('wtf!!!', x.shape)\n",
    "        \"\"\"\n",
    "        h1 = torch_func.relu(self.conv1_bn(self.conv1(x)))\n",
    "        h2 = torch_func.relu(self.conv2_bn(self.conv2(h1)))\n",
    "        h3 = self.drop1(self.mp1(h2))\n",
    "        h4 = torch_func.relu(self.conv3_bn(self.conv3(h3)))\n",
    "        h5 = torch_func.relu(self.conv4_bn(self.conv4(h4)))\n",
    "        h6 = self.drop2(self.mp2(h5))\n",
    "        h6 = h6.view(h6.size(0), -1) # \"reshape\" for the fully connected layer (keeping the batch size)\n",
    "        h7 = torch_func.relu(self.drop3(self.lin1_bn(self.lin1(h6))))\n",
    "        h8 = self.lin2(h7)\n",
    "\n",
    "        # Note that you should always apply the batch normalization, max pooling (torch_func.max_pool2d),\n",
    "        # and dropout BEFORE you apply the activation function!!\n",
    "        hn = h8\n",
    "        y = torch_func.sigmoid(hn)\n",
    "        \"\"\"\n",
    "        y = None\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class which formats the spectrogram data in the way needed for convolutional neural network training\n",
    "class TagSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list):\n",
    "        \"\"\"\n",
    "        Create spectrogram based drum dataset for CNN training\n",
    "        :param feat_list: list with spectrograms (np.array) for individual tracks\n",
    "        :param targ_list: list with targets (np.array) for individual tracks\n",
    "        \"\"\"\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.length = len(self.features)\n",
    "        super(TagSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a snipped by index, from the whole dataset\n",
    "        :param index: index of the snipped to be returned\n",
    "        :return: a snipped for CNN training\n",
    "        \"\"\"\n",
    "        # convert to PyTorch tensor and return\n",
    "        #return torch.from_numpy(self.features[index]).unsqueeze_(0), torch.from_numpy(self.targets[index])\n",
    "        return torch.from_numpy(self.features[index]).squeeze_(0), torch.from_numpy(self.targets[index])\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn drum transcription experiment\n",
    "def cnn():\n",
    "    print('Training CNN...')\n",
    "\n",
    "    args = Args()\n",
    "    args.batch_size = 64\n",
    "    args.no_cuda = not g_use_cuda\n",
    "\n",
    "    # setup pytorch\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(TagSet(training_features, training_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(TagSet(validation_features, validation_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(TagSet(test_features, test_targets),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # f, t = iter(test_loader).next()\n",
    "    # print(f.shape)\n",
    "    # print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
