{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 3 Experiment!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2 imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# filters, might be useful for separate and detect\n",
    "from scipy.signal import butter, freqz\n",
    "from scipy.ndimage.filters import maximum_filter, uniform_filter\n",
    "\n",
    "# classifier for segment and classify method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.audio.spectrogram import LogarithmicFilteredSpectrogram\n",
    "from madmom.audio import Signal\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "from madmom.evaluation import OnsetEvaluation, OnsetSumEvaluation\n",
    "from madmom.features import CNNOnsetProcessor\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "\"\"\"\n",
    "# plotting library for visualization for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'pgf.rcfonts': False})\n",
    "\n",
    "COLAB_DRIVE_BASE = \"/content/g-drive\"\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if in colab, mount gdrive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  print('trying to mount google drive...')\n",
    "  drive.mount(COLAB_DRIVE_BASE, force_remount=True)\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "# some global parameter settings we will need along the way\n",
    "#\n",
    "EPSILON = np.finfo(np.float32).eps  # small epsilon needed sometimes for computational stability (div by zeros)\n",
    "\n",
    "\"\"\"\n",
    "SETTINGS = {  # settings for spectrogram (feature) calculation\n",
    "    'fps': 100,  # frames per second of our resulting spectrograms\n",
    "    'fmin': 30,  # minimum frequency\n",
    "    'fmax': 15000,  # maximum frequency of spectrogram\n",
    "    'frame_size': 2048,  # frame size for spectrogram\n",
    "    'sample_rate': 44100,  # input sample rate - input audio will be resampled to this sample rate.\n",
    "    'num_bands': 12,  # bands per octave (freq. factor 2)\n",
    "    'num_channels': 1,  # input audio will be converted to mono\n",
    "    'norm_filters': True,  # normalize triangular filters for log/log spectrogram to have equal area\n",
    "}\n",
    "\n",
    "# drum label names\n",
    "# all arrays and lists containing instruments will always follow this index system, 0:KD (kick/bass drum),\n",
    "# 1:SD (snare drum), 2: HH (hi-hat).\n",
    "names_3_map = ['KD', 'SD', 'HH']\n",
    "num_3_drum_notes = len(names_3_map)\n",
    "\"\"\"\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "if IN_COLAB:\n",
    "  PATH = os.path.join(COLAB_DRIVE_BASE, 'My Drive/Colab Notebooks')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DATA_PATH = os.path.join(PATH, 'data/drums_simple')  # change this value if you copied the dataset somewhere else!\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'annotations')\n",
    "SAMPLE_ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'sample_annotations')\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, 'samples')\n",
    "CACHE_PATH = os.path.join(DATA_PATH, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "MODEL_PATH = os.path.join(DATA_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# some info about our data\n",
    "NUM_KITS = 4  # we have 4 different drum kits\n",
    "NUM_TRACKS = 4  # and 4 tracks per kit\n",
    "FPS = SETTINGS['fps']  # shorthand to the FPS we use for our spectrogram\n",
    "RANK = num_3_drum_notes  # we use three instruments\n",
    "\n",
    "# turn on / off plotting (for debugging)\n",
    "plot = False\n",
    "plot_len = 400\n",
    "\"\"\"\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 12345\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "######## ADJUST DATA PATHS ACCORDING TO YOUR LOCAL CONFIGURATION ########\n",
    "DATA_PATH_1 = os.path.join(PATH, 'data/part_1')\n",
    "AUDIO_PATH_1 = os.path.join(DATA_PATH_1, 'mp3.zip')\n",
    "ANNOTATIONS_PATH_1 = os.path.join(DATA_PATH_1, 'annotations_final.csv')\n",
    "META_DATA_PATH_1 = os.path.join(DATA_PATH_1, 'clip_info_final.csv')\n",
    "\n",
    "CACHE_PATH_1 = os.path.join(DATA_PATH_1, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH_1):\n",
    "    os.makedirs(CACHE_PATH_1)\n",
    "MODEL_PATH_1 = os.path.join(DATA_PATH_1, 'models')\n",
    "if not os.path.exists(MODEL_PATH_1):\n",
    "    os.makedirs(MODEL_PATH_1)  \n",
    "    \n",
    "CNN_MODEL_NAME = 'cnn_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = np.vectorize(lambda v : v.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio, annotations and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = search_files(AUDIO_PATH_1, '.mp3', recursion_depth=1)\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "# librosa cant load these files for some reason\n",
    "# norine_braun-now_and_zen-08-gently-117-146.mp3\n",
    "del audio_files[10687]\n",
    "# jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3\n",
    "del audio_files[12821]\n",
    "# american_baroque-dances_and_suites_of_rameau_and_couperin-26-loracle_suite_in_d_from_les_fetes_dhebe_rameau-0-29\n",
    "del audio_files[13701]\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "annotations = np.genfromtxt(ANNOTATIONS_PATH_1, dtype=str, delimiter='\\t')\n",
    "meta_data = np.genfromtxt(META_DATA_PATH_1, dtype=str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogMelSpectrogram from Music Auto Tagging (+ caching from Ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        # src = src[(n_sample-n_sample_fit)/2:(n_sample+n_sample_fit)/2]\n",
    "        src = src[int((n_sample-n_sample_fit)/2):int((n_sample+n_sample_fit)/2)]\n",
    "        \n",
    "    #logam = librosa.logamplitude\n",
    "    logam = librosa.core.power_to_db\n",
    "    \n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref_power=1.0)\n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS))\n",
    "    \n",
    "    \"\"\"\n",
    "    stft = librosa.core.stft(y=src, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=N_FFT, n_mels=N_MELS)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    ret = db_mel_spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret\n",
    "\n",
    "def init_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH_1, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "        else:\n",
    "            feat = compute_melgram(audio_file)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "        feature_list.append(feat)\n",
    "        if len(feature_list)%5000 == 0:\n",
    "            print('computed', len(feature_list), 'features...')\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = init_features(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep 50 top tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_50_tags(annotations):\n",
    "    \"\"\"\n",
    "    returns annotations filtered by top 50 most frequent tags\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    anno_values = anno[1:, 1:len(anno[0])-1]\n",
    "    anno_int = np.asarray(replace(anno_values), dtype=int)\n",
    "    anno_sum = anno_int.sum(axis=0)\n",
    "    anno_sorted = np.sort(anno_sum)[::-1]\n",
    "    smallest_tag_value = anno_sorted[49]\n",
    "    \n",
    "    tag_indices = np.where(anno_sum >= smallest_tag_value)\n",
    "    tag_array = [i+1 for i in tag_indices[0]]\n",
    "    cols = [0] + tag_array\n",
    "    cols = cols + [len(annotations[0])-1]\n",
    "    \n",
    "    return anno[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = filter_top_50_tags(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Track Title Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_title_dictionary(meta_data):\n",
    "    meta = meta_data.copy()\n",
    "    \n",
    "    filtered_meta = meta[1:, [2,9]]\n",
    "    clean_meta = np.asarray(replace(filtered_meta))\n",
    "\n",
    "    meta_dict = {}\n",
    "    for i, d in enumerate(clean_meta):\n",
    "        meta_dict[d[1]] = d[0]\n",
    "    \n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = compute_title_dictionary(meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Validation / Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_audio(audio_files):\n",
    "    grouped_audio = []\n",
    "    same_track = []\n",
    "    for i, a in enumerate(audio_files):\n",
    "        if i == 0:\n",
    "            same_track.append(a)\n",
    "        else:\n",
    "            previous_title = meta_dict[audio_files[i-1].split(AUDIO_PATH_1+'/')[1]]\n",
    "            current_title = meta_dict[audio_files[i].split(AUDIO_PATH_1+'/')[1]]\n",
    "            if previous_title == current_title:\n",
    "                same_track.append(a)\n",
    "            else:\n",
    "                grouped_audio.append(same_track)\n",
    "                same_track = []\n",
    "                same_track.append(a)\n",
    "\n",
    "    grouped_audio.append(same_track)\n",
    "    return grouped_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_audio = group_audio(audio_files)\n",
    "#grouped_audio[100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
