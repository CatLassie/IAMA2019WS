{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 3 Experiment!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2 imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# filters, might be useful for separate and detect\n",
    "from scipy.signal import butter, freqz\n",
    "from scipy.ndimage.filters import maximum_filter, uniform_filter\n",
    "\n",
    "# classifier for segment and classify method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.audio.spectrogram import LogarithmicFilteredSpectrogram\n",
    "from madmom.audio import Signal\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "from madmom.evaluation import OnsetEvaluation, OnsetSumEvaluation\n",
    "from madmom.features import CNNOnsetProcessor\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "\"\"\"\n",
    "# plotting library for visualization for debugging\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'pgf.rcfonts': False})\n",
    "\n",
    "COLAB_DRIVE_BASE = \"/content/g-drive\"\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# if in colab, mount gdrive\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  print('trying to mount google drive...')\n",
    "  drive.mount(COLAB_DRIVE_BASE, force_remount=True)\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "# some global parameter settings we will need along the way\n",
    "#\n",
    "EPSILON = np.finfo(np.float32).eps  # small epsilon needed sometimes for computational stability (div by zeros)\n",
    "\n",
    "\"\"\"\n",
    "SETTINGS = {  # settings for spectrogram (feature) calculation\n",
    "    'fps': 100,  # frames per second of our resulting spectrograms\n",
    "    'fmin': 30,  # minimum frequency\n",
    "    'fmax': 15000,  # maximum frequency of spectrogram\n",
    "    'frame_size': 2048,  # frame size for spectrogram\n",
    "    'sample_rate': 44100,  # input sample rate - input audio will be resampled to this sample rate.\n",
    "    'num_bands': 12,  # bands per octave (freq. factor 2)\n",
    "    'num_channels': 1,  # input audio will be converted to mono\n",
    "    'norm_filters': True,  # normalize triangular filters for log/log spectrogram to have equal area\n",
    "}\n",
    "\n",
    "# drum label names\n",
    "# all arrays and lists containing instruments will always follow this index system, 0:KD (kick/bass drum),\n",
    "# 1:SD (snare drum), 2: HH (hi-hat).\n",
    "names_3_map = ['KD', 'SD', 'HH']\n",
    "num_3_drum_notes = len(names_3_map)\n",
    "\"\"\"\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "if IN_COLAB:\n",
    "  PATH = os.path.join(COLAB_DRIVE_BASE, 'My Drive/Colab Notebooks')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DATA_PATH = os.path.join(PATH, 'data/drums_simple')  # change this value if you copied the dataset somewhere else!\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'annotations')\n",
    "SAMPLE_ANNOTATIONS_PATH = os.path.join(DATA_PATH, 'sample_annotations')\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, 'samples')\n",
    "CACHE_PATH = os.path.join(DATA_PATH, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.makedirs(CACHE_PATH)\n",
    "MODEL_PATH = os.path.join(DATA_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# some info about our data\n",
    "NUM_KITS = 4  # we have 4 different drum kits\n",
    "NUM_TRACKS = 4  # and 4 tracks per kit\n",
    "FPS = SETTINGS['fps']  # shorthand to the FPS we use for our spectrogram\n",
    "RANK = num_3_drum_notes  # we use three instruments\n",
    "\n",
    "# turn on / off plotting (for debugging)\n",
    "plot = False\n",
    "plot_len = 400\n",
    "\"\"\"\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 1234 #12345\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "######## ADJUST DATA PATHS ACCORDING TO YOUR LOCAL CONFIGURATION ########\n",
    "DATA_PATH_1 = os.path.join(PATH, 'data/part_1')\n",
    "AUDIO_PATH_1 = os.path.join(DATA_PATH_1, 'mp3.zip')\n",
    "ANNOTATIONS_PATH_1 = os.path.join(DATA_PATH_1, 'annotations_final.csv')\n",
    "META_DATA_PATH_1 = os.path.join(DATA_PATH_1, 'clip_info_final.csv')\n",
    "\n",
    "CACHE_PATH_1 = os.path.join(DATA_PATH_1, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH_1):\n",
    "    os.makedirs(CACHE_PATH_1)\n",
    "MODEL_PATH_1 = os.path.join(DATA_PATH_1, 'models')\n",
    "if not os.path.exists(MODEL_PATH_1):\n",
    "    os.makedirs(MODEL_PATH_1)  \n",
    "    \n",
    "CNN_MODEL_NAME = 'cnn_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "replace = np.vectorize(lambda v : v.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio, annotations and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = search_files(AUDIO_PATH_1, '.mp3', recursion_depth=1)\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "# librosa cant load these files for some reason\n",
    "# norine_braun-now_and_zen-08-gently-117-146.mp3\n",
    "del audio_files[10687]\n",
    "# jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3\n",
    "del audio_files[12821]\n",
    "# american_baroque-dances_and_suites_of_rameau_and_couperin-26-loracle_suite_in_d_from_les_fetes_dhebe_rameau-0-29\n",
    "del audio_files[13701]\n",
    "\n",
    "#print(len(audio_files))\n",
    "\n",
    "annotations = np.genfromtxt(ANNOTATIONS_PATH_1, dtype=str, delimiter='\\t')\n",
    "meta_data = np.genfromtxt(META_DATA_PATH_1, dtype=str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogMelSpectrogram from Music Auto Tagging (+ caching from Ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        # src = src[(n_sample-n_sample_fit)/2:(n_sample+n_sample_fit)/2]\n",
    "        src = src[int((n_sample-n_sample_fit)/2):int((n_sample+n_sample_fit)/2)]\n",
    "        \n",
    "    #logam = librosa.logamplitude\n",
    "    logam = librosa.core.power_to_db\n",
    "    \n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref_power=1.0)\n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS))\n",
    "    \n",
    "    \"\"\"\n",
    "    stft = librosa.core.stft(y=src, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=N_FFT, n_mels=N_MELS)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    ret = db_mel_spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret\n",
    "\n",
    "def init_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH_1, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "        else:\n",
    "            feat = compute_melgram(audio_file)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "        feature_list.append(feat)\n",
    "        if len(feature_list)%5000 == 0:\n",
    "            print('computed', len(feature_list), 'features...')\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep 50 top tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_50_tags(annotations):\n",
    "    \"\"\"\n",
    "    returns annotations filtered by top 50 most frequent tags\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    anno_values = anno[1:, 1:len(anno[0])-1]\n",
    "    anno_int = np.asarray(replace(anno_values), dtype=int)\n",
    "    anno_sum = anno_int.sum(axis=0)\n",
    "    anno_sorted = np.sort(anno_sum)[::-1]\n",
    "    smallest_tag_value = anno_sorted[49]\n",
    "    \n",
    "    tag_indices = np.where(anno_sum >= smallest_tag_value)\n",
    "    tag_array = [i+1 for i in tag_indices[0]]\n",
    "    cols = [0] + tag_array\n",
    "    cols = cols + [len(annotations[0])-1]\n",
    "    \n",
    "    return anno[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = filter_top_50_tags(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Validation / Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_title_dictionary(meta_data):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> track title\n",
    "    \"\"\"\n",
    "    meta = meta_data.copy()\n",
    "    \n",
    "    filtered_meta = meta[1:, [2,9]]\n",
    "    clean_meta = np.asarray(replace(filtered_meta))\n",
    "\n",
    "    meta_dict = {}\n",
    "    for i, d in enumerate(clean_meta):\n",
    "        meta_dict[d[1]] = d[0]\n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "def compute_target_dictionary(annotations):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> list of annotations\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    filtered_anno = anno[1:, 1:]\n",
    "    clean_anno = np.asarray(replace(filtered_anno))\n",
    "\n",
    "    target_dict = {}\n",
    "    for i, d in enumerate(clean_anno):\n",
    "        target_dict[d[50]] = d[:50].astype(np.float32)\n",
    "    \n",
    "    return target_dict\n",
    "\n",
    "def group_audio(audio_files, meta_dict):\n",
    "    \"\"\"\n",
    "    returns audio grouped by track title (based on dictionary)\n",
    "    \"\"\"\n",
    "    grouped_audio = []\n",
    "    same_track = []\n",
    "    for i, a in enumerate(audio_files):\n",
    "        if i == 0:\n",
    "            same_track.append(a)\n",
    "        else:\n",
    "            previous_title = meta_dict[audio_files[i-1].split(AUDIO_PATH_1+'/')[1]]\n",
    "            current_title = meta_dict[audio_files[i].split(AUDIO_PATH_1+'/')[1]]\n",
    "            if previous_title == current_title:\n",
    "                same_track.append(a)\n",
    "            else:\n",
    "                grouped_audio.append(same_track)\n",
    "                same_track = []\n",
    "                same_track.append(a)\n",
    "\n",
    "    grouped_audio.append(same_track)\n",
    "    return grouped_audio\n",
    "\n",
    "def shuffle_and_split_files(grouped_audio):\n",
    "    \"\"\"\n",
    "    returns approx. 50% as training, 25% as validation, 25% as test data (randomly shuffled)\n",
    "    \"\"\"\n",
    "    half_idx = int(len(grouped_audio)/2)\n",
    "    three_quarter_idx = int(half_idx/2) + half_idx\n",
    "    \n",
    "    grouped_audio_shuffled = random.Random(seed).sample(grouped_audio, len(grouped_audio))\n",
    "\n",
    "    training_audio = grouped_audio_shuffled[:half_idx]\n",
    "    validation_audio = grouped_audio_shuffled[half_idx:three_quarter_idx]\n",
    "    test_audio = grouped_audio_shuffled[three_quarter_idx:]\n",
    "\n",
    "    training_audio = [item for sublist in training_audio for item in sublist]\n",
    "    validation_audio = [item for sublist in validation_audio for item in sublist]\n",
    "    test_audio = [item for sublist in test_audio for item in sublist]\n",
    "    \n",
    "    return np.array(training_audio), np.array(validation_audio), np.array(test_audio)\n",
    "\n",
    "def init_targets(audio_files, target_dict):\n",
    "    targets = list(map(lambda v : target_dict[v.split(AUDIO_PATH_1+'/')[1]], audio_files))\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = compute_title_dictionary(meta_data)\n",
    "target_dict = compute_target_dictionary(top_annotations)\n",
    "grouped_audio = group_audio(audio_files, meta_dict)\n",
    "training_audio, validation_audio, test_audio = shuffle_and_split_files(grouped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = init_features(training_audio)\n",
    "val_feat = init_features(validation_audio)\n",
    "test_feat = init_features(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targ = init_targets(training_audio, target_dict)\n",
    "val_targ = init_targets(validation_audio, target_dict)\n",
    "test_targ = init_targets(test_audio, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use step size to configure amount of data\n",
    "\n",
    "step_size = 48\n",
    "training_features = train_feat[0::step_size]\n",
    "validation_features = val_feat[0::step_size]\n",
    "test_features = test_feat[0::step_size]\n",
    "\n",
    "training_targets = train_targ[0::step_size]\n",
    "validation_targets = val_targ[0::step_size]\n",
    "test_targets = test_targ[0::step_size]\n",
    "\"\"\"\n",
    "print(len(training_features))\n",
    "print(len(validation_features))\n",
    "print(len(test_features))\n",
    "\n",
    "print(len(training_targets))\n",
    "print(len(validation_targets))\n",
    "print(len(test_targets))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger base class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #\n",
    "        # In this constructor, create the layers needed to build the network.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(128)\n",
    "        self.mp1 = nn.MaxPool2d((2,4), stride=(2,4))\n",
    "        self.drop1 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 384, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(384)\n",
    "        self.mp2 = nn.MaxPool2d((4,5), stride=(4,5))\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(384, 768, kernel_size=3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(768)\n",
    "        self.mp3 = nn.MaxPool2d((3,8), stride=(3,8))\n",
    "        self.drop3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(768, 2048, kernel_size=3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(2048)\n",
    "        self.mp4 = nn.MaxPool2d((4,8), stride=(4,8))\n",
    "        self.drop4 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.lin1 = nn.Linear(2048, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function calculates a forward pass through the network (i.e. calculates the output for given input x).\n",
    "        # Hand x through the layers of the network and calculate the output.\n",
    "\n",
    "        h1 = torch_func.relu(self.conv1_bn(self.conv1(x)))\n",
    "        h2 = self.drop1(self.mp1(h1))\n",
    "        h3 = torch_func.relu(self.conv2_bn(self.conv2(h2)))\n",
    "        h4 = self.drop2(self.mp2(h3))\n",
    "        h5 = torch_func.relu(self.conv3_bn(self.conv3(h4)))\n",
    "        h6 = self.drop3(self.mp3(h5))\n",
    "        h7 = torch_func.relu(self.conv4_bn(self.conv4(h6)))\n",
    "        h8 = self.drop4(self.mp4(h7))\n",
    "        h8 = h8.reshape(h8.size(0), -1)\n",
    "        y = torch_func.sigmoid(self.lin1(h8))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for one epoch of NN training.\n",
    "    Within one epoch, all the data is used once, we use mini-batch gradient descent.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be trained\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param train_loader: Data provider\n",
    "    :param optimizer: Optimizer (Gradient descent update algorithm)\n",
    "    :param epoch: Current epoch number\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout layers for example).\n",
    "    model.train()\n",
    "    # we measure the needed time\n",
    "    t = time.time()\n",
    "    # iterate over training data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device (GPU) if necessary\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data.float())\n",
    "        # calculate loss\n",
    "        loss = torch_func.binary_cross_entropy(output, target)\n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using our optimizer\n",
    "        optimizer.step()\n",
    "        # print some outputs if we reached our logging intervall\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "       \n",
    "def test_nn(args, model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Function wich iterates over test data (eval or test set) and calculates loss.\n",
    "    Here no parameter update is done\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be tested\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param test_loader: Data provider\n",
    "    :return: cumulative test loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    test_loss = 0\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in test_loader:\n",
    "            # move data to device (GPU) if necessasry\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data.float())\n",
    "            # claculate loss and add it to our cumulative loss\n",
    "            test_loss += torch_func.binary_cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        test_loss, len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss\n",
    "  \n",
    "\n",
    "    \n",
    "def inference_cnn(model, device, data):\n",
    "    \"\"\"\n",
    "    Function calculating the actual output of the network, given some input.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The network to be used\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param data: Data for which the output should be calculated\n",
    "    :return: output of network\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    output = None\n",
    "    # move input to device if necessary\n",
    "    data = torch.from_numpy(data)\n",
    "    data = data.to(device)\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        output = model(data.float())\n",
    "    return output\n",
    "\n",
    "\n",
    "    \n",
    "# class which formats the spectrogram data in the way needed for convolutional neural network training\n",
    "class TagSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list):\n",
    "        \"\"\"\n",
    "        Create spectrogram based drum dataset for CNN training\n",
    "        :param feat_list: list with spectrograms (np.array) for individual tracks\n",
    "        :param targ_list: list with targets (np.array) for individual tracks\n",
    "        \"\"\"\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.length = len(self.features)\n",
    "        super(TagSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a snipped by index, from the whole dataset\n",
    "        :param index: index of the snipped to be returned\n",
    "        :return: a snipped for CNN training\n",
    "        \"\"\"\n",
    "        # convert to PyTorch tensor and return\n",
    "        #return torch.from_numpy(self.features[index]).unsqueeze_(0), torch.from_numpy(self.targets[index])\n",
    "        return torch.from_numpy(self.features[index]).squeeze_(0), torch.from_numpy(self.targets[index])\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn tagging experiment\n",
    "def cnn():\n",
    "    print('Training CNN...')\n",
    "\n",
    "    # parameters for NN training\n",
    "    args = Args()\n",
    "    args.batch_size = 64\n",
    "    args.max_epochs = 1000\n",
    "    args.patience = 4\n",
    "    args.lr = 0.01 # 0.001, 0.0001\n",
    "    args.momentum = 0.5\n",
    "    args.no_cuda = not g_use_cuda\n",
    "    args.seed = 1\n",
    "    args.log_interval = 100\n",
    "\n",
    "    # setup pytorch\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    # create model and optimizer, we use plain SGD with momentum\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr) # not SGD+momentum!!!\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(TagSet(training_features, training_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(TagSet(validation_features, validation_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(TagSet(test_features, test_targets),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_test_loss = 9999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_nn(args, model, device, train_loader, optimizer, epoch)\n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        test_loss = test_nn(args, model, device, valid_loader)\n",
    "        # check for early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH_1, CNN_MODEL_NAME + '.model'))\n",
    "            best_test_loss = test_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate CNN...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    test_nn(args, model, device, test_loader)\n",
    "    \n",
    "    # calculate actual output for the test data\n",
    "    results_cnn = [None for _ in range(len(test_features))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_features):\n",
    "        # run the inference method\n",
    "        result = inference_cnn(model, device, cur_test_feat)\n",
    "        results_cnn[test_idx] = result.numpy()[0]\n",
    "\n",
    "    return results_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall results and print output    \n",
    "score = roc_auc_score(test_targets, results)\n",
    "print('Tagger ROC AUC score is:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
