{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Peter Rjabcsenko\"\n",
    "STUDENT_ID = \"1228563\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2 imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import everything we will need first...\n",
    "# some generic stuff, numpy will help us with math!\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# madmom audio processing stuff and evaluation\n",
    "import madmom\n",
    "from madmom.utils import search_files\n",
    "\n",
    "# pytorch, deep learning library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "\n",
    "# paths to our small example dataset\n",
    "PATH = os.getcwd()\n",
    "\n",
    "# use GPU for NN training?\n",
    "g_use_cuda = True\n",
    "\n",
    "# seed for RNG for reproducible results\n",
    "seed = 1234 #12345\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra imports, constants and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "######## ADJUST DATA PATHS ACCORDING TO YOUR LOCAL CONFIGURATION ########\n",
    "DATA_PATH_1 = os.path.join(PATH, 'data/part_1')\n",
    "AUDIO_PATH_1 = os.path.join(DATA_PATH_1, 'mp3.zip')\n",
    "ANNOTATIONS_PATH_1 = os.path.join(DATA_PATH_1, 'annotations_final.csv')\n",
    "META_DATA_PATH_1 = os.path.join(DATA_PATH_1, 'clip_info_final.csv')\n",
    "\n",
    "CACHE_PATH_1 = os.path.join(DATA_PATH_1, 'feat_cache')\n",
    "if not os.path.exists(CACHE_PATH_1):\n",
    "    os.makedirs(CACHE_PATH_1)\n",
    "MODEL_PATH_1 = os.path.join(DATA_PATH_1, 'models')\n",
    "if not os.path.exists(MODEL_PATH_1):\n",
    "    os.makedirs(MODEL_PATH_1)  \n",
    "    \n",
    "CNN_MODEL_NAME = 'cnn_model'\n",
    "\n",
    "# function for formatting input data\n",
    "replace = np.vectorize(lambda v : v.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio, annotations and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = search_files(AUDIO_PATH_1, '.mp3', recursion_depth=1)\n",
    "\n",
    "# librosa cant load these files for some reason\n",
    "# norine_braun-now_and_zen-08-gently-117-146.mp3\n",
    "del audio_files[10687]\n",
    "# jacob_heringman-josquin_des_prez_lute_settings-19-gintzler__pater_noster-204-233.mp3\n",
    "del audio_files[12821]\n",
    "# american_baroque-dances_and_suites_of_rameau_and_couperin-26-loracle_suite_in_d_from_les_fetes_dhebe_rameau-0-29\n",
    "del audio_files[13701]\n",
    "\n",
    "annotations = np.genfromtxt(ANNOTATIONS_PATH_1, dtype=str, delimiter='\\t')\n",
    "meta_data = np.genfromtxt(META_DATA_PATH_1, dtype=str, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogMelSpectrogram from Music Auto Tagging with some adjustments (+ caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        # src = src[(n_sample-n_sample_fit)/2:(n_sample+n_sample_fit)/2]\n",
    "        src = src[int((n_sample-n_sample_fit)/2):int((n_sample+n_sample_fit)/2)]\n",
    "        \n",
    "    #logam = librosa.logamplitude\n",
    "    logam = librosa.core.power_to_db\n",
    "    \n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref_power=1.0)\n",
    "    \"\"\"\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS))\n",
    "        \n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret\n",
    "\n",
    "def init_features(files, cache=True, cache_ext='.cache.npy', **kwargs):\n",
    "    \"\"\"\n",
    "    Create features for given audio files or load them from cache.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        List with audio file names.\n",
    "    cache : bool, optional\n",
    "        Cache features or use cached ones if available.\n",
    "    cache_ext : str, optional\n",
    "        Extension used for caching.\n",
    "    kwargs : dict, optional\n",
    "        Additional arguments passed for feature computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_list : list\n",
    "        List containing the computed/loaded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feature_list = []\n",
    "    for audio_file in files:\n",
    "        file_path, file_name = os.path.split(audio_file)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        cache_file = os.path.join(CACHE_PATH_1, file_base + cache_ext)\n",
    "        if cache and os.path.exists(cache_file):\n",
    "            feat = np.load(cache_file)\n",
    "        else:\n",
    "            feat = compute_melgram(audio_file)\n",
    "            if cache:\n",
    "                np.save(cache_file, feat)\n",
    "        feature_list.append(feat)\n",
    "        if len(feature_list)%5000 == 0:\n",
    "            print('computed', len(feature_list), 'features...')\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep 50 top tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_50_tags(annotations):\n",
    "    \"\"\"\n",
    "    returns annotations filtered by top 50 most frequent tags\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    anno_values = anno[1:, 1:len(anno[0])-1]\n",
    "    anno_int = np.asarray(replace(anno_values), dtype=int)\n",
    "    anno_sum = anno_int.sum(axis=0)\n",
    "    anno_sorted = np.sort(anno_sum)[::-1]\n",
    "    smallest_tag_value = anno_sorted[49]\n",
    "    \n",
    "    tag_indices = np.where(anno_sum >= smallest_tag_value)\n",
    "    tag_array = [i+1 for i in tag_indices[0]]\n",
    "    cols = [0] + tag_array\n",
    "    cols = cols + [len(annotations[0])-1]\n",
    "    \n",
    "    return anno[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = filter_top_50_tags(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train / Validation / Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_title_dictionary(meta_data):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> track title\n",
    "    \"\"\"\n",
    "    meta = meta_data.copy()\n",
    "    \n",
    "    filtered_meta = meta[1:, [2,9]]\n",
    "    clean_meta = np.asarray(replace(filtered_meta))\n",
    "\n",
    "    meta_dict = {}\n",
    "    for i, d in enumerate(clean_meta):\n",
    "        meta_dict[d[1]] = d[0]\n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "def compute_target_dictionary(annotations):\n",
    "    \"\"\"\n",
    "    returns dictionary: audio file name -> list of annotations\n",
    "    \"\"\"\n",
    "    anno = annotations.copy()\n",
    "    \n",
    "    filtered_anno = anno[1:, 1:]\n",
    "    clean_anno = np.asarray(replace(filtered_anno))\n",
    "\n",
    "    target_dict = {}\n",
    "    for i, d in enumerate(clean_anno):\n",
    "        target_dict[d[50]] = d[:50].astype(np.float32)\n",
    "    \n",
    "    return target_dict\n",
    "\n",
    "def group_audio(audio_files, meta_dict):\n",
    "    \"\"\"\n",
    "    returns audio grouped by track title (based on dictionary)\n",
    "    \"\"\"\n",
    "    grouped_audio = []\n",
    "    same_track = []\n",
    "    for i, a in enumerate(audio_files):\n",
    "        if i == 0:\n",
    "            same_track.append(a)\n",
    "        else:\n",
    "            previous_title = meta_dict[audio_files[i-1].split(AUDIO_PATH_1+'/')[1]]\n",
    "            current_title = meta_dict[audio_files[i].split(AUDIO_PATH_1+'/')[1]]\n",
    "            if previous_title == current_title:\n",
    "                same_track.append(a)\n",
    "            else:\n",
    "                grouped_audio.append(same_track)\n",
    "                same_track = []\n",
    "                same_track.append(a)\n",
    "\n",
    "    grouped_audio.append(same_track)\n",
    "    return grouped_audio\n",
    "\n",
    "def shuffle_and_split_files(grouped_audio):\n",
    "    \"\"\"\n",
    "    returns approx. 50% as training, 25% as validation, 25% as test data (randomly shuffled)\n",
    "    \"\"\"\n",
    "    half_idx = int(len(grouped_audio)/2)\n",
    "    three_quarter_idx = int(half_idx/2) + half_idx\n",
    "    \n",
    "    grouped_audio_shuffled = random.Random(seed).sample(grouped_audio, len(grouped_audio))\n",
    "\n",
    "    training_audio = grouped_audio_shuffled[:half_idx]\n",
    "    validation_audio = grouped_audio_shuffled[half_idx:three_quarter_idx]\n",
    "    test_audio = grouped_audio_shuffled[three_quarter_idx:]\n",
    "\n",
    "    training_audio = [item for sublist in training_audio for item in sublist]\n",
    "    validation_audio = [item for sublist in validation_audio for item in sublist]\n",
    "    test_audio = [item for sublist in test_audio for item in sublist]\n",
    "    \n",
    "    return np.array(training_audio), np.array(validation_audio), np.array(test_audio)\n",
    "\n",
    "def init_targets(audio_files, target_dict):\n",
    "    \"\"\"\n",
    "    returns targtes based on shuffled audio\n",
    "    \"\"\"\n",
    "    targets = list(map(lambda v : target_dict[v.split(AUDIO_PATH_1+'/')[1]], audio_files))\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = compute_title_dictionary(meta_data)\n",
    "target_dict = compute_target_dictionary(top_annotations)\n",
    "grouped_audio = group_audio(audio_files, meta_dict)\n",
    "training_audio, validation_audio, test_audio = shuffle_and_split_files(grouped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = init_features(training_audio)\n",
    "val_feat = init_features(validation_audio)\n",
    "test_feat = init_features(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targ = init_targets(training_audio, target_dict)\n",
    "val_targ = init_targets(validation_audio, target_dict)\n",
    "test_targ = init_targets(test_audio, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and targets used for CNN training and testing\n",
    "\n",
    "# use step size to configure amount of data used by the CNN\n",
    "step_size = 48\n",
    "training_features = train_feat[0::step_size]\n",
    "validation_features = val_feat[0::step_size]\n",
    "test_features = test_feat[0::step_size]\n",
    "\n",
    "training_targets = train_targ[0::step_size]\n",
    "validation_targets = val_targ[0::step_size]\n",
    "test_targets = test_targ[0::step_size]\n",
    "\n",
    "# for testing on all data\n",
    "test_features_all = test_feat\n",
    "test_targets_all = test_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model, helper functions and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger base class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #\n",
    "        # In this constructor, create the layers needed to build the network.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(128)\n",
    "        self.mp1 = nn.MaxPool2d((2,4), stride=(2,4))\n",
    "        self.drop1 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 384, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(384)\n",
    "        self.mp2 = nn.MaxPool2d((4,5), stride=(4,5))\n",
    "        self.drop2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(384, 768, kernel_size=3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(768)\n",
    "        self.mp3 = nn.MaxPool2d((3,8), stride=(3,8))\n",
    "        self.drop3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(768, 2048, kernel_size=3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(2048)\n",
    "        self.mp4 = nn.MaxPool2d((4,8), stride=(4,8))\n",
    "        self.drop4 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.lin1 = nn.Linear(2048, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function calculates a forward pass through the network (i.e. calculates the output for given input x).\n",
    "        # Hand x through the layers of the network and calculate the output.\n",
    "\n",
    "        h1 = torch_func.relu(self.conv1_bn(self.conv1(x)))\n",
    "        h2 = self.drop1(self.mp1(h1))\n",
    "        h3 = torch_func.relu(self.conv2_bn(self.conv2(h2)))\n",
    "        h4 = self.drop2(self.mp2(h3))\n",
    "        h5 = torch_func.relu(self.conv3_bn(self.conv3(h4)))\n",
    "        h6 = self.drop3(self.mp3(h5))\n",
    "        h7 = torch_func.relu(self.conv4_bn(self.conv4(h6)))\n",
    "        h8 = self.drop4(self.mp4(h7))\n",
    "        h8 = h8.reshape(h8.size(0), -1)\n",
    "        y = torch_func.sigmoid(self.lin1(h8))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training / test / inference functions and data set class for the data loader\n",
    "\n",
    "def train_nn(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for one epoch of NN training.\n",
    "    Within one epoch, all the data is used once, we use mini-batch gradient descent.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be trained\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param train_loader: Data provider\n",
    "    :param optimizer: Optimizer (Gradient descent update algorithm)\n",
    "    :param epoch: Current epoch number\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout layers for example).\n",
    "    model.train()\n",
    "    # we measure the needed time\n",
    "    t = time.time()\n",
    "    # iterate over training data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device (GPU) if necessary\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data.float())\n",
    "        # calculate loss\n",
    "        loss = torch_func.binary_cross_entropy(output, target)\n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using our optimizer\n",
    "        optimizer.step()\n",
    "        # print some outputs if we reached our logging intervall\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "       \n",
    "def test_nn(args, model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Function wich iterates over test data (eval or test set) and calculates loss.\n",
    "    Here no parameter update is done\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The model to be tested\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param test_loader: Data provider\n",
    "    :return: cumulative test loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    test_loss = 0\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in test_loader:\n",
    "            # move data to device (GPU) if necessasry\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data.float())\n",
    "            # claculate loss and add it to our cumulative loss\n",
    "            test_loss += torch_func.binary_cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        test_loss, len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss\n",
    "  \n",
    "\n",
    "    \n",
    "def inference_cnn(model, device, data):\n",
    "    \"\"\"\n",
    "    Function calculating the actual output of the network, given some input.\n",
    "    :param args: NN parameters for training and inference\n",
    "    :param model: The network to be used\n",
    "    :param device: PyTorch device: CPU or GPU\n",
    "    :param data: Data for which the output should be calculated\n",
    "    :return: output of network\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout layers for example).\n",
    "    model.eval()\n",
    "    output = None\n",
    "    # move input to device if necessary\n",
    "    data = torch.from_numpy(data)\n",
    "    data = data.to(device)\n",
    "    # do not calculate gradients since we do not want to do updates\n",
    "    with torch.no_grad():\n",
    "        output = model(data.float())\n",
    "    return output\n",
    "\n",
    "\n",
    "    \n",
    "# class which formats the spectrogram data in the way needed for convolutional neural network training\n",
    "class TagSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list):\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.length = len(self.features)\n",
    "        super(TagSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get 1 feature/target pair\n",
    "        # convert to PyTorch tensor and return\n",
    "        return torch.from_numpy(self.features[index]).squeeze_(0), torch.from_numpy(self.targets[index])\n",
    "\n",
    "    \n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn tagging experiment\n",
    "def cnn():\n",
    "    print('Training CNN...')\n",
    "\n",
    "    # parameters for NN training\n",
    "    args = Args()\n",
    "    args.batch_size = 8 #64\n",
    "    args.max_epochs = 25 #1000\n",
    "    args.patience = 4\n",
    "    args.lr = 0.01 # 0.001, 0.0001\n",
    "    args.momentum = 0.5\n",
    "    args.no_cuda = not g_use_cuda\n",
    "    args.seed = 1\n",
    "    args.log_interval = 10 #100\n",
    "\n",
    "    # setup pytorch\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    # create model and optimizer\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(TagSet(training_features, training_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(TagSet(validation_features, validation_targets),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(TagSet(test_features, test_targets),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_test_loss = 9999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_nn(args, model, device, train_loader, optimizer, epoch)\n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        test_loss = test_nn(args, model, device, valid_loader)\n",
    "        # check for early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH_1, CNN_MODEL_NAME + '.model'))\n",
    "            best_test_loss = test_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate CNN...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    test_nn(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run trainig\n",
    "cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved CNN model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(test_features):\n",
    "    no_cuda = not g_use_cuda\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    # load model\n",
    "    model = Net().to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH_1, CNN_MODEL_NAME + '.model')))\n",
    "    print('model loaded...')\n",
    "    \n",
    "    # calculate actual output for the test data\n",
    "    results_cnn = [None for _ in range(len(test_features))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_features):\n",
    "        if test_idx % 100 == 0:\n",
    "            completion = int((test_idx / len(test_features))*100)\n",
    "            print(str(completion)+'% complete...')\n",
    "        \n",
    "        # run the inference method\n",
    "        result = inference_cnn(model, device, cur_test_feat)\n",
    "        results_cnn[test_idx] = result.numpy()[0]\n",
    "\n",
    "    return results_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROC AUC on limited test set (2%)\n",
    "#### result: 66.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_inference(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(test_targets, results)\n",
    "print('Tagger ROC AUC score on limited test set is:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROC AUC on full test set\n",
    "#### result: 65.45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = run_inference(test_features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_all = roc_auc_score(test_targets_all, results_all)\n",
    "print('Tagger ROC AUC score on full test set is:', score_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "All necessary data used for computations should be located under \"data/part_1\" relative to the jupyter notebook path.\n",
    "\n",
    "The \"annotaions_final.csv\", \"clip_info_final.csv\" and \"mpr3.zip\" folder from the MagnaTagATune dataset (you can adjust the default paths in the second code block).\n",
    "\n",
    "Executing the notebook will additionally create a \"data/part_1/feat_cache\" folder containing the computed spectrograms, which will be loaded if present in the folder instead of being computed from scracth.\n",
    "\n",
    "A \"data/part_1/models\" folder will also be created if the CNN training block is executed which will contain the \"cnn_model.model\" file. Note that the evaluation block always loads the model from the \"models\" folder, so please make sure to either run the training before evaluation (might take up to 2 hours on a regular machine) or alternatively create the folder manually and copy the provided \"cnn_model.model\" file into it.\n",
    "\n",
    "#### Methodology:\n",
    "LogMelSpectrograms were used as features along with the top 50 most frequent tags from the MagnaTagATune dataset as targets for the CNN.\n",
    "\n",
    "The data was randomly split into training, validation and test sets with an approx. 50%/25%/25% ratio with extra care being taken to ensure that no fragments of a single track (same title) end up in different sets.\n",
    "This resulted in the following splits:\n",
    "\n",
    "Training - 12749 features;<br/>\n",
    "Validation - 6448 features;<br/>\n",
    "Test - 6663 features;\n",
    "\n",
    "However since training would have been infeasible due to long training times, it was performed only on a subset of the original data (approx. 2% or 540 features).\n",
    "The reduced splits:\n",
    "\n",
    "Training - 266 features;<br/>\n",
    "Validation - 135 features;<br/>\n",
    "Test - 139 features;\n",
    "\n",
    "Final testing however was performed <b>both</b> on the <b>Full Test Set</b> (6663) and the <b>Reduced Test Set</b> (139).\n",
    "\n",
    "#### Results and Observations:\n",
    "As mentioned above training was conducted on a reduced training set with 1 epoch taking up approx. 5 minutes and the whole training phase lasting 120 minutes (23 epochs) on a MacBook Pro 2017.\n",
    "\n",
    "ROC area under the curve was used to measure CNN performance.\n",
    "\n",
    "The network achieved the following ROC AUC socres:\n",
    "<br/>\n",
    "66.00% on the Reduced Test Set (139 features)\n",
    "<br/>\n",
    "65.45% on the Full Test Set (6663 features)\n",
    "\n",
    "Since the results were fairly similar in both cases and considerably above 50% (which is associated with random guessing), we can conclude that this implementation is quite promising.\n",
    "\n",
    "The obvious next step to improving the CNN would be to actually train it on the whole training split, since it is well known that deep neural networks benefit from large amounts of data. Also maybe utilise some tricks to reduce training time :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
