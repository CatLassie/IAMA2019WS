{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import madmom\n",
    "import librosa\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madmom.utils import search_files, match_file\n",
    "\n",
    "AUDIO_FILES = search_files('data/train', '.flac')\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.flac'):\n",
    "    \"\"\"\n",
    "    Find matching audio files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ann_files : list\n",
    "        List with annotation file names.\n",
    "    audio_files : list\n",
    "        List with audio file names to be matched\n",
    "    ann_suffix : str, optional\n",
    "        Suffix of the annotation files. If 'None'\n",
    "        the suffix is inferred from the annotation\n",
    "        files.\n",
    "    audio_suffix : str, optional\n",
    "        Suffix of the audio files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matched_files : list\n",
    "        List of matched audio file (names).\n",
    "    matched_indices : list\n",
    "        List of matching indices in `audio_files`.\n",
    "        \n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## FIX FOR ABOVE FUNCTION FOR TASK 4 and 5 ########\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.flac'):\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(audio_files.index(matches[0]))\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "# len(AUDIO_FILES)\n",
    "\n",
    "# from scripts import utilities\n",
    "# utilities.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "SR = 44100 # samping rate\n",
    "FRAME_SIZE = 2048 # number of samples per frame\n",
    "HOP_SIZE = int(SR / FPS) # hop size depends on sampling rate and frame rate\n",
    "NUM_BANDS = 40 # number of mel bins\n",
    "\n",
    "def pre_process(filename, frame_size=2048, frame_rate=FPS, num_bands=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Pre-process the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File to be processed.\n",
    "    frame_size : int\n",
    "        Size of the frames.\n",
    "    frame_rate : float\n",
    "        Frame rate used for the STFT.\n",
    "    num_bands : int\n",
    "        Number of frequency bands for the Mel filterbank.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram.\n",
    "\n",
    "    \"\"\"    \n",
    "    # STEP 1: read in audio\n",
    "    signal, sampling_rate_unused = librosa.load(filename, sr=SR) # read file\n",
    "    \n",
    "    # STEP 2,3: compute stft (default windowing function is Hann)\n",
    "    stft = librosa.core.stft(y=signal, n_fft=frame_size, hop_length=HOP_SIZE)\n",
    "    \n",
    "    # STEP 4: discard phase info and square magnitudes\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    \n",
    "    # STEP 5: apply mel scaling\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=frame_size, n_mels=num_bands)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    \n",
    "    # STEP 6: apply DB scaling\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    # double check\n",
    "    # mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=num_bands)\n",
    "    # db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    # print((db_mel_spectrogram)[0])\n",
    "        \n",
    "    spectrogram = db_mel_spectrogram\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "from librosa.display import specshow\n",
    "\n",
    "def test_pre_process():\n",
    "    texasName = AUDIO_FILES[19] #AUDIO_FILES[19]\n",
    "\n",
    "    spectrogram = pre_process(texasName, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "\n",
    "    # print(spectrogram.shape)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    specshow(spectrogram, sr=SR, hop_length=HOP_SIZE, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "# test_pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for collecting pre-processed spectrograms\n",
    "# Note: it is not necessary to use this list but recommended in order to\n",
    "#       avoid recomputation of the same features over and over again.\n",
    "#       *_AUDIO_IDX canbe used to acces the precomputed spectrograms by\n",
    "#       index.\n",
    "SPECTROGRAMS = []\n",
    "\n",
    "for audio_file in AUDIO_FILES:\n",
    "    spec = pre_process(audio_file, FRAME_SIZE, FPS, NUM_BANDS) # params missing in tuwel\n",
    "    SPECTROGRAMS.append(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onset detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "ONSET_ANNOTATION_FILES = search_files('data/train', '.onsets')\n",
    "ONSET_AUDIO_FILES, ONSET_AUDIO_IDX = find_audio_files(ONSET_ANNOTATION_FILES, AUDIO_FILES)\n",
    "ONSET_AUDIO = [SPECTROGRAMS[i] for i in ONSET_AUDIO_IDX]\n",
    "ONSET_ANNOTATIONS = [madmom.io.load_onsets(f) for f in ONSET_ANNOTATION_FILES]\n",
    "\n",
    "assert len(ONSET_ANNOTATION_FILES) == 321\n",
    "assert len(ONSET_AUDIO_FILES) == 321\n",
    "assert len(ONSET_AUDIO) == 321\n",
    "assert len(ONSET_ANNOTATIONS) == 321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection_function(spectrogram):\n",
    "    \"\"\"\n",
    "    Compute an onset detection function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "\n",
    "    \"\"\"\n",
    "    spectrogram_T = spectrogram.transpose()\n",
    "    \n",
    "    odf = []\n",
    "    for i, frame in enumerate(spectrogram_T):\n",
    "        sum = 0\n",
    "        for j, bin in enumerate(frame):\n",
    "            diff = spectrogram_T[i][j] - (spectrogram_T[i-1][j] if i > 0 else 0)\n",
    "            flux = diff if diff >= 0 else 0\n",
    "            sum = sum + flux\n",
    "\n",
    "        odf.append(sum / NUM_BANDS)\n",
    "                    \n",
    "    return odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def odf_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "\n",
    "    odf = onset_detection_function(spec)\n",
    "    # fix the weird librosa offset\n",
    "    #odf = [0.0, 0.0] + odf\n",
    "    #odf.pop()\n",
    "    #odf.pop()\n",
    "\n",
    "    odf_lib = librosa.onset.onset_strength(sr=SR, S=spec)\n",
    "\n",
    "    # print('odf_lib:', odf_lib[6], \" len: \", len(odf_lib))\n",
    "    # print(odf[6])\n",
    "\n",
    "    print(len(odf), \"and\", len(odf_lib))\n",
    "    #for i, elem in enumerate(odf):\n",
    "    #    print(odf[i] == odf_lib[i])\n",
    "# odf_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEFT = 2 # 3 default value\n",
    "MAX_RIGHT = 3 # 1 \n",
    "AVG_LEFT = 10 # 10\n",
    "AVG_RIGHT = 11 # 11\n",
    "MIN_DIST = 3 # 3 (30ms)\n",
    "            # 0.5 is used # 0.07 threshold\n",
    "\n",
    "def detect_onsets(odf, threshold, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the onsets in the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    threshold : float\n",
    "        Threshold for peak picking\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    onsets : numpy array\n",
    "        Detected onsets (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "            \n",
    "    new_odf = []\n",
    "    \n",
    "    ######## MOVING AVERAGE AND THRESHOLD ########\n",
    "    \n",
    "    for i in range(0, len(odf)):\n",
    "        l = i - AVG_LEFT if i - AVG_LEFT > 0 else 0\n",
    "        r = i + AVG_RIGHT if i + AVG_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        new_val = odf[i] - np.average(odf[l:r])\n",
    "        new_odf.append(new_val if new_val >= threshold else 0)\n",
    "    \n",
    "    ######## LOCAL MAXIMUM ########\n",
    "    \n",
    "    for i in range(0, len(new_odf)):\n",
    "        l = i - MAX_LEFT if i - MAX_LEFT > 0 else 0\n",
    "        r = i + MAX_RIGHT if i + MAX_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        if new_odf[i] < max(new_odf[l:r]):\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## MINIMUM DISTANCE ########\n",
    "    \n",
    "    last = -1\n",
    "    for i in range(0, len(new_odf)):\n",
    "        if new_odf[i] > 0 and (last < 0 or i - last > MIN_DIST):\n",
    "            last = i\n",
    "        else:\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## SELECTING ONSETS ########\n",
    "\n",
    "    onsets = np.array([])\n",
    "    \n",
    "    for i, el in enumerate(new_odf):\n",
    "        if new_odf[i] > 0:\n",
    "            onsets = np.append(onsets, i)\n",
    "        \n",
    "    return onsets / frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def detect_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "    odf = onset_detection_function(spec)\n",
    "    odf_1 = np.array(odf)\n",
    "    odf_2 = np.array(odf)\n",
    "    onsets = detect_onsets(odf_1, 0.4, FPS)\n",
    "    onsets_lib = librosa.util.peak_pick(odf_2,3,1,10,11,0.4,3)/100 # librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_2)/100\n",
    "    gt = ONSET_ANNOTATIONS[19]\n",
    "    print(onsets[0:20], \"\\n\", len(onsets))\n",
    "    print(onsets_lib[0:20], \"\\n\", len(onsets_lib))\n",
    "    print(gt[0:20], \"\\n\", len(gt))\n",
    "#detect_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def compute_odfs():\n",
    "    odfs = []\n",
    "    for i, spec in enumerate(ONSET_AUDIO[0:321]):\n",
    "        odfs.append(onset_detection_function(spec))\n",
    "    return odfs\n",
    "# odfs = compute_odfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def parameter_test():\n",
    "    under = 0\n",
    "    over = 0\n",
    "    total = 0\n",
    "    for i, odf in enumerate(odfs):\n",
    "        odf_1 = np.array(odf)\n",
    "        odf_2 = np.array(odf)\n",
    "        odf_3 = np.array(odf)\n",
    "        odf_4 = np.array(odf)\n",
    "\n",
    "        onsets = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_1)\n",
    "        peaks = librosa.util.peak_pick(odf_2,wait=3, pre_max=3, post_max=1, pre_avg=10, post_avg=11, delta=0.4)\n",
    "        onsets_m = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_3,\n",
    "                                             wait=3, pre_max=3, post_max=1, pre_avg=10, post_avg=11, delta=0.07)\n",
    "        onsets_c = detect_onsets(odf_4, 0.4, FPS)\n",
    "            \n",
    "        x = onsets_c #peaks\n",
    "        y = ONSET_ANNOTATIONS[i] # onsets\n",
    "\n",
    "        diff = len(x) - len(y) \n",
    "        if(diff < 0):\n",
    "            under = under + diff\n",
    "        else:\n",
    "            over = over + diff\n",
    "        if(i % 25 == 0):   \n",
    "            print(i, \":\", len(x), \"and\", len(y), \"=\", diff)\n",
    "        \n",
    "        total = total + len(ONSET_ANNOTATIONS[i])\n",
    "\n",
    "    print(\"under\", under)\n",
    "    print(\"over\", over)\n",
    "    print(\"total\", total)\n",
    "# parameter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# for task 4\n",
    "ODFS = []\n",
    "\n",
    "# list for collecting the onset detections\n",
    "onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    odf = onset_detection_function(spec)\n",
    "    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "    onset_detections.append(onsets)\n",
    "    \n",
    "    ODFS.append(odf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_onsets(onsets, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected onsets against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    onsets : list\n",
    "        List with onset detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        Averaged precision.\n",
    "    recall : float\n",
    "        Averaged recall.\n",
    "    fmeasure : float\n",
    "        Averaged f-measure.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_fmeasure = 0\n",
    "    for i in range(0, len(onsets)):\n",
    "        tp, fp, tn, fn, errors = madmom.evaluation.onsets.onset_evaluation(onsets[i], annotations[i], window=0.025)\n",
    "        p = len(tp) / (len(tp) + len(fp)) if len(tp) > 0 else 0\n",
    "        r = len(tp) / (len(tp) + len(fn)) if len(tp) > 0 else 0\n",
    "        f = 2*p*r / (p + r) if p + r > 0 else 0\n",
    "        sum_precision = sum_precision + p\n",
    "        sum_recall = sum_recall + r\n",
    "        sum_fmeasure = sum_fmeasure + f\n",
    "    \n",
    "    precision = sum_precision / len(onsets)\n",
    "    recall = sum_recall / len(onsets)\n",
    "    fmeasure = sum_fmeasure / len(onsets)\n",
    "    return precision, recall, fmeasure\n",
    "    \n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(verbose=False):\n",
    "    frame_sizes = [1024, 2048, 4096]\n",
    "    num_bands = [20, 40, 80]\n",
    "    thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    best_fmeasure = 0\n",
    "    best_frame_size = 0\n",
    "    best_num_bands = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for i in range(0, len(frame_sizes)):\n",
    "        for j in range(0, len(num_bands)):\n",
    "            for k in range(0, len(thresholds)):\n",
    "                FRAME_SIZE = frame_sizes[i]\n",
    "                NUM_BANDS = num_bands[j]\n",
    "                THRESHOLD = thresholds[k]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"parameters:\", FRAME_SIZE, NUM_BANDS, THRESHOLD)\n",
    "                \n",
    "                # spectrograms\n",
    "                specs = []\n",
    "                for audio_file in AUDIO_FILES:\n",
    "                    spec = pre_process(audio_file, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "                    specs.append(spec)\n",
    "\n",
    "                onset_audio = [specs[i] for i in ONSET_AUDIO_IDX]\n",
    "                \n",
    "                # onset detections\n",
    "                ods = []\n",
    "                for l, spec in enumerate(onset_audio):\n",
    "                    odf = onset_detection_function(spec)\n",
    "                    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "                    ods.append(onsets)\n",
    "                \n",
    "                # evaluation\n",
    "                precision, recall, fmeasure = evaluate_onsets(ods, ONSET_ANNOTATIONS)\n",
    "                if verbose:\n",
    "                    print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (precision, recall, fmeasure))\n",
    "                    print('')\n",
    "                \n",
    "                if fmeasure > best_fmeasure:\n",
    "                    best_fmeasure = fmeasure\n",
    "                    best_frame_size = FRAME_SIZE\n",
    "                    best_num_bands = NUM_BANDS\n",
    "                    best_threshold = THRESHOLD\n",
    "                    \n",
    "    return best_fmeasure, best_frame_size, best_num_bands, best_threshold\n",
    "\n",
    "# uncomment and run block to optimize parameters\n",
    "# best_fmeasure, best_frame_size, best_num_bands, best_threshold = optimize_parameters(verbose=True)\n",
    "# print(\"best found parameters are:\", best_frame_size, best_num_bands, best_threshold, \"with F-measure:\", best_fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter optimization was run on the following parameters: <br>\n",
    "frame size (1024, 2048 and 4096), <br>\n",
    "number of mel bins (20, 40 and 80) and <br>\n",
    "threshold (in range from 0 to 1.0 (or 1.5 in some cases) with step size 0.1).\n",
    "<br><br>\n",
    "An example of a well performing combination: 2048 40 0.5 with precision: 79.6%\n",
    "recall: 75.4%, F-measure: 75.8%\n",
    "<br><br>\n",
    "The results are uploaded to the root directory in 3 separate files grouped for convenience by the frame size parameter: \"1024 param config.txt\", \"2048 param config.txt\" and \"4096 param config.txt\"\n",
    "<br><br>\n",
    "The first and most obvious observation in all cases is the influence of the threshold parameter on precision and recall values, starting with a low threshold value (high recall) and moving upwards (high precision) we can see how hitting a sweet spot with the threshold somewhere in the middle is necessary for a good F-measure value.\n",
    "<br><br>\n",
    "Furthermore we can see that selecting 20 as the number of mel bins almost universally yields slightly worse results regarldess of other parameters (within reasonable bounds) than the other 2 values. 20 seems to be too few bins, while 40 and 80 perform more or less similarly. <br>\n",
    "That being said we still achieved an F-measure of 74.5% with parameters 2048 20 0.3, while our overall best achieved F-measure was at 75.9%, so probably this difference is negligible\n",
    "<br><br>\n",
    "Most interestingly though one can see how picking 4096 as frame size results in significantly worse F-measure values, in best cases barely hitting the 65% mark, while 1024 and 2048 are consistently above 70%, often reaching the maximum of 75.9% with proper threshold and bin number parameters. <br> This can be attributed to the fact that by selecting a larger frame size one loses some of the temporal accuracy that is essential for onset detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning-based onset detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(audio, annotations, diffs=False, early_stopping=False,\n",
    "          verbose=True, model='model.pkl', **kwargs):\n",
    "    \"\"\"\n",
    "    Train an MLP on the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : list\n",
    "        List of audio files or precomputed spectrograms.\n",
    "    annotations : list of numpy arrays\n",
    "        List with corresponding onset annotations.\n",
    "    diffs : bool, optional\n",
    "        Include diffs as input features (step 7).\n",
    "    early_stopping : bool, optional\n",
    "        Use early stopping to prevent overfitting (step 8).\n",
    "    verbose : bool, optional\n",
    "        Be verbose during training.\n",
    "    model : str, optional\n",
    "        Save the fitted model to given file name.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mlp : MLPRegressor\n",
    "        Trained MLP.\n",
    "\n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    # define MLP\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(50, 50), tol=1e-4, max_iter=100,\n",
    "                       early_stopping=early_stopping, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(mlp)\n",
    "        \n",
    "    # prepare input features and targets\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    ######## INPUT PREPARATION ########\n",
    "    \n",
    "    # concatenate all features and transpose to fit the MLP input format\n",
    "    spectral_features = np.concatenate((audio), axis=1)\n",
    "    spectral_features_T = spectral_features.transpose()\n",
    "    x = spectral_features_T\n",
    "    \n",
    "    # add spectral flux to features\n",
    "    if diffs:\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('adding spectral flux to input features...')\n",
    "            print('')\n",
    "        flux = kwargs['flux']\n",
    "        flux = np.concatenate((flux))\n",
    "        flux = np.vstack(flux)\n",
    "        x = np.concatenate((x, flux), axis=1)\n",
    "\n",
    "    # create target as 0 array with value 1 where index matches the frame \n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(0, len(audio)):\n",
    "        spec_T = audio[i].transpose()\n",
    "        target = np.zeros(len(spec_T))\n",
    "        \n",
    "        onset_frames = np.rint(annotations[i] * FPS)\n",
    "        for j in range(0, len(onset_frames)):\n",
    "            target[int(onset_frames[j])] = 1\n",
    "\n",
    "        y = np.append(y, target)\n",
    "        \n",
    "    ###################################\n",
    "    \n",
    "    # reshape x and y\n",
    "    # Note: depending on your data pre-processing these lines might\n",
    "    #       need to be adjusted accordingly\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    \n",
    "    # train model\n",
    "    if verbose:\n",
    "        print('training model:', model)\n",
    "    mlp.fit(x.squeeze(), y.squeeze())\n",
    "    \n",
    "    # save model and return it\n",
    "    with open(model, 'wb') as f:\n",
    "        pickle.dump(mlp, f)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def test_target_gen():\n",
    "    targets = np.array([])\n",
    "\n",
    "    for i in range(0, len(ONSET_AUDIO)):\n",
    "        spec_T = ONSET_AUDIO[i].transpose()\n",
    "        onset_frames = np.rint(ONSET_ANNOTATIONS[i] * FPS)\n",
    "\n",
    "        target = np.zeros(len(spec_T))\n",
    "        for j in range(0, len(onset_frames)):\n",
    "            target[int(onset_frames[j])] = 1\n",
    "\n",
    "        targets = np.append(targets, target)\n",
    "        #print(onset_frames)\n",
    "        #print(len(target))\n",
    "        #print(target)\n",
    "\n",
    "    print(len(targets))\n",
    "#test_target_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, False, False, model='model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_THRESHOLD = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 2 ####\n",
    "\n",
    "# Function for optimizing the threshold parameter\n",
    "def optimize_mlp_threshold(model, thresholds=[], diffs=False, verbose=True, **kwargs):\n",
    "    best_threshold = 0\n",
    "    best_fmeasure = 0\n",
    "    \n",
    "    if diffs and verbose:\n",
    "        print('running optimization with spectral flux...')\n",
    "        print('')\n",
    "\n",
    "    for i in range(0, len(thresholds)):\n",
    "        ods_opt = []\n",
    "        for j, spec in enumerate(ONSET_AUDIO):\n",
    "            \n",
    "            x = spec.transpose()\n",
    "            \n",
    "            # add spectral flux to features\n",
    "            if diffs:\n",
    "                flux = kwargs['flux']\n",
    "                flux = np.vstack(flux[j])\n",
    "                x = np.concatenate((x, flux), axis=1)\n",
    "            \n",
    "            mlp_odf = model.predict(x)\n",
    "            mlp_onsets = detect_onsets(mlp_odf, thresholds[i], FPS)\n",
    "            ods_opt.append(mlp_onsets)\n",
    "        \n",
    "        p, r, f = evaluate_onsets(ods_opt, ONSET_ANNOTATIONS)\n",
    "        if verbose:\n",
    "            print('Current threshold:', thresholds[i])\n",
    "            print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))\n",
    "            print('')\n",
    "        \n",
    "        if f > best_fmeasure:\n",
    "            best_fmeasure = f\n",
    "            best_threshold = thresholds[i]\n",
    "        \n",
    "    if verbose:\n",
    "        print('Optimized threshold is:', best_threshold, 'with F measure:', best_fmeasure)\n",
    "    return best_threshold\n",
    "\n",
    "mlp_thresholds = np.arange(0,0.005,0.0005)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_THRESHOLD = optimize_mlp_threshold(model=MLP_MODEL, thresholds=mlp_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 3 ####\n",
    "\n",
    "mlp_onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    mlp_odf = MLP_MODEL.predict(spec.transpose())\n",
    "    mlp_onsets = detect_onsets(mlp_odf, MLP_THRESHOLD, FPS)\n",
    "    mlp_onset_detections.append(mlp_onsets)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP Regressor performed rather poorly compared to the hand-crafted method.\n",
    "Its performance averaged around 53% (across multiple trainings) compared to the approximate 75% of the hand-crafted one.\n",
    "\n",
    "It seems like having only the spectrogram as input features for the MLP is not enough for it to be able to generate a reasonable ODF, most likely because the features are treated in isolation and are viewed as a set by the MLP and the temporal structure of the spectrogram is not taken into account (i.e. differences in energy between consecutive frames).\n",
    "\n",
    "The reason the MLP did not fail completely could be possibly attributed to the fact that even if viewed in isolation a spectral feature vector still carries some information that is relevant to onset detection, for example high frequency content that is usually present at onsets. This could have helped the MLP learn a somewhat functioning ODF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3e: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CALCULATE FLUX ########\n",
    "\n",
    "FLUX = []\n",
    "for i in range(0, len(ONSET_AUDIO)):\n",
    "    diff = onset_detection_function(ONSET_AUDIO[i])\n",
    "    FLUX.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_DIFF_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, True, False, model='model_diff.pkl', flux=FLUX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_DIFF_THRESHOLD = 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_diff_thresholds = np.arange(0.03,0.11,0.01)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_DIFF_THRESHOLD = optimize_mlp_threshold(model=MLP_DIFF_MODEL, thresholds=mlp_diff_thresholds, diffs=True, flux=FLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_diff_detections = []\n",
    "\n",
    "# for task 4\n",
    "MLP_DIFF_ODFS = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    flux = np.vstack(FLUX[i])\n",
    "    x = np.concatenate((spec.transpose(), flux), axis=1)\n",
    "\n",
    "    mlp_diff_odf = MLP_DIFF_MODEL.predict(x)\n",
    "    mlp_diff_onsets = detect_onsets(mlp_diff_odf, MLP_DIFF_THRESHOLD, FPS)\n",
    "    mlp_diff_detections.append(mlp_diff_onsets)\n",
    "    \n",
    "    MLP_DIFF_ODFS.append(mlp_diff_odf)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_diff_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection with temporal diffs\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3g:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP Regressor that included the spectral flux as input additionally to the spectrogram performed much better than the one using only the spectrogram. Having the difference in energy from previous frame to the current one as input proved to be essential for learning an ODF.\n",
    "\n",
    "The Diff MLP model achieved 76.5% F-measure score, compared to the MLP models 53% and hand-crafted methods 75%.\n",
    "\n",
    "While the 1.5% increase in performance doesnt seem like a tremendously big one, we need to keep in mind that the regressor used for the task was a fairly generic one, so a network that is more configured to this particular task would probably achieve better results\n",
    "\n",
    "In conclusion we can say that using a neural network in combination with some smartly prepared input data is probably the most optimal way to approach onset detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tempo estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "TEMPO_ANNOTATION_FILES = search_files('data/train', '.bpm')\n",
    "TEMPO_AUDIO_FILES, TEMPO_AUDIO_IDX = find_audio_files(TEMPO_ANNOTATION_FILES, AUDIO_FILES)\n",
    "TEMPO_AUDIO = [SPECTROGRAMS[i] for i in TEMPO_AUDIO_IDX]\n",
    "TEMPO_ANNOTATIONS = [madmom.io.load_tempo(f)[0, 0] for f in TEMPO_ANNOTATION_FILES]\n",
    "\n",
    "assert len(TEMPO_ANNOTATION_FILES) == 107\n",
    "assert len(TEMPO_AUDIO_FILES) == 107\n",
    "assert len(TEMPO_AUDIO) == 107\n",
    "assert len(TEMPO_ANNOTATIONS) == 107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "# index 34 is test index (al_Media-104717(18.2-28.2))\n",
    "\n",
    "print(TEMPO_ANNOTATION_FILES[34]) # ok\n",
    "print(TEMPO_ANNOTATIONS[34]) # oks\n",
    "print(TEMPO_AUDIO_FILES[34]) # ok\n",
    "print(len(TEMPO_AUDIO[34][0])) # ok, should be around 989 (1001 is fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPO_MLP_ODFS = [MLP_DIFF_ODFS[i] for i in TEMPO_AUDIO_IDX]\n",
    "#print(len(TEMPO_MLP_ODFS[34]))\n",
    "\n",
    "#TEMPO_ODFS = [ODFS[i] for i in TEMPO_AUDIO_IDX]\n",
    "#print(len(TEMPO_ODFS[34]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tempo(odf, min_bpm=60, max_bpm=180, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the tempo of the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    min_bpm : float\n",
    "        Minimum tempo, given in beats per minute (BPM).\n",
    "    max_bpm : float\n",
    "        Maximum tempo, given in beats per minute (BPM).\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tempo : float\n",
    "        Detected tempo (in BPM).\n",
    "\n",
    "    \"\"\"\n",
    "    odf_copy = odf.copy()\n",
    "    \n",
    "    ######## MEDIAN FILTER ########\n",
    "    \n",
    "    if kwargs['median_filter']:\n",
    "        new_odf = []\n",
    "        med_left = 10 # 10\n",
    "        med_right = 11 # 11\n",
    "        for i in range(0, len(odf_copy)):\n",
    "            l = i - med_left if i - med_left > 0 else 0\n",
    "            r = i + med_right if i + med_right < len(odf_copy) else len(odf_copy)\n",
    "\n",
    "            new_odf.append(odf_copy[i] if odf_copy[i] > np.median(odf_copy[l:r]) else 0)\n",
    "\n",
    "        odf_copy = new_odf\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    highest_correlation = 0\n",
    "    tempo = min_bpm\n",
    "    for i in range(min_bpm, max_bpm+1):\n",
    "        shift = round((60 * frame_rate) / i) # number of frames to shift\n",
    "        shifted_odf = odf_copy[shift:] \n",
    "        \n",
    "        sum = 0\n",
    "        for j in range(0, len(shifted_odf)):\n",
    "            sum = sum + odf_copy[j] * shifted_odf[j]\n",
    "            \n",
    "        if sum > highest_correlation:\n",
    "            highest_correlation = sum\n",
    "            tempo = i\n",
    "\n",
    "    return float(tempo)\n",
    "\n",
    "\n",
    "def evaluate_tempo(tempi, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected tempi against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tempi : list\n",
    "        List with tempo detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy_1 : float\n",
    "        Averaged accuracy 1.\n",
    "    accuracy_2 : float\n",
    "        Averaged accuracy 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_acc1 = 0\n",
    "    sum_acc2 = 0\n",
    "    for i in range(0, len(tempi)):\n",
    "        result = madmom.evaluation.tempo.TempoEvaluation(tempi[i], annotations[i], tolerance=0.04, double=True, triple=False, sort=False)\n",
    "        sum_acc1 = sum_acc1 + result.acc1\n",
    "        sum_acc2 = sum_acc2 + result.acc2\n",
    "        \n",
    "    return sum_acc1 / len(tempi), sum_acc2 / len(tempi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEMPO PIPELINE ####\n",
    "# depends on the ODFs computed by the MLP with diffs\n",
    "# Optimal params: 70, 170, False\n",
    "\n",
    "def tempo_pipeline(min_bpm=60, max_bpm=180, median_filter=False):\n",
    "    tempi = []\n",
    "    for i in range(0, len(TEMPO_MLP_ODFS)):\n",
    "        tempo = detect_tempo(TEMPO_MLP_ODFS[i], min_bpm=min_bpm, max_bpm=max_bpm, frame_rate=FPS, median_filter=median_filter)\n",
    "        tempi.append(tempo)\n",
    "\n",
    "    acc_1, acc_2 = evaluate_tempo(tempi, TEMPO_ANNOTATIONS)\n",
    "    print('parameters:', min_bpm, 'to', max_bpm, 'bpm with median filter', median_filter)\n",
    "    print('Accuracy metric 1 (w/o double/half tempos):', acc_1, '\\nAccuracy metric 2 (with double/half tempos):', acc_2)\n",
    "    print('')\n",
    "    return tempi, acc_1, acc_2\n",
    "\n",
    "TEMPI, ACC_1, ACC_2 = tempo_pipeline(min_bpm=70, max_bpm=170, median_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEMPO PARAMETER OPTIMIZATION ####\n",
    "\n",
    "def tempo_parameter_optimization():\n",
    "    min_bpms = [40, 50, 60, 70, 80]\n",
    "    max_bpms = [140, 150, 160, 170, 180, 190, 200, 210, 220]\n",
    "    median_filters = [False, True]\n",
    "    \n",
    "    best_tempi = []\n",
    "    best_acc1 = 0\n",
    "    best_acc2 = 0\n",
    "    best_min_bpm = 0\n",
    "    best_max_bpm = 0\n",
    "    best_median_filter = False\n",
    "    \n",
    "    for i in range(0, len(min_bpms)):\n",
    "        for j in range(0, len(max_bpms)):\n",
    "            for k in range(0, len(median_filters)):\n",
    "                t, a1, a2 = tempo_pipeline(min_bpms[i], max_bpms[j], median_filters[k])\n",
    "                \n",
    "                if a1+a2 > best_acc1 + best_acc2:\n",
    "                    best_acc1 = a1\n",
    "                    best_acc2 = a2\n",
    "                    best_tempi = t\n",
    "                    best_min_bpm = min_bpms[i]\n",
    "                    best_max_bpm = max_bpms[j]\n",
    "                    best_median_filter = median_filters[j]\n",
    "    \n",
    "    return best_tempi, best_acc1, best_acc2, best_min_bpm, best_max_bpm, best_median_filter\n",
    "\n",
    "# UNCOMMENT LINE TO RUN PARAMETER OPTIMIZATION\n",
    "# BEST_TEMPI, BEST_ACC_1, BEST_ACC_2, BEST_MIN_BPM, BEST_MAX_BPM, BEST_MEDIAN_FILTER = tempo_parameter_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested parameters:\n",
    "min bpm range: 40 - 80 with step size 10\n",
    "max bpm range: 140 - 220 with step size 10\n",
    "with and without a median filter as a preprocessing step for the ODF\n",
    "the ODF itself was taken from the MLP Diff prediction\n",
    "\n",
    "The implemented method suffers greatly from double/half tempo errors, this is indicated by the difference between the 2 accuracy measures. Acc 1 measure varying between 40-50% and Acc 2 measure varying between 80-85% with best results achieved being 48.6% for Acc 1 and 86% for Acc 2 with min bpm 70, max bpm 170 and no median filtering (numbers may slightly vary because of the MLP Diff model retraining).\n",
    "\n",
    "Expanding the bpm range to the maximum of 40-220 did not result in performance increase even though the the minimum and maximum ground truth tempos of the audios are 41 and 208 bpm respectively, which seems counterintuitive at first but then it only makes sense that the algorithm works better when confined to a smaller interval, so a reasonable middle ground must be found when choosing the bpm interval.\n",
    "\n",
    "An optional median filter was added as a preprocessing step to the ODF, which filtered out all values below a \n",
    "moving median, the size of the median window was selected to be the same size as of the moving average window for peak picking. The use of the median filter proved to be inconsequential, it improved the results slightly in some cases and worsened them in others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "BEAT_ANNOTATION_FILES = search_files('data/train', '.beats')\n",
    "BEAT_AUDIO_FILES, BEAT_AUDIO_IDX = find_audio_files(BEAT_ANNOTATION_FILES, AUDIO_FILES)\n",
    "BEAT_AUDIO = [SPECTROGRAMS[i] for i in BEAT_AUDIO_IDX]\n",
    "BEAT_ANNOTATIONS = [madmom.io.load_beats(f) for f in BEAT_ANNOTATION_FILES]\n",
    "\n",
    "assert len(BEAT_ANNOTATION_FILES) == 177\n",
    "assert len(BEAT_AUDIO_FILES) == 177\n",
    "assert len(BEAT_AUDIO) == 177\n",
    "assert len(BEAT_ANNOTATIONS) == 177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "# index 45 is test index (al_Media-104717(18.2-28.2))\n",
    "\n",
    "print(BEAT_ANNOTATION_FILES[45]) # ok\n",
    "print(len(BEAT_ANNOTATIONS[45])) # ok\n",
    "print(BEAT_AUDIO_FILES[45]) # ok\n",
    "print(len(BEAT_AUDIO[45][0])) # ok, should be around 989 (1001 is fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAT_MLP_ODFS = [MLP_DIFF_ODFS[i] for i in BEAT_AUDIO_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_beats(odf, min_bpm=60, max_bpm=180, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the beats in an onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    min_bpm : float\n",
    "        Minimum tempo, given in beats per minute (BPM).\n",
    "    max_bpm : float\n",
    "        Maximum tempo, given in beats per minute (BPM).\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beats : numpy array\n",
    "        Detected beats (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "    # determine tempo from within this function in order to be used\n",
    "    # with a single input (the ODF)\n",
    "    tempo = detect_tempo(odf, min_bpm, max_bpm, frame_rate)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return beats\n",
    "\n",
    "\n",
    "def evaluate_beats(beats, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected beats against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beats : list\n",
    "        List with beats detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cmlt : float\n",
    "        Averaged CMLt.\n",
    "    amlt : float\n",
    "        Averaged AMLt.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return cmlt, amlt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BEAT PIPLINE ####\n",
    "\n",
    "def beat_pipeline():\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BEAT PARAMETER OPTIMIZATION ####\n",
    "\n",
    "def beat_parameter_optimization():\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUMMARY GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
