{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import madmom\n",
    "import librosa\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madmom.utils import search_files, match_file\n",
    "\n",
    "AUDIO_FILES = search_files('data/train', '.flac')\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.flac'):\n",
    "    \"\"\"\n",
    "    Find matching audio files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ann_files : list\n",
    "        List with annotation file names.\n",
    "    audio_files : list\n",
    "        List with audio file names to be matched\n",
    "    ann_suffix : str, optional\n",
    "        Suffix of the annotation files. If 'None'\n",
    "        the suffix is inferred from the annotation\n",
    "        files.\n",
    "    audio_suffix : str, optional\n",
    "        Suffix of the audio files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matched_files : list\n",
    "        List of matched audio file (names).\n",
    "    matched_indices : list\n",
    "        List of matching indices in `audio_files`.\n",
    "        \n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "# len(AUDIO_FILES)\n",
    "\n",
    "# from scripts import utilities\n",
    "# utilities.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "SR = 44100 # samping rate\n",
    "FRAME_SIZE = 2048 # number of samples per frame\n",
    "HOP_SIZE = int(SR / FPS) # hop size depends on sampling rate and frame rate\n",
    "NUM_BANDS = 40 # number of mel bins\n",
    "\n",
    "def pre_process(filename, frame_size=2048, frame_rate=FPS, num_bands=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Pre-process the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File to be processed.\n",
    "    frame_size : int\n",
    "        Size of the frames.\n",
    "    frame_rate : float\n",
    "        Frame rate used for the STFT.\n",
    "    num_bands : int\n",
    "        Number of frequency bands for the Mel filterbank.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram.\n",
    "\n",
    "    \"\"\"    \n",
    "    # STEP 1: read in audio\n",
    "    signal, sampling_rate_unused = librosa.load(filename, sr=SR) # read file\n",
    "    \n",
    "    # STEP 2,3: compute stft (default windowing function is Hann)\n",
    "    stft = librosa.core.stft(y=signal, n_fft=frame_size, hop_length=HOP_SIZE)\n",
    "    \n",
    "    # STEP 4: discard phase info and square magnitudes\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    \n",
    "    # STEP 5: apply mel scaling\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=frame_size, n_mels=num_bands)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    \n",
    "    # STEP 6: apply DB scaling\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    # double check\n",
    "    # mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=num_bands)\n",
    "    # db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    # print((db_mel_spectrogram)[0])\n",
    "        \n",
    "    spectrogram = db_mel_spectrogram\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "from librosa.display import specshow\n",
    "\n",
    "def test_pre_process():\n",
    "    texasName = AUDIO_FILES[19] #AUDIO_FILES[19]\n",
    "\n",
    "    spectrogram = pre_process(texasName, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "\n",
    "    # print(spectrogram.shape)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    specshow(spectrogram, sr=SR, hop_length=HOP_SIZE, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "# test_pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for collecting pre-processed spectrograms\n",
    "# Note: it is not necessary to use this list but recommended in order to\n",
    "#       avoid recomputation of the same features over and over again.\n",
    "#       *_AUDIO_IDX canbe used to acces the precomputed spectrograms by\n",
    "#       index.\n",
    "SPECTROGRAMS = []\n",
    "\n",
    "for audio_file in AUDIO_FILES:\n",
    "    spec = pre_process(audio_file, FRAME_SIZE, FPS, NUM_BANDS) # params missing in tuwel\n",
    "    SPECTROGRAMS.append(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onset detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "ONSET_ANNOTATION_FILES = search_files('data/train', '.onsets')\n",
    "ONSET_AUDIO_FILES, ONSET_AUDIO_IDX = find_audio_files(ONSET_ANNOTATION_FILES, AUDIO_FILES)\n",
    "ONSET_AUDIO = [SPECTROGRAMS[i] for i in ONSET_AUDIO_IDX]\n",
    "ONSET_ANNOTATIONS = [madmom.io.load_onsets(f) for f in ONSET_ANNOTATION_FILES]\n",
    "\n",
    "assert len(ONSET_ANNOTATION_FILES) == 321\n",
    "assert len(ONSET_AUDIO_FILES) == 321\n",
    "assert len(ONSET_AUDIO) == 321\n",
    "assert len(ONSET_ANNOTATIONS) == 321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection_function(spectrogram):\n",
    "    \"\"\"\n",
    "    Compute an onset detection function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "\n",
    "    \"\"\"\n",
    "    spectrogram_T = spectrogram.transpose()\n",
    "    \n",
    "    odf = []\n",
    "    for i, frame in enumerate(spectrogram_T):\n",
    "        sum = 0\n",
    "        for j, bin in enumerate(frame):\n",
    "            diff = spectrogram_T[i][j] - (spectrogram_T[i-1][j] if i > 0 else 0)\n",
    "            flux = diff if diff >= 0 else 0\n",
    "            sum = sum + flux\n",
    "\n",
    "        odf.append(sum / NUM_BANDS)\n",
    "                    \n",
    "    return odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def odf_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "\n",
    "    odf = onset_detection_function(spec)\n",
    "    # fix the weird librosa offset\n",
    "    #odf = [0.0, 0.0] + odf\n",
    "    #odf.pop()\n",
    "    #odf.pop()\n",
    "\n",
    "    odf_lib = librosa.onset.onset_strength(sr=SR, S=spec)\n",
    "\n",
    "    # print('odf_lib:', odf_lib[6], \" len: \", len(odf_lib))\n",
    "    # print(odf[6])\n",
    "\n",
    "    print(len(odf), \"and\", len(odf_lib))\n",
    "    #for i, elem in enumerate(odf):\n",
    "    #    print(odf[i] == odf_lib[i])\n",
    "# odf_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEFT = 2 # 3 default value\n",
    "MAX_RIGHT = 3 # 1 \n",
    "AVG_LEFT = 10 # 10\n",
    "AVG_RIGHT = 11 # 11\n",
    "MIN_DIST = 3 # 3 (30ms)\n",
    "            # 0.5 is used # 0.07 threshold\n",
    "\n",
    "def detect_onsets(odf, threshold, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the onsets in the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    threshold : float\n",
    "        Threshold for peak picking\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    onsets : numpy array\n",
    "        Detected onsets (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "            \n",
    "    new_odf = []\n",
    "    \n",
    "    ######## MOVING AVERAGE AND THRESHOLD ########\n",
    "    \n",
    "    for i in range(0, len(odf)):\n",
    "        l = i - AVG_LEFT if i - AVG_LEFT > 0 else 0\n",
    "        r = i + AVG_RIGHT if i + AVG_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        new_val = odf[i] - np.average(odf[l:r])\n",
    "        new_odf.append(new_val if new_val >= threshold else 0)\n",
    "    \n",
    "    ######## LOCAL MAXIMUM ########\n",
    "    \n",
    "    for i in range(0, len(new_odf)):\n",
    "        l = i - MAX_LEFT if i - MAX_LEFT > 0 else 0\n",
    "        r = i + MAX_RIGHT if i + MAX_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        if new_odf[i] < max(new_odf[l:r]):\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## MINIMUM DISTANCE ########\n",
    "    \n",
    "    last = -1\n",
    "    for i in range(0, len(new_odf)):\n",
    "        if new_odf[i] > 0 and (last < 0 or i - last > MIN_DIST):\n",
    "            last = i\n",
    "        else:\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## SELECTING ONSETS ########\n",
    "\n",
    "    onsets = np.array([])\n",
    "    \n",
    "    for i, el in enumerate(new_odf):\n",
    "        if new_odf[i] > 0:\n",
    "            onsets = np.append(onsets, i)\n",
    "        \n",
    "    return onsets / frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def detect_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "    odf = onset_detection_function(spec)\n",
    "    odf_1 = np.array(odf)\n",
    "    odf_2 = np.array(odf)\n",
    "    onsets = detect_onsets(odf_1, 0.4, FPS)\n",
    "    onsets_lib = librosa.util.peak_pick(odf_2,3,1,10,11,0.4,3)/100 # librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_2)/100\n",
    "    gt = ONSET_ANNOTATIONS[19]\n",
    "    print(onsets[0:20], \"\\n\", len(onsets))\n",
    "    print(onsets_lib[0:20], \"\\n\", len(onsets_lib))\n",
    "    print(gt[0:20], \"\\n\", len(gt))\n",
    "#detect_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def compute_odfs():\n",
    "    odfs = []\n",
    "    for i, spec in enumerate(ONSET_AUDIO[0:321]):\n",
    "        odfs.append(onset_detection_function(spec))\n",
    "    return odfs\n",
    "# odfs = compute_odfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def parameter_test():\n",
    "    under = 0\n",
    "    over = 0\n",
    "    total = 0\n",
    "    for i, odf in enumerate(odfs):\n",
    "        odf_1 = np.array(odf)\n",
    "        odf_2 = np.array(odf)\n",
    "        odf_3 = np.array(odf)\n",
    "        odf_4 = np.array(odf)\n",
    "\n",
    "        onsets = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_1)\n",
    "        peaks = librosa.util.peak_pick(odf_2,wait=3, pre_max=3, post_max=1, pre_avg=10, post_avg=11, delta=0.4)\n",
    "        onsets_m = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_3,\n",
    "                                             wait=3, pre_max=3, post_max=1, pre_avg=10, post_avg=11, delta=0.07)\n",
    "        onsets_c = detect_onsets(odf_4, 0.4, FPS)\n",
    "            \n",
    "        x = onsets_c #peaks\n",
    "        y = ONSET_ANNOTATIONS[i] # onsets\n",
    "\n",
    "        diff = len(x) - len(y) \n",
    "        if(diff < 0):\n",
    "            under = under + diff\n",
    "        else:\n",
    "            over = over + diff\n",
    "        if(i % 25 == 0):   \n",
    "            print(i, \":\", len(x), \"and\", len(y), \"=\", diff)\n",
    "        \n",
    "        total = total + len(ONSET_ANNOTATIONS[i])\n",
    "\n",
    "    print(\"under\", under)\n",
    "    print(\"over\", over)\n",
    "    print(\"total\", total)\n",
    "# parameter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# list for collecting the onset detections\n",
    "onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    odf = onset_detection_function(spec)\n",
    "    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "    onset_detections.append(onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_onsets(onsets, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected onsets against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    onsets : list\n",
    "        List with onset detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        Averaged precision.\n",
    "    recall : float\n",
    "        Averaged recall.\n",
    "    fmeasure : float\n",
    "        Averaged f-measure.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_fmeasure = 0\n",
    "    for i in range(0, len(onsets)):\n",
    "        tp, fp, tn, fn, errors = madmom.evaluation.onsets.onset_evaluation(onsets[i], annotations[i], window=0.025)\n",
    "        p = len(tp) / (len(tp) + len(fp)) if len(tp) > 0 else 0\n",
    "        r = len(tp) / (len(tp) + len(fn)) if len(tp) > 0 else 0\n",
    "        f = 2*p*r / (p + r) if p + r > 0 else 0\n",
    "        sum_precision = sum_precision + p\n",
    "        sum_recall = sum_recall + r\n",
    "        sum_fmeasure = sum_fmeasure + f\n",
    "    \n",
    "    precision = sum_precision / len(onsets)\n",
    "    recall = sum_recall / len(onsets)\n",
    "    fmeasure = sum_fmeasure / len(onsets)\n",
    "    return precision, recall, fmeasure\n",
    "    \n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(verbose=False):\n",
    "    frame_sizes = [1024, 2048, 4096]\n",
    "    num_bands = [20, 40, 80]\n",
    "    thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    best_fmeasure = 0\n",
    "    best_frame_size = 0\n",
    "    best_num_bands = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for i in range(0, len(frame_sizes)):\n",
    "        for j in range(0, len(num_bands)):\n",
    "            for k in range(0, len(thresholds)):\n",
    "                FRAME_SIZE = frame_sizes[i]\n",
    "                NUM_BANDS = num_bands[j]\n",
    "                THRESHOLD = thresholds[k]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"parameters:\", FRAME_SIZE, NUM_BANDS, THRESHOLD)\n",
    "                \n",
    "                # spectrograms\n",
    "                specs = []\n",
    "                for audio_file in AUDIO_FILES:\n",
    "                    spec = pre_process(audio_file, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "                    specs.append(spec)\n",
    "\n",
    "                onset_audio = [specs[i] for i in ONSET_AUDIO_IDX]\n",
    "                \n",
    "                # onset detections\n",
    "                ods = []\n",
    "                for l, spec in enumerate(onset_audio):\n",
    "                    odf = onset_detection_function(spec)\n",
    "                    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "                    ods.append(onsets)\n",
    "                \n",
    "                # evaluation\n",
    "                precision, recall, fmeasure = evaluate_onsets(ods, ONSET_ANNOTATIONS)\n",
    "                if verbose:\n",
    "                    print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (precision, recall, fmeasure))\n",
    "                    print('')\n",
    "                \n",
    "                if fmeasure > best_fmeasure:\n",
    "                    best_fmeasure = fmeasure\n",
    "                    best_frame_size = FRAME_SIZE\n",
    "                    best_num_bands = NUM_BANDS\n",
    "                    best_threshold = THRESHOLD\n",
    "                    \n",
    "    return best_fmeasure, best_frame_size, best_num_bands, best_threshold\n",
    "\n",
    "# uncomment and run block to optimize parameters\n",
    "# best_fmeasure, best_frame_size, best_num_bands, best_threshold = optimize_parameters(verbose=True)\n",
    "# print(\"best found parameters are:\", best_frame_size, best_num_bands, best_threshold, \"with F-measure:\", best_fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter optimization was run on the following parameters: <br>\n",
    "frame size (1024, 2048 and 4096), <br>\n",
    "number of mel bins (20, 40 and 80) and <br>\n",
    "threshold (in range from 0 to 1.0 (or 1.5 in some cases) with step size 0.1).\n",
    "<br><br>\n",
    "An example of a well performing combination: 2048 40 0.5 with precision: 79.6%\n",
    "recall: 75.4%, F-measure: 75.8%\n",
    "<br><br>\n",
    "The results are uploaded to the root directory in 3 separate files grouped for convenience by the frame size parameter: \"1024 param config.txt\", \"2048 param config.txt\" and \"4096 param config.txt\"\n",
    "<br><br>\n",
    "The first and most obvious observation in all cases is the influence of the threshold parameter on precision and recall values, starting with a low threshold value (high recall) and moving upwards (high precision) we can see how hitting a sweet spot with the threshold somewhere in the middle is necessary for a good F-measure value.\n",
    "<br><br>\n",
    "Furthermore we can see that selecting 20 as the number of mel bins almost universally yields slightly worse results regarldess of other parameters (within reasonable bounds) than the other 2 values. 20 seems to be too few bins, while 40 and 80 perform more or less similarly. <br>\n",
    "That being said we still achieved an F-measure of 74.5% with parameters 2048 20 0.3, while our overall best achieved F-measure was at 75.9%, so probably this difference is negligible\n",
    "<br><br>\n",
    "Most interestingly though one can see how picking 4096 as frame size results in significantly worse F-measure values, in best cases barely hitting the 65% mark, while 1024 and 2048 are consistently above 70%, often reaching the maximum of 75.9% with proper threshold and bin number parameters. <br> This can be attributed to the fact that by selecting a larger frame size one loses some of the temporal accuracy that is essential for onset detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning-based onset detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(audio, annotations, diffs=False, early_stopping=False,\n",
    "          verbose=True, model='model.pkl', **kwargs):\n",
    "    \"\"\"\n",
    "    Train an MLP on the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : list\n",
    "        List of audio files or precomputed spectrograms.\n",
    "    annotations : list of numpy arrays\n",
    "        List with corresponding onset annotations.\n",
    "    diffs : bool, optional\n",
    "        Include diffs as input features (step 7).\n",
    "    early_stopping : bool, optional\n",
    "        Use early stopping to prevent overfitting (step 8).\n",
    "    verbose : bool, optional\n",
    "        Be verbose during training.\n",
    "    model : str, optional\n",
    "        Save the fitted model to given file name.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mlp : MLPRegressor\n",
    "        Trained MLP.\n",
    "\n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    # define MLP\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(50, 50), tol=1e-4, max_iter=100,\n",
    "                       early_stopping=early_stopping, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(mlp)\n",
    "        \n",
    "    # prepare input features and targets\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    ######## INPUT PREPARATION ########\n",
    "    \n",
    "    # concatenate all features and transpose to fit the MLP input format\n",
    "    spectral_features = np.concatenate((audio), axis=1)\n",
    "    spectral_features_T = spectral_features.transpose()\n",
    "    x = spectral_features_T\n",
    "    \n",
    "    # add spectral flux to features\n",
    "    if diffs:\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('adding spectral flux to input features...')\n",
    "            print('')\n",
    "        flux = kwargs['flux']\n",
    "        flux = np.concatenate((flux))\n",
    "        flux = np.vstack(flux)\n",
    "        x = np.concatenate((x, flux), axis=1)\n",
    "\n",
    "    # create target as 0 array with value 1 where index matches the frame \n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(0, len(audio)):\n",
    "        spec_T = audio[i].transpose()\n",
    "        target = np.zeros(len(spec_T))\n",
    "        \n",
    "        onset_frames = np.rint(annotations[i] * FPS)\n",
    "        for j in range(0, len(onset_frames)):\n",
    "            target[int(onset_frames[j])] = 1\n",
    "\n",
    "        y = np.append(y, target)\n",
    "        \n",
    "    ###################################\n",
    "    \n",
    "    # reshape x and y\n",
    "    # Note: depending on your data pre-processing these lines might\n",
    "    #       need to be adjusted accordingly\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    \n",
    "    # train model\n",
    "    if verbose:\n",
    "        print('training model:', model)\n",
    "    mlp.fit(x.squeeze(), y.squeeze())\n",
    "    \n",
    "    # save model and return it\n",
    "    with open(model, 'wb') as f:\n",
    "        pickle.dump(mlp, f)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "def test_target_gen():\n",
    "    targets = np.array([])\n",
    "\n",
    "    for i in range(0, len(ONSET_AUDIO)):\n",
    "        spec_T = ONSET_AUDIO[i].transpose()\n",
    "        onset_frames = np.rint(ONSET_ANNOTATIONS[i] * FPS)\n",
    "\n",
    "        target = np.zeros(len(spec_T))\n",
    "        for j in range(0, len(onset_frames)):\n",
    "            target[int(onset_frames[j])] = 1\n",
    "\n",
    "        targets = np.append(targets, target)\n",
    "        #print(onset_frames)\n",
    "        #print(len(target))\n",
    "        #print(target)\n",
    "\n",
    "    print(len(targets))\n",
    "#test_target_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, False, False, model='model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_THRESHOLD = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 2 ####\n",
    "\n",
    "# Function for optimizing the threshold parameter\n",
    "def optimize_mlp_threshold(model, thresholds=[], diffs=False, verbose=True, **kwargs):\n",
    "    best_threshold = 0\n",
    "    best_fmeasure = 0\n",
    "    \n",
    "    if diffs and verbose:\n",
    "        print('running optimization with spectral flux...')\n",
    "        print('')\n",
    "\n",
    "    for i in range(0, len(thresholds)):\n",
    "        ods_opt = []\n",
    "        for j, spec in enumerate(ONSET_AUDIO):\n",
    "            \n",
    "            x = spec.transpose()\n",
    "            \n",
    "            # add spectral flux to features\n",
    "            if diffs:\n",
    "                flux = kwargs['flux']\n",
    "                flux = np.vstack(flux[j])\n",
    "                x = np.concatenate((x, flux), axis=1)\n",
    "            \n",
    "            mlp_odf = model.predict(x)\n",
    "            mlp_onsets = detect_onsets(mlp_odf, thresholds[i], FPS)\n",
    "            ods_opt.append(mlp_onsets)\n",
    "        \n",
    "        p, r, f = evaluate_onsets(ods_opt, ONSET_ANNOTATIONS)\n",
    "        if verbose:\n",
    "            print('Current threshold:', thresholds[i])\n",
    "            print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))\n",
    "            print('')\n",
    "        \n",
    "        if f > best_fmeasure:\n",
    "            best_fmeasure = f\n",
    "            best_threshold = thresholds[i]\n",
    "        \n",
    "    if verbose:\n",
    "        print('Optimized threshold is:', best_threshold, 'with F measure:', best_fmeasure)\n",
    "    return best_threshold\n",
    "\n",
    "mlp_thresholds = np.arange(0,0.005,0.0005)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_THRESHOLD = optimize_mlp_threshold(model=MLP_MODEL, thresholds=mlp_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STEP 3 ####\n",
    "\n",
    "mlp_onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    mlp_odf = MLP_MODEL.predict(spec.transpose())\n",
    "    mlp_onsets = detect_onsets(mlp_odf, MLP_THRESHOLD, FPS)\n",
    "    mlp_onset_detections.append(mlp_onsets)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d: TO BE DONE!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3e: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CALCULATE FLUX ########\n",
    "\n",
    "FLUX = []\n",
    "for i in range(0, len(ONSET_AUDIO)):\n",
    "    diff = onset_detection_function(ONSET_AUDIO[i])\n",
    "    FLUX.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_DIFF_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, True, False, model='model_diff.pkl', flux=FLUX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_DIFF_THRESHOLD = 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_diff_thresholds = np.arange(0.03,0.11,0.01)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_DIFF_THRESHOLD = optimize_mlp_threshold(model=MLP_DIFF_MODEL, thresholds=mlp_diff_thresholds, diffs=True, flux=FLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_diff_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    flux = np.vstack(FLUX[i])\n",
    "    x = np.concatenate((spec.transpose(), flux), axis=1)\n",
    "\n",
    "    mlp_diff_odf = MLP_DIFF_MODEL.predict(x)\n",
    "    mlp_diff_onsets = detect_onsets(mlp_diff_odf, MLP_DIFF_THRESHOLD, FPS)\n",
    "    mlp_diff_detections.append(mlp_diff_onsets)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_diff_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection with temporal diffs\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3g: TO BE DONE!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
