{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import madmom\n",
    "import librosa\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madmom.utils import search_files, match_file\n",
    "\n",
    "AUDIO_FILES = search_files('data/train', '.flac')\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.flac'):\n",
    "    \"\"\"\n",
    "    Find matching audio files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ann_files : list\n",
    "        List with annotation file names.\n",
    "    audio_files : list\n",
    "        List with audio file names to be matched\n",
    "    ann_suffix : str, optional\n",
    "        Suffix of the annotation files. If 'None'\n",
    "        the suffix is inferred from the annotation\n",
    "        files.\n",
    "    audio_suffix : str, optional\n",
    "        Suffix of the audio files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matched_files : list\n",
    "        List of matched audio file (names).\n",
    "    matched_indices : list\n",
    "        List of matching indices in `audio_files`.\n",
    "        \n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "# len(AUDIO_FILES)\n",
    "\n",
    "# from scripts import utilities\n",
    "# utilities.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "SR = 44100 # samping rate\n",
    "FRAME_SIZE = 2048 # number of samples per frame\n",
    "HOP_SIZE = int(SR / FPS) # hop size depends on sampling rate and frame rate\n",
    "NUM_BANDS = 40 # number of mel bins\n",
    "\n",
    "def pre_process(filename, frame_size=2048, frame_rate=FPS, num_bands=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Pre-process the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File to be processed.\n",
    "    frame_size : int\n",
    "        Size of the frames.\n",
    "    frame_rate : float\n",
    "        Frame rate used for the STFT.\n",
    "    num_bands : int\n",
    "        Number of frequency bands for the Mel filterbank.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram.\n",
    "\n",
    "    \"\"\"    \n",
    "    # STEP 1: read in audio\n",
    "    signal, sampling_rate_unused = librosa.load(filename, sr=SR) # read file\n",
    "    \n",
    "    # STEP 2,3: compute stft (default windowing function is Hann)\n",
    "    stft = librosa.core.stft(y=signal, n_fft=frame_size, hop_length=HOP_SIZE)\n",
    "    \n",
    "    # STEP 4: discard phase info and square magnitudes\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    \n",
    "    # STEP 5: apply mel scaling\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=frame_size, n_mels=num_bands)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    \n",
    "    # STEP 6: apply DB scaling\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    # double check\n",
    "    # mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=num_bands)\n",
    "    # db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    # print((db_mel_spectrogram)[0])\n",
    "        \n",
    "    spectrogram = db_mel_spectrogram\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "from librosa.display import specshow\n",
    "\n",
    "def test_pre_process():\n",
    "    texasName = AUDIO_FILES[19] #AUDIO_FILES[19]\n",
    "\n",
    "    spectrogram = pre_process(texasName, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "\n",
    "    # print(spectrogram.shape)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    specshow(spectrogram, sr=SR, hop_length=HOP_SIZE, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "# test_pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for collecting pre-processed spectrograms\n",
    "# Note: it is not necessary to use this list but recommended in order to\n",
    "#       avoid recomputation of the same features over and over again.\n",
    "#       *_AUDIO_IDX canbe used to acces the precomputed spectrograms by\n",
    "#       index.\n",
    "SPECTROGRAMS = []\n",
    "\n",
    "for audio_file in AUDIO_FILES:\n",
    "    spec = pre_process(audio_file)\n",
    "    SPECTROGRAMS.append(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onset detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "ONSET_ANNOTATION_FILES = search_files('data/train', '.onsets')\n",
    "ONSET_AUDIO_FILES, ONSET_AUDIO_IDX = find_audio_files(ONSET_ANNOTATION_FILES, AUDIO_FILES)\n",
    "ONSET_AUDIO = [SPECTROGRAMS[i] for i in ONSET_AUDIO_IDX]\n",
    "ONSET_ANNOTATIONS = [madmom.io.load_onsets(f) for f in ONSET_ANNOTATION_FILES]\n",
    "\n",
    "assert len(ONSET_ANNOTATION_FILES) == 321\n",
    "assert len(ONSET_AUDIO_FILES) == 321\n",
    "assert len(ONSET_AUDIO) == 321\n",
    "assert len(ONSET_ANNOTATIONS) == 321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection_function(spectrogram):\n",
    "    \"\"\"\n",
    "    Compute an onset detection function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "\n",
    "    \"\"\"\n",
    "    spectrogram_T = spectrogram.transpose()\n",
    "    \n",
    "    odf = []\n",
    "    for i, frame in enumerate(spectrogram_T):\n",
    "        sum = 0\n",
    "        for j, bin in enumerate(frame):\n",
    "            diff = spectrogram_T[i][j] - (spectrogram_T[i-1][j] if i > 0 else 0)\n",
    "            flux = diff if diff >= 0 else 0\n",
    "            sum = sum + flux\n",
    "\n",
    "        odf.append(sum / NUM_BANDS)\n",
    "                    \n",
    "    return odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "def odf_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "\n",
    "    odf = onset_detection_function(spec)\n",
    "    # fix the weird librosa offset\n",
    "    #odf = [0.0, 0.0] + odf\n",
    "    #odf.pop()\n",
    "    #odf.pop()\n",
    "\n",
    "    odf_lib = librosa.onset.onset_strength(sr=SR, S=spec)\n",
    "\n",
    "    # print('odf_lib:', odf_lib[6], \" len: \", len(odf_lib))\n",
    "    # print(odf[6])\n",
    "\n",
    "    print(len(odf), \"and\", len(odf_lib))\n",
    "    #for i, elem in enumerate(odf):\n",
    "    #    print(odf[i] == odf_lib[i])\n",
    "    \n",
    "# odf_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_onsets(odf, threshold, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the onsets in the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    threshold : float\n",
    "        Threshold for peak picking\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    onsets : numpy array\n",
    "        Detected onsets (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    max_w_left = 3\n",
    "    max_w_right = 3\n",
    "    avg_w_left = 10\n",
    "    avg_w_right = 11\n",
    "    min_distance = 3 #30ms\n",
    "    \n",
    "    ######## MOVING AVERAGE AND THRESHOLD ########\n",
    "    avg_odf = []\n",
    "    \n",
    "    ### first window ###\n",
    "    sum = 0\n",
    "    for j in range(0, avg_w_right):\n",
    "        sum = sum + odf[j]\n",
    "    avg = sum / avg_w_right\n",
    "    \n",
    "    for j in range(0, avg_w_right):\n",
    "        diff = odf[j] - avg\n",
    "        val = diff if diff > threshold else 0\n",
    "        avg_odf.append(val)\n",
    "        \n",
    "    #print(avg_odf)\n",
    "    \n",
    "    ### other windows ###\n",
    "    print(\"\")\n",
    "    w_step = avg_w_left + avg_w_right\n",
    "    for i in range(avg_w_right, len(odf), w_step):\n",
    "        sum = 0\n",
    "        avg = 0\n",
    "        upper_bound = i + min(w_step, len(odf) - i)\n",
    "        for j in range(i, upper_bound):\n",
    "            #print(j)\n",
    "            sum = sum + odf[j]\n",
    "        avg = sum / avg_w_right\n",
    "        \n",
    "        for j in range(i, upper_bound):\n",
    "            diff = odf[j] - avg\n",
    "            val = diff if diff > threshold else 0\n",
    "            avg_odf.append(val)\n",
    "        #print(min(w_step, len(odf) - i))\n",
    "        \n",
    "    #print(avg_odf[0])\n",
    "\n",
    "    ######## LOCAL MAXIMUM ########\n",
    "    \n",
    "    ######## MINIMUM DISTANCE ########\n",
    "    \n",
    "   # for i, el in enumerate(odf):\n",
    "   #     print(odf[i])\n",
    "    \n",
    "    onsets = np.array([])\n",
    "    \n",
    "    for i, el in enumerate(avg_odf):\n",
    "        if avg_odf[i] > 0:\n",
    "            onsets = np.append(onsets, i)\n",
    "            #print(avg_odf[i])\n",
    "        \n",
    "    return onsets / frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "def detect_test():\n",
    "    spec = ONSET_AUDIO[19]\n",
    "    odf = onset_detection_function(spec)\n",
    "    odf = np.array(odf)\n",
    "\n",
    "    print(odf)\n",
    "    onsets = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf)\n",
    "    print(odf)\n",
    "    peaks = librosa.util.peak_pick(odf,3,3,10,11,0.0695,3)\n",
    "\n",
    "    #print(len(onsets))\n",
    "    #print(onsets/100)\n",
    "    print(len(peaks))\n",
    "    print(peaks/100)\n",
    "\n",
    "\n",
    "    # onsets = detect_onsets(odf, 0.0695, FPS)\n",
    "\n",
    "# detect_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "def param_single_test():\n",
    "    # onsets = detect_onsets(odf_lib, 5, FPS)\n",
    "\n",
    "    # testing\n",
    "\n",
    "    spec = ONSET_AUDIO[19]\n",
    "\n",
    "    odf = onset_detection_function(spec)\n",
    "    odf = np.array(odf)\n",
    "\n",
    "    onsets = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf)\n",
    "    # peaks = librosa.util.peak_pick(odf,5,5,7,7,0.066,3) # also good\n",
    "    # peaks = librosa.util.peak_pick(odf,3,3,7,7,0.07,3) # also good\n",
    "    # peaks = librosa.util.peak_pick(odf,3,3,11,11,0.0695,3) # also good\n",
    "    peaks = librosa.util.peak_pick(odf,3,3,10,11,0.0695,3)\n",
    "\n",
    "    #onsets_lib = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf_lib)\n",
    "\n",
    "    #y, sampling_rate_unused = librosa.load(AUDIO_FILES[19], sr=SR)\n",
    "    #onsets_full_lib = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, y=y)\n",
    "\n",
    "    print(len(onsets))\n",
    "    print(onsets/100)\n",
    "    print(len(peaks))\n",
    "    print(peaks/100)\n",
    "    #print(onsets_lib/100)\n",
    "    #print(onsets_full_lib/100)\n",
    "\n",
    "# param_single_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM\n",
    "\n",
    "# print(len(ONSET_ANNOTATIONS[19]))\n",
    "# ONSET_ANNOTATIONS[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM\n",
    "def parameter_test():\n",
    "    under = 0\n",
    "    over = 0\n",
    "    for i, spec in enumerate(ONSET_AUDIO):\n",
    "        odf = onset_detection_function(spec)\n",
    "        odf = np.array(odf)\n",
    "\n",
    "        onsets = librosa.onset.onset_detect(sr=SR, hop_length=HOP_SIZE, onset_envelope=odf)\n",
    "        peaks = librosa.util.peak_pick(odf,3,3,10,11,0.0695,3)\n",
    "\n",
    "        diff = len(peaks) - len(onsets) \n",
    "        if(diff < 0):\n",
    "            under = under + diff\n",
    "        else:\n",
    "            over = over + diff\n",
    "        if(i % 25 == 0):   \n",
    "            print(i, \":\", len(peaks), \"and\", len(onsets), \"=\", diff)\n",
    "\n",
    "    print(\"under\", under)\n",
    "    print(\"over\", over)\n",
    "    \n",
    "# parameter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
