{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this exercise in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says \"YOUR ANSWER HERE\" or `YOUR CODE HERE` and remove the `raise NotImplementedError()` lines. Please add your name and student ID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Peter Rjabcsenko\"\n",
    "STUDENT_ID = \"1228563\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8a88854cc9a769c1894cecb8f07f58b",
     "grade": false,
     "grade_id": "cell-f1656939e389d353",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Intelligent Audio and Music Analysis Exercise 1\n",
    "\n",
    "The goal of this exercise is to learn the basics in onset detection, beat\n",
    "tracking and tempo estimation.\n",
    "\n",
    "After completing this exercise you should have learned some music information\n",
    "retrieval (MIR) basics and fostered your knowledge about these topics.\n",
    "\n",
    "All data needed for this exercise is in the `data` directory. \n",
    "The folder contains audio files as well as annotations\n",
    "(simple text files, one annotation per line) for `onsets`, `beats`, and `tempo`.\n",
    "Not all audio files have all kinds of annotations, thus depending on the task\n",
    "only a subset of all files can be used for evaluation.\n",
    "\n",
    "For development of the algorithms, you can use any software packages as long\n",
    "as you code the steps by yourself (exceptions are indicated).\n",
    "\n",
    "Note: steps marked as optional are not needed to be implemented to achieve all points,\n",
    "but can compensate for otherwise missing points throughout this exercise.\n",
    "\n",
    "Grading will be based on the solution and not on the achieved performance.\n",
    "Max. 100 points are achievable.\n",
    "\n",
    "The notebook structure for tasks 1 to 3 is rather strict and split into sub-tasks to\n",
    "provide some guidance. Tasks 4 to 6 are more flexible, but it is recommended to define\n",
    "the functions similar to those of tasks 1 to 3.\n",
    "\n",
    "Recommended software packages:\n",
    "\n",
    "- madmom (https://github.com/CPJKU/madmom)\n",
    "- librosa (https://github.com/librosa/librosa)\n",
    "- mir_eval (https://github.com/craffel/mir_eval)\n",
    "\n",
    "You are free to add code and textual cells as you need them.\n",
    "However `CONSTANTS` should not be altered.\n",
    "You may add visualisations, tables, etc. to enhance your assignment.\n",
    "\n",
    "### Chocolate challenge\n",
    "\n",
    "There will be again a chocolate challenge comprising prices for the following sub-challenges:\n",
    "\n",
    "1. best performing tempo estimation on a hidden test set,\n",
    "2. best performing beat tracking on a hidden test set,\n",
    "3. nicest visualisation.\n",
    "\n",
    "In order to participate in the challenge, please make sure that the `chocolate_challenge()`\n",
    "function writes the detected tempo and beats of the supplied test file in `data/test` to `.txt`\n",
    "files.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e74465796bf26f53f500669961af37b1",
     "grade": false,
     "grade_id": "cell-7f185f0443dee9dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import madmom\n",
    "import librosa\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f771f61d3086192a42a6030d9c7615a",
     "grade": false,
     "grade_id": "cell-130d2b2673903794",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4f9c5caf580aa5fe70531515d0a0689",
     "grade": false,
     "grade_id": "cell-2418ea65d9a46989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "FPS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65a377843de0301bc15a7058b8eaee10",
     "grade": false,
     "grade_id": "cell-5ed2b72dbb11ae48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Define audio files and function to match them to annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd2f40cda0817fa2d5b428da40aefd7",
     "grade": false,
     "grade_id": "cell-6ba362236963e577",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from madmom.utils import search_files, match_file\n",
    "\n",
    "AUDIO_FILES = search_files('data/train', '.wav')\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.wav'):\n",
    "    \"\"\"\n",
    "    Find matching audio files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ann_files : list\n",
    "        List with annotation file names.\n",
    "    audio_files : list\n",
    "        List with audio file names to be matched\n",
    "    ann_suffix : str, optional\n",
    "        Suffix of the annotation files. If 'None'\n",
    "        the suffix is inferred from the annotation\n",
    "        files.\n",
    "    audio_suffix : str, optional\n",
    "        Suffix of the audio files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matched_files : list\n",
    "        List of matched audio file (names).\n",
    "    matched_indices : list\n",
    "        List of matching indices in `audio_files`.\n",
    "        \n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## FIX FOR ABOVE FUNCTION FOR TASK 4 and 5 ########\n",
    "\n",
    "def find_audio_files(ann_files, audio_files, ann_suffix=None, audio_suffix='.wav'):\n",
    "    matched_files = []\n",
    "    matched_indices = []\n",
    "    for i, ann_file in enumerate(ann_files):\n",
    "        if ann_suffix is None:\n",
    "            ann_suffix = os.path.splitext(ann_file)[1]\n",
    "        matches = match_file(ann_file, audio_files,\n",
    "                             ann_suffix, audio_suffix)\n",
    "        if len(matches) == 1:\n",
    "            matched_files.append(matches[0])\n",
    "            matched_indices.append(audio_files.index(matches[0]))\n",
    "        else:\n",
    "            continue\n",
    "    return matched_files, matched_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70517371fab5a4d78a116e26fe8fdbeb",
     "grade": false,
     "grade_id": "cell-ac69eefc609c51a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Audio pre-processing\n",
    "---\n",
    "\n",
    "## Task 1: audio pre-processing (10 points)\n",
    "\n",
    "\n",
    "Step 1: read in the audio signal (all audio files: `.wav` format, 44.1kHz, 16bit, mono)\n",
    "\n",
    "Step 2: split signal into overlapping frames of length 2048 samples and a frame rate of 100 fps\n",
    "\n",
    "Step 3: for each frame compute the STFT\n",
    "\n",
    "Step 4: discard phase information and keep only the magnitudes\n",
    "  \n",
    "Step 5: filter the magnitudes with a Mel filterbank (40 bands)\n",
    "\n",
    "Step 6: apply logarithmic scaling (adding a constant for numerical stability)\n",
    "\n",
    "You are allowed to use the functionality of any audio framework to load the audio files and compute the discrete Fourier transform.\n",
    "However, all remaining steps should be coded by yourself and recognisable as such.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae3f187761c2b543889cee95bf7f95c9",
     "grade": true,
     "grade_id": "cell-baac2f7921c5fca7",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "SR = 44100 # samping rate\n",
    "FRAME_SIZE = 2048 # number of samples per frame\n",
    "HOP_SIZE = int(SR / FPS) # hop size depends on sampling rate and frame rate\n",
    "NUM_BANDS = 40 # number of mel bins\n",
    "\n",
    "def pre_process(filename, frame_size=2048, frame_rate=FPS, num_bands=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Pre-process the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        File to be processed.\n",
    "    frame_size : int\n",
    "        Size of the frames.\n",
    "    frame_rate : float\n",
    "        Frame rate used for the STFT.\n",
    "    num_bands : int\n",
    "        Number of frequency bands for the Mel filterbank.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram.\n",
    "\n",
    "    \"\"\"\n",
    "    # STEP 1: read in audio\n",
    "    signal, sr = librosa.load(filename, sr=SR) # read file\n",
    "    \n",
    "    # STEP 2,3: compute stft (default windowing function is Hann)\n",
    "    stft = librosa.core.stft(y=signal, n_fft=frame_size, hop_length=HOP_SIZE)\n",
    "    \n",
    "    # STEP 4: discard phase info and square magnitudes\n",
    "    initial_spectrogram = abs(stft)**2\n",
    "    \n",
    "    # STEP 5: apply mel scaling\n",
    "    mel_bins = librosa.filters.mel(sr=SR, n_fft=frame_size, n_mels=num_bands)\n",
    "    mel_spectrogram = mel_bins.dot(initial_spectrogram)\n",
    "    \n",
    "    # STEP 6: apply DB scaling\n",
    "    db_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "            \n",
    "    spectrogram = db_mel_spectrogram\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6af43988e6d8ae3e9f12032eff1a760d",
     "grade": false,
     "grade_id": "cell-4026e42943e1421a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Pre-compute the spectrograms for all audio files with onset annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a90c65fedb87d0ea0add8ef6bf6ccb78",
     "grade": false,
     "grade_id": "cell-31e05812abccfa3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# list for collecting pre-processed spectrograms\n",
    "# Note: it is not necessary to use this list but recommended in order to\n",
    "#       avoid recomputation of the same features over and over again.\n",
    "#       *_AUDIO_IDX canbe used to acces the precomputed spectrograms by\n",
    "#       index.\n",
    "SPECTROGRAMS = []\n",
    "\n",
    "for audio_file in AUDIO_FILES:\n",
    "    spec = pre_process(audio_file)\n",
    "    SPECTROGRAMS.append(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a5620ace84ce3aef350a6d98c9279c4",
     "grade": false,
     "grade_id": "cell-d7dbd7ad573386a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Onset detection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7634723a3c9dbf130ea8b395e5eef0c",
     "grade": false,
     "grade_id": "cell-9172f0d069f42513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "ONSET_ANNOTATION_FILES = search_files('data/train', '.onsets')\n",
    "ONSET_AUDIO_FILES, ONSET_AUDIO_IDX = find_audio_files(ONSET_ANNOTATION_FILES, AUDIO_FILES)\n",
    "ONSET_AUDIO = [SPECTROGRAMS[i] for i in ONSET_AUDIO_IDX]\n",
    "ONSET_ANNOTATIONS = [madmom.io.load_onsets(f) for f in ONSET_ANNOTATION_FILES]\n",
    "\n",
    "assert len(ONSET_ANNOTATION_FILES) == 321\n",
    "assert len(ONSET_AUDIO_FILES) == 321\n",
    "assert len(ONSET_AUDIO) == 321\n",
    "assert len(ONSET_ANNOTATIONS) == 321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c2053db2f44e55e1d0c3519e25b8138",
     "grade": false,
     "grade_id": "cell-a840486c57356ff9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: signal processing-based onset detection (20 + 5 points)\n",
    "\n",
    "For onset detection, the spectral flux should be used.\n",
    "\n",
    "### Task 2a: define onset detection function (5 points)\n",
    "\n",
    "Step 1: compute the temporal difference  \n",
    "\n",
    "Step 2: keep only the positive differences\n",
    "\n",
    "Step 3: sum or average these differences, to obtain the onset detection function (ODF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d2996c1dc4cbd4088f0c9b0c384181",
     "grade": true,
     "grade_id": "cell-7f5102c2d244b849",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def onset_detection_function(spectrogram):\n",
    "    \"\"\"\n",
    "    Compute an onset detection function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spectrogram : numpy array\n",
    "        Spectrogram\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "\n",
    "    \"\"\"\n",
    "    spectrogram_T = spectrogram.transpose()\n",
    "    \n",
    "    odf = []\n",
    "    for i, frame in enumerate(spectrogram_T):\n",
    "        sum = 0\n",
    "        for j, bin in enumerate(frame):\n",
    "            diff = spectrogram_T[i][j] - (spectrogram_T[i-1][j] if i > 0 else 0)\n",
    "            flux = diff if diff >= 0 else 0\n",
    "            sum = sum + flux\n",
    "\n",
    "        odf.append(sum / NUM_BANDS)\n",
    "           \n",
    "    return odf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ad8e92bd54d86ee9b2735f41ed1bcf7",
     "grade": false,
     "grade_id": "cell-bb2efc1980355db5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2b: detect onsets from onset detection function (6 points)\n",
    "\n",
    "To detect the onsets in the ODF, the following procedure should be applied:\n",
    "\n",
    "Step 1: (optional) subtract a moving average from the ODF\n",
    "\n",
    "Step 2: discard all ODF values below a certain threshold \n",
    "\n",
    "Step 3: select local maxima as onset positions\n",
    "\n",
    "Step 4: (optional) discard onsets too close together (recommended value: within 30ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a80af3a2b17d6e11ac64d8027c81e883",
     "grade": true,
     "grade_id": "cell-31f355f1a67333ec",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEFT = 2 # left side widnow size for local maximum\n",
    "MAX_RIGHT = 3 # right side widnow size for local maximum\n",
    "AVG_LEFT = 10 # left side widnow size for moving average\n",
    "AVG_RIGHT = 11 # right side widnow size for moving average\n",
    "MIN_DIST = 3 # (30ms) minimum distance\n",
    "            # 0.5 threshold is used\n",
    "\n",
    "def detect_onsets(odf, threshold, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the onsets in the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    threshold : float\n",
    "        Threshold for peak picking\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    onsets : numpy array\n",
    "        Detected onsets (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "            \n",
    "    new_odf = []\n",
    "    \n",
    "    ######## MOVING AVERAGE AND THRESHOLD ########\n",
    "    \n",
    "    for i in range(0, len(odf)):\n",
    "        l = i - AVG_LEFT if i - AVG_LEFT > 0 else 0\n",
    "        r = i + AVG_RIGHT if i + AVG_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        new_val = odf[i] - np.average(odf[l:r])\n",
    "        new_odf.append(new_val if new_val >= threshold else 0)\n",
    "    \n",
    "    ######## LOCAL MAXIMUM ########\n",
    "    \n",
    "    for i in range(0, len(new_odf)):\n",
    "        l = i - MAX_LEFT if i - MAX_LEFT > 0 else 0\n",
    "        r = i + MAX_RIGHT if i + MAX_RIGHT < len(odf) else len(odf)\n",
    "        \n",
    "        if new_odf[i] < max(new_odf[l:r]):\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## MINIMUM DISTANCE ########\n",
    "    \n",
    "    last = -1\n",
    "    for i in range(0, len(new_odf)):\n",
    "        if new_odf[i] > 0 and (last < 0 or i - last > MIN_DIST):\n",
    "            last = i\n",
    "        else:\n",
    "            new_odf[i] = 0\n",
    "    \n",
    "    ######## SELECTING ONSETS ########\n",
    "\n",
    "    onsets = np.array([])\n",
    "    \n",
    "    for i, el in enumerate(new_odf):\n",
    "        if new_odf[i] > 0:\n",
    "            onsets = np.append(onsets, i)\n",
    "        \n",
    "    return onsets / frame_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca7d20b1f638cc5238804b599618be3c",
     "grade": false,
     "grade_id": "cell-024a2ff7e6faf132",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2c: predict onsets on dataset (4 points)\n",
    "\n",
    "Run the complete onset detection pipeline on all audio files of the dataset.\n",
    "\n",
    "Step 1: Pre-process the audio.\n",
    "\n",
    "Step 2: Compute the ODF.\n",
    "\n",
    "Step 3: Detect the onsets. Set the threshold such that F-measure gets maximises on the dataset (see also task 2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "987836b36572d008c09613f50e35d959",
     "grade": true,
     "grade_id": "cell-6a842bf537e31a7b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define additional constants\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# list for collecting the onset detections\n",
    "onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    odf = onset_detection_function(spec)\n",
    "    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "    onset_detections.append(onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41714aa3c0228d8cefcccf00a3221846",
     "grade": false,
     "grade_id": "cell-d928050c49da1ea3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2d: evaluate detected onsets against the ground truth (5 points)\n",
    "\n",
    "Evaluate onset detection performance with `precision`, `recall`, and `fmeasure`.\n",
    "Either use the `madmom.evaluate.onsets` module or the `mir_eval` package.\n",
    "Compute the average over all files with corresponding onset annotations.\n",
    "As an evaluation window, ±25ms should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f98ef0b66bfca564cd4e98eb607a89c",
     "grade": true,
     "grade_id": "cell-3b89a4e462da935c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal processing-based onset detection\n",
      "Precision: 0.796\n",
      "Recall:    0.754\n",
      "F-measure: 0.758\n"
     ]
    }
   ],
   "source": [
    "def evaluate_onsets(onsets, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected onsets against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    onsets : list\n",
    "        List with onset detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        Averaged precision.\n",
    "    recall : float\n",
    "        Averaged recall.\n",
    "    fmeasure : float\n",
    "        Averaged f-measure.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_fmeasure = 0\n",
    "    for i in range(0, len(onsets)):\n",
    "        tp, fp, tn, fn, errors = madmom.evaluation.onsets.onset_evaluation(onsets[i], annotations[i], window=0.025)\n",
    "        p = len(tp) / (len(tp) + len(fp)) if len(tp) > 0 else 0\n",
    "        r = len(tp) / (len(tp) + len(fn)) if len(tp) > 0 else 0\n",
    "        f = 2*p*r / (p + r) if p + r > 0 else 0\n",
    "        sum_precision = sum_precision + p\n",
    "        sum_recall = sum_recall + r\n",
    "        sum_fmeasure = sum_fmeasure + f\n",
    "    \n",
    "    precision = sum_precision / len(onsets)\n",
    "    recall = sum_recall / len(onsets)\n",
    "    fmeasure = sum_fmeasure / len(onsets)\n",
    "    return precision, recall, fmeasure\n",
    "    \n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6462bc870bd6738c81e414bd2c0b73d1",
     "grade": false,
     "grade_id": "cell-778fba3a7f0e31d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2e: (optional) optimise parameters (5 points)\n",
    "\n",
    "Optimise the parameters of task 1 and 2 to get the best performance on the dataset.\n",
    "\n",
    "Parameters to be optimised: frame size (e.g. 1024, 2048, 4096), number of filter bands\n",
    "(e.g. 20, 40, 80), different logarithmic scaling parameters (e.g. natural logarithm or\n",
    "base 10; adding a constant) and the detection threshold.\n",
    "Replace the default arguments/values in the functions with the optimised parameters.\n",
    "\n",
    "The values in parentheses are suggested variations, experiment as you like.\n",
    "Please be aware that parameters may very likely have mutual influences.\n",
    "A coarse optimisation is enough. The main goal of this step is to understand \n",
    "the impact of these variations rather than getting another 0.01% performance.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae833a92362dfe0168cc2f6a76a2d473",
     "grade": true,
     "grade_id": "cell-108c552fe3533fa0",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_parameters(verbose=False):\n",
    "    frame_sizes = [1024, 2048, 4096]\n",
    "    num_bands = [20, 40, 80]\n",
    "    thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    best_fmeasure = 0\n",
    "    best_frame_size = 0\n",
    "    best_num_bands = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for i in range(0, len(frame_sizes)):\n",
    "        for j in range(0, len(num_bands)):\n",
    "            for k in range(0, len(thresholds)):\n",
    "                FRAME_SIZE = frame_sizes[i]\n",
    "                NUM_BANDS = num_bands[j]\n",
    "                THRESHOLD = thresholds[k]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"parameters:\", FRAME_SIZE, NUM_BANDS, THRESHOLD)\n",
    "                \n",
    "                # spectrograms\n",
    "                specs = []\n",
    "                for audio_file in AUDIO_FILES:\n",
    "                    spec = pre_process(audio_file, FRAME_SIZE, FPS, NUM_BANDS)\n",
    "                    specs.append(spec)\n",
    "\n",
    "                onset_audio = [specs[i] for i in ONSET_AUDIO_IDX]\n",
    "                \n",
    "                # onset detections\n",
    "                ods = []\n",
    "                for l, spec in enumerate(onset_audio):\n",
    "                    odf = onset_detection_function(spec)\n",
    "                    onsets = detect_onsets(odf, THRESHOLD, FPS)\n",
    "                    ods.append(onsets)\n",
    "                \n",
    "                # evaluation\n",
    "                precision, recall, fmeasure = evaluate_onsets(ods, ONSET_ANNOTATIONS)\n",
    "                if verbose:\n",
    "                    print('Signal processing-based onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (precision, recall, fmeasure))\n",
    "                    print('')\n",
    "                \n",
    "                if fmeasure > best_fmeasure:\n",
    "                    best_fmeasure = fmeasure\n",
    "                    best_frame_size = FRAME_SIZE\n",
    "                    best_num_bands = NUM_BANDS\n",
    "                    best_threshold = THRESHOLD\n",
    "                    \n",
    "    return best_fmeasure, best_frame_size, best_num_bands, best_threshold\n",
    "\n",
    "# uncomment and run block to optimize parameters\n",
    "# best_fmeasure, best_frame_size, best_num_bands, best_threshold = optimize_parameters(verbose=True)\n",
    "# print(\"best found parameters are:\", best_frame_size, best_num_bands, best_threshold, \"with F-measure:\", best_fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccf23dfe4c11cc0fb7e9472888072092",
     "grade": false,
     "grade_id": "cell-d13c4fff6b6567ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Put your observations/findings about task 2e in textual form below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "701bf98dfde305ecdb0a0e402fb84346",
     "grade": true,
     "grade_id": "cell-9d7b0b663d65b505",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Parameter optimization was run on the following parameters: <br>\n",
    "frame size (1024, 2048 and 4096), <br>\n",
    "number of mel bins (20, 40 and 80) and <br>\n",
    "threshold (in range from 0 to 1.0 (or 1.5 in some cases) with step size 0.1).\n",
    "<br><br>\n",
    "An example of a well performing combination: 2048 40 0.5 with precision: 79.6%\n",
    "recall: 75.4%, F-measure: 75.8%\n",
    "<br><br>\n",
    "The results are uploaded to the root directory in 3 separate files grouped for convenience by the frame size parameter: \"1024 param config.txt\", \"2048 param config.txt\" and \"4096 param config.txt\"\n",
    "<br><br>\n",
    "The first and most obvious observation in all cases is the influence of the threshold parameter on precision and recall values, starting with a low threshold value (high recall) and moving upwards (high precision) we can see how hitting a sweet spot with the threshold somewhere in the middle is necessary for a good F-measure value.\n",
    "<br><br>\n",
    "Furthermore we can see that selecting 20 as the number of mel bins almost universally yields slightly worse results regarldess of other parameters (within reasonable bounds) than the other 2 values. 20 seems to be too few bins, while 40 and 80 perform more or less similarly. <br>\n",
    "That being said we still achieved an F-measure of 74.5% with parameters 2048 20 0.3, while our overall best achieved F-measure was at 75.9%, so probably this difference is negligible\n",
    "<br><br>\n",
    "Most interestingly though one can see how picking 4096 as frame size results in significantly worse F-measure values, in best cases barely hitting the 65% mark, while 1024 and 2048 are consistently above 70%, often reaching the maximum of 75.9% with proper threshold and bin number parameters. <br> This can be attributed to the fact that by selecting a larger frame size one loses some of the temporal accuracy that is essential for onset detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c34555602407647abac535c866f75043",
     "grade": false,
     "grade_id": "cell-3159baf6dc708776",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Task 3: machine learning-based onset detection (20 points)\n",
    "\n",
    "A simple machine learning approach should be investigated. The question to \n",
    "be answered is: can a simple neural network improve the onset detection\n",
    "performance compared to the standard spectral flux approach above?\n",
    "\n",
    "In order to answer this question, the hand-crafted ODF computation should be\n",
    "replaced by a multiplayer perceptron (MLP).\n",
    "\n",
    "### Task 3a: define a trainig function (10 points)\n",
    "\n",
    "Step 1: Use `sklearn` to create an `MLPRegressor` with given parameters.\n",
    "\n",
    "Step 2: Use the same features as in the audio pre-processing section (task 1)\n",
    "        as inputs (or if task 2e was done: use the optimised parameters).\n",
    "\n",
    "Step 3: As targets, use the annotated onset positions of the dataset and\n",
    "        assign each target frame a value of 1.\n",
    "\n",
    "Step 4: Concatenate all audio frames and target frames to be used for training.\n",
    "\n",
    "Step 5: Fit the model with the given data.\n",
    "\n",
    "Step 6: Save the model to the given file name. Use Python's `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1b68f9c353d9d30f9f2885a4c0bda24",
     "grade": true,
     "grade_id": "cell-6bea20b2d14c34e7",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(audio, annotations, diffs=False, early_stopping=False,\n",
    "          verbose=True, model='model.pkl', **kwargs):\n",
    "    \"\"\"\n",
    "    Train an MLP on the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : list\n",
    "        List of audio files or precomputed spectrograms.\n",
    "    annotations : list of numpy arrays\n",
    "        List with corresponding onset annotations.\n",
    "    diffs : bool, optional\n",
    "        Include diffs as input features (step 7).\n",
    "    early_stopping : bool, optional\n",
    "        Use early stopping to prevent overfitting (step 8).\n",
    "    verbose : bool, optional\n",
    "        Be verbose during training.\n",
    "    model : str, optional\n",
    "        Save the fitted model to given file name.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mlp : MLPRegressor\n",
    "        Trained MLP.\n",
    "\n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    # define MLP\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(50, 50), tol=1e-4, max_iter=100,\n",
    "                       early_stopping=early_stopping, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(mlp)\n",
    "        \n",
    "    # prepare input features and targets\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    ######## INPUT PREPARATION ########\n",
    "    \n",
    "    # concatenate all features and transpose to fit the MLP input format\n",
    "    spectral_features = np.concatenate((audio), axis=1)\n",
    "    spectral_features_T = spectral_features.transpose()\n",
    "    x = spectral_features_T\n",
    "    \n",
    "    # add spectral flux to features\n",
    "    if diffs:\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('adding spectral flux to input features...')\n",
    "            print('')\n",
    "        flux = kwargs['flux']\n",
    "        flux = np.concatenate((flux))\n",
    "        flux = np.vstack(flux)\n",
    "        x = np.concatenate((x, flux), axis=1)\n",
    "    \n",
    "    # create target as 0 array with value 1 where index matches the frame \n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(0, len(audio)):\n",
    "        spec_T = audio[i].transpose()\n",
    "        target = np.zeros(len(spec_T))\n",
    "        \n",
    "        onset_frames = np.rint(annotations[i] * FPS)\n",
    "        for j in range(0, len(onset_frames)):\n",
    "            target[int(onset_frames[j])] = 1\n",
    "\n",
    "        y = np.append(y, target)\n",
    "        \n",
    "    ###################################\n",
    "    \n",
    "    # reshape x and y\n",
    "    # Note: depending on your data pre-processing these lines might\n",
    "    #       need to be adjusted accordingly\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    \n",
    "    # train model\n",
    "    if verbose:\n",
    "        print('training model:', model)\n",
    "    mlp.fit(x.squeeze(), y.squeeze())\n",
    "    \n",
    "    # save model and return it\n",
    "    with open(model, 'wb') as f:\n",
    "        pickle.dump(mlp, f)\n",
    "    return mlp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb388fece7fdad01f786c07cf9da8294",
     "grade": false,
     "grade_id": "cell-e552a370d1c46014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3b: train the model (2 points)\n",
    "\n",
    "Train the model on the dataset and save as `model.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7742fc3bf6136e4f2b740cc034dfd9a",
     "grade": true,
     "grade_id": "cell-3df2febe76c7a610",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(50, 50), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "training model: model.pkl\n",
      "Iteration 1, loss = 1.25240510\n",
      "Iteration 2, loss = 0.02946456\n",
      "Iteration 3, loss = 0.02688286\n",
      "Iteration 4, loss = 0.02408670\n",
      "Iteration 5, loss = 0.02256656\n",
      "Iteration 6, loss = 0.02181341\n",
      "Iteration 7, loss = 0.02148313\n",
      "Iteration 8, loss = 0.02131591\n",
      "Iteration 9, loss = 0.02125165\n",
      "Iteration 10, loss = 0.02124676\n",
      "Iteration 11, loss = 0.02119086\n",
      "Iteration 12, loss = 0.02119527\n",
      "Iteration 13, loss = 0.02115439\n",
      "Iteration 14, loss = 0.02113956\n",
      "Iteration 15, loss = 0.02111525\n",
      "Iteration 16, loss = 0.02109830\n",
      "Iteration 17, loss = 0.02108808\n",
      "Iteration 18, loss = 0.02106825\n",
      "Iteration 19, loss = 0.02106446\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "MLP_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, False, False, model='model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "489c057bbfbeca12a42650a33826b05d",
     "grade": false,
     "grade_id": "cell-c4fe2a13d12097ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3c: evaluate performance on the dataset (3 points)\n",
    "\n",
    "Step 1: Predict onset activations for the dataset.\n",
    "\n",
    "Step 2: Adjust the threshold parameter to yield the best F-measure on the dataset\n",
    "        (use the `detect_onsets()` function defined in task 2b).\n",
    "\n",
    "Step 3: Evaluate performance on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_THRESHOLD = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current threshold: 0.0\n",
      "MLP onset detection\n",
      "Precision: 0.387\n",
      "Recall:    0.683\n",
      "F-measure: 0.454\n",
      "\n",
      "Current threshold: 0.0005\n",
      "MLP onset detection\n",
      "Precision: 0.455\n",
      "Recall:    0.644\n",
      "F-measure: 0.507\n",
      "\n",
      "Current threshold: 0.001\n",
      "MLP onset detection\n",
      "Precision: 0.531\n",
      "Recall:    0.588\n",
      "F-measure: 0.541\n",
      "\n",
      "Current threshold: 0.0015\n",
      "MLP onset detection\n",
      "Precision: 0.586\n",
      "Recall:    0.538\n",
      "F-measure: 0.540\n",
      "\n",
      "Current threshold: 0.002\n",
      "MLP onset detection\n",
      "Precision: 0.613\n",
      "Recall:    0.494\n",
      "F-measure: 0.519\n",
      "\n",
      "Current threshold: 0.0025\n",
      "MLP onset detection\n",
      "Precision: 0.635\n",
      "Recall:    0.458\n",
      "F-measure: 0.495\n",
      "\n",
      "Current threshold: 0.003\n",
      "MLP onset detection\n",
      "Precision: 0.642\n",
      "Recall:    0.430\n",
      "F-measure: 0.472\n",
      "\n",
      "Current threshold: 0.0035\n",
      "MLP onset detection\n",
      "Precision: 0.646\n",
      "Recall:    0.407\n",
      "F-measure: 0.452\n",
      "\n",
      "Current threshold: 0.004\n",
      "MLP onset detection\n",
      "Precision: 0.629\n",
      "Recall:    0.385\n",
      "F-measure: 0.431\n",
      "\n",
      "Current threshold: 0.0045000000000000005\n",
      "MLP onset detection\n",
      "Precision: 0.618\n",
      "Recall:    0.368\n",
      "F-measure: 0.414\n",
      "\n",
      "Optimized threshold is: 0.001 with F measure: 0.5406673342754031\n"
     ]
    }
   ],
   "source": [
    "# Function for optimizing the threshold parameter\n",
    "def optimize_mlp_threshold(model, thresholds=[], diffs=False, verbose=True, **kwargs):\n",
    "    best_threshold = 0\n",
    "    best_fmeasure = 0\n",
    "    \n",
    "    if diffs and verbose:\n",
    "        print('running optimization with spectral flux...')\n",
    "        print('')\n",
    "\n",
    "    for i in range(0, len(thresholds)):\n",
    "        ods_opt = []\n",
    "        for j, spec in enumerate(ONSET_AUDIO):\n",
    "            \n",
    "            x = spec.transpose()\n",
    "            \n",
    "            # add spectral flux to features\n",
    "            if diffs:\n",
    "                flux = kwargs['flux']\n",
    "                flux = np.vstack(flux[j])\n",
    "                x = np.concatenate((x, flux), axis=1)\n",
    "            \n",
    "            mlp_odf = model.predict(x)\n",
    "            mlp_onsets = detect_onsets(mlp_odf, thresholds[i], FPS)\n",
    "            ods_opt.append(mlp_onsets)\n",
    "        \n",
    "        p, r, f = evaluate_onsets(ods_opt, ONSET_ANNOTATIONS)\n",
    "        if verbose:\n",
    "            print('Current threshold:', thresholds[i])\n",
    "            print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))\n",
    "            print('')\n",
    "        \n",
    "        if f > best_fmeasure:\n",
    "            best_fmeasure = f\n",
    "            best_threshold = thresholds[i]\n",
    "        \n",
    "    if verbose:\n",
    "        print('Optimized threshold is:', best_threshold, 'with F measure:', best_fmeasure)\n",
    "    return best_threshold\n",
    "\n",
    "mlp_thresholds = np.arange(0,0.005,0.0005)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_THRESHOLD = optimize_mlp_threshold(model=MLP_MODEL, thresholds=mlp_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18fc24ad035429b62f64b038fd5b4c50",
     "grade": true,
     "grade_id": "cell-7ba1da598d0a27c6",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP onset detection\n",
      "Precision: 0.531\n",
      "Recall:    0.588\n",
      "F-measure: 0.541\n"
     ]
    }
   ],
   "source": [
    "mlp_onset_detections = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    mlp_odf = MLP_MODEL.predict(spec.transpose())\n",
    "    mlp_onsets = detect_onsets(mlp_odf, MLP_THRESHOLD, FPS)\n",
    "    mlp_onset_detections.append(mlp_onsets)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_onset_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24b9ce18bfd5f6a5cba8f55beff26eeb",
     "grade": false,
     "grade_id": "cell-72c833717e7fc443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3d: describe your findings (5 points)\n",
    "\n",
    "Describe your findings/observations in textual form below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "121a8d757cb143d081a7a45233b1b4f6",
     "grade": true,
     "grade_id": "cell-df0e597cb0b8ffdf",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The MLP Regressor performed rather poorly compared to the hand-crafted method. Its performance averaged around 53% (across multiple trainings) compared to the approximate 75% of the hand-crafted one.\n",
    "\n",
    "It seems like having only the spectrogram as input features for the MLP is not enough for it to be able to generate a reasonable ODF, most likely because the features are treated in isolation and are viewed as a set by the MLP and the temporal structure of the spectrogram is not taken into account (i.e. differences in energy between consecutive frames).\n",
    "\n",
    "The reason the MLP did not fail completely could be possibly attributed to the fact that even if viewed in isolation a spectral feature vector still carries some information that is relevant to onset detection, for example high frequency content that is usually present at onsets. This could have helped the MLP learn a somewhat functioning ODF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cedab89905aa3339278acc9e880a1756",
     "grade": false,
     "grade_id": "cell-4c93d30b03f83b14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3e: add temporal differences as additional features (2 points)\n",
    "\n",
    "Train a new model with first order temporal differences (as in spectral flux) as\n",
    "aditional features (stacked to the magnitudes) and save as `model_diff.pkl`.\n",
    "\n",
    "Note: modify the `train()` function of task 3a to be able to be called with `diffs=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate flux\n",
    "\n",
    "FLUX = []\n",
    "for i in range(0, len(ONSET_AUDIO)):\n",
    "    diff = onset_detection_function(ONSET_AUDIO[i])\n",
    "    FLUX.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bd745cba8b976b3c48b79f3ee9f883d",
     "grade": true,
     "grade_id": "cell-71baed20fc3c66d3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(50, 50), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "\n",
      "adding spectral flux to input features...\n",
      "\n",
      "training model: model_diff.pkl\n",
      "Iteration 1, loss = 0.80333189\n",
      "Iteration 2, loss = 0.02634326\n",
      "Iteration 3, loss = 0.02376980\n",
      "Iteration 4, loss = 0.02132270\n",
      "Iteration 5, loss = 0.01976381\n",
      "Iteration 6, loss = 0.01900296\n",
      "Iteration 7, loss = 0.01833693\n",
      "Iteration 8, loss = 0.01779528\n",
      "Iteration 9, loss = 0.01742194\n",
      "Iteration 10, loss = 0.01722965\n",
      "Iteration 11, loss = 0.01713869\n",
      "Iteration 12, loss = 0.01701473\n",
      "Iteration 13, loss = 0.01692768\n",
      "Iteration 14, loss = 0.01679406\n",
      "Iteration 15, loss = 0.01675342\n",
      "Iteration 16, loss = 0.01671032\n",
      "Iteration 17, loss = 0.01665241\n",
      "Iteration 18, loss = 0.01660277\n",
      "Iteration 19, loss = 0.01656642\n",
      "Iteration 20, loss = 0.01655342\n",
      "Iteration 21, loss = 0.01651611\n",
      "Iteration 22, loss = 0.01651152\n",
      "Iteration 23, loss = 0.01649539\n",
      "Iteration 24, loss = 0.01649190\n",
      "Iteration 25, loss = 0.01647249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "MLP_DIFF_MODEL = train(ONSET_AUDIO, ONSET_ANNOTATIONS, True, False, model='model_diff.pkl', flux=FLUX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09448aa96220667844569a18e718ffa4",
     "grade": false,
     "grade_id": "cell-52a046460b699bd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3f: evaluate model (3 points)\n",
    "\n",
    "Compare the performance of this model with the one of task task 2 and task 3b.\n",
    "Again, use a suitable threshold which to maximises the performance on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A solid arbitrary starting value for the threshold\n",
    "MLP_DIFF_THRESHOLD = 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running optimization with spectral flux...\n",
      "\n",
      "Current threshold: 0.03\n",
      "MLP onset detection\n",
      "Precision: 0.667\n",
      "Recall:    0.812\n",
      "F-measure: 0.708\n",
      "\n",
      "Current threshold: 0.04\n",
      "MLP onset detection\n",
      "Precision: 0.700\n",
      "Recall:    0.799\n",
      "F-measure: 0.725\n",
      "\n",
      "Current threshold: 0.05\n",
      "MLP onset detection\n",
      "Precision: 0.729\n",
      "Recall:    0.786\n",
      "F-measure: 0.738\n",
      "\n",
      "Current threshold: 0.060000000000000005\n",
      "MLP onset detection\n",
      "Precision: 0.755\n",
      "Recall:    0.772\n",
      "F-measure: 0.748\n",
      "\n",
      "Current threshold: 0.07\n",
      "MLP onset detection\n",
      "Precision: 0.777\n",
      "Recall:    0.759\n",
      "F-measure: 0.752\n",
      "\n",
      "Current threshold: 0.08000000000000002\n",
      "MLP onset detection\n",
      "Precision: 0.793\n",
      "Recall:    0.744\n",
      "F-measure: 0.753\n",
      "\n",
      "Current threshold: 0.09000000000000001\n",
      "MLP onset detection\n",
      "Precision: 0.808\n",
      "Recall:    0.731\n",
      "F-measure: 0.751\n",
      "\n",
      "Current threshold: 0.1\n",
      "MLP onset detection\n",
      "Precision: 0.820\n",
      "Recall:    0.714\n",
      "F-measure: 0.747\n",
      "\n",
      "Optimized threshold is: 0.08000000000000002 with F measure: 0.7526878542099927\n"
     ]
    }
   ],
   "source": [
    "mlp_diff_thresholds = np.arange(0.03,0.11,0.01)  # thresholds to use for optimization\n",
    "\n",
    "# COMMENT OUT LINE BELOW TO AVOID RUNNING THRESHOLD OPTIMIZATION (might take up to a minute or two)\n",
    "MLP_DIFF_THRESHOLD = optimize_mlp_threshold(model=MLP_DIFF_MODEL, thresholds=mlp_diff_thresholds, diffs=True, flux=FLUX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f0bff20d75262aab5d0f88211bbd7eb",
     "grade": true,
     "grade_id": "cell-c66728fdf5b45653",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP onset detection with temporal diffs\n",
      "Precision: 0.793\n",
      "Recall:    0.744\n",
      "F-measure: 0.753\n"
     ]
    }
   ],
   "source": [
    "mlp_diff_detections = []\n",
    "\n",
    "# for task 4\n",
    "MLP_DIFF_ODFS = []\n",
    "\n",
    "for i, spec in enumerate(ONSET_AUDIO):\n",
    "    flux = np.vstack(FLUX[i])\n",
    "    x = np.concatenate((spec.transpose(), flux), axis=1)\n",
    "\n",
    "    mlp_diff_odf = MLP_DIFF_MODEL.predict(x)\n",
    "    mlp_diff_onsets = detect_onsets(mlp_diff_odf, MLP_DIFF_THRESHOLD, FPS)\n",
    "    mlp_diff_detections.append(mlp_diff_onsets)\n",
    "    \n",
    "    MLP_DIFF_ODFS.append(mlp_diff_odf)\n",
    "\n",
    "# evaluate against ground truth\n",
    "p, r, f = evaluate_onsets(mlp_diff_detections, ONSET_ANNOTATIONS)\n",
    "\n",
    "print('MLP onset detection with temporal diffs\\nPrecision: %.3f\\nRecall:    %.3f\\nF-measure: %.3f' % (p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ad4b06e575763ea20da82b6ca17c310",
     "grade": false,
     "grade_id": "cell-753a89f648a5b1d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3g: describe your findings (5 points)\n",
    "\n",
    "Describe your findings/observations in textual form below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a82a390665a3fbbdc0ce1cb97cffd089",
     "grade": true,
     "grade_id": "cell-a7c6934cc2700a89",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The MLP Regressor that included the spectral flux as input additionally to the spectrogram performed much better than the one using only the spectrogram. Having the difference in energy from previous frame to the current one as input proved to be essential for learning an ODF.\n",
    "\n",
    "The Diff MLP model achieved 76.5% F-measure score, compared to the MLP models 53% and hand-crafted methods 75%.\n",
    "\n",
    "While the 1.5% increase in performance doesnt seem like a tremendously big one, we need to keep in mind that the regressor used for the task was a fairly generic one, so a network that is more configured to this particular task would probably achieve better results\n",
    "\n",
    "In conclusion we can say that using a neural network in combination with some smartly prepared input data is probably the most optimal way to approach onset detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07a3ec78dc4e30994067da6b9c0742e3",
     "grade": false,
     "grade_id": "cell-55c4396d02239624",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Tempo estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fa9442a29a873fb00653ae8c462cb1b",
     "grade": false,
     "grade_id": "cell-d1e433eb845afcda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "TEMPO_ANNOTATION_FILES = search_files('data/train', '.bpm')\n",
    "TEMPO_AUDIO_FILES, TEMPO_AUDIO_IDX = find_audio_files(TEMPO_ANNOTATION_FILES, AUDIO_FILES)\n",
    "TEMPO_AUDIO = [SPECTROGRAMS[i] for i in TEMPO_AUDIO_IDX]\n",
    "TEMPO_ANNOTATIONS = [madmom.io.load_tempo(f)[0, 0] for f in TEMPO_ANNOTATION_FILES]\n",
    "\n",
    "assert len(TEMPO_ANNOTATION_FILES) == 107\n",
    "assert len(TEMPO_AUDIO_FILES) == 107\n",
    "assert len(TEMPO_AUDIO) == 107\n",
    "assert len(TEMPO_ANNOTATIONS) == 107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "756441d9a73de96a46c4ad72586a9531",
     "grade": false,
     "grade_id": "cell-7780af77a978d69a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 4: detect tempo of ODF (25 + 5 points)\n",
    "\n",
    "To detect the tempo/periodicity the ODF, the following procedure should be\n",
    "applied:\n",
    "\n",
    "Step 1: Compute the auto-correlation function (ACF) of the ODF.\n",
    "\n",
    "Step 2: Select an appropriate peak of the ACF as the main periodicity.\n",
    "\n",
    "Step 3: Compute the tempo (in bpm, beats per minute).\n",
    "\n",
    "Step 4: Evaluate the mean tempo estimation performance (e.g. with `madmom.evaluation.tempo` module)\n",
    "        on the dataset. Use `Accuracy 1` (with 4% tolerance) and `Accuracy 2` (allowing 4% tolerance,\n",
    "        including double and half tempo variants) as metrics.\n",
    "\n",
    "Step 5: (optional) optimise the parameters to get the best performance on the dataset\n",
    "        parameters to be optimised: lag range for ACF computation (lower bound: 40-80bpm,\n",
    "        upper bound 140-220bpm), peak selection mechanism (e.g. clustering of peaks).\n",
    "        Replace the default arguments/values in the function definition with the optimised parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODFs computed by the MLP with diffs\n",
    "TEMPO_MLP_ODFS = [MLP_DIFF_ODFS[i] for i in TEMPO_AUDIO_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a282c541b595eafe4d8f9794a4743d84",
     "grade": true,
     "grade_id": "cell-a3b78dead9f249ad",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detect_tempo(odf, min_bpm=60, max_bpm=180, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the tempo of the onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    min_bpm : float\n",
    "        Minimum tempo, given in beats per minute (BPM).\n",
    "    max_bpm : float\n",
    "        Maximum tempo, given in beats per minute (BPM).\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tempo : float\n",
    "        Detected tempo (in BPM).\n",
    "\n",
    "    \"\"\"\n",
    "    odf_copy = odf.copy()\n",
    "    \n",
    "    ######## MEDIAN FILTER ########\n",
    "    \n",
    "    if kwargs['median_filter']:\n",
    "        new_odf = []\n",
    "        med_left = 10 # 10\n",
    "        med_right = 11 # 11\n",
    "        for i in range(0, len(odf_copy)):\n",
    "            l = i - med_left if i - med_left > 0 else 0\n",
    "            r = i + med_right if i + med_right < len(odf_copy) else len(odf_copy)\n",
    "\n",
    "            new_odf.append(odf_copy[i] if odf_copy[i] > np.median(odf_copy[l:r]) else 0)\n",
    "\n",
    "        odf_copy = new_odf\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    highest_correlation = 0\n",
    "    tempo = min_bpm\n",
    "    for i in range(min_bpm, max_bpm+1):\n",
    "        shift = round((60 * frame_rate) / i) # number of frames to shift\n",
    "        shifted_odf = odf_copy[shift:] \n",
    "        \n",
    "        sum = 0\n",
    "        for j in range(0, len(shifted_odf)):\n",
    "            sum = sum + odf_copy[j] * shifted_odf[j]\n",
    "            \n",
    "        if sum > highest_correlation:\n",
    "            highest_correlation = sum\n",
    "            tempo = i\n",
    "\n",
    "    return float(tempo)\n",
    "\n",
    "\n",
    "def evaluate_tempo(tempi, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected tempi against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tempi : list\n",
    "        List with tempo detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy_1 : float\n",
    "        Averaged accuracy 1.\n",
    "    accuracy_2 : float\n",
    "        Averaged accuracy 2.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_acc1 = 0\n",
    "    sum_acc2 = 0\n",
    "    for i in range(0, len(tempi)):\n",
    "        result = madmom.evaluation.tempo.TempoEvaluation(tempi[i], annotations[i], tolerance=0.04, double=True, triple=False, sort=False)\n",
    "        sum_acc1 = sum_acc1 + result.acc1\n",
    "        sum_acc2 = sum_acc2 + result.acc2\n",
    "        \n",
    "    return sum_acc1 / len(tempi), sum_acc2 / len(tempi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 70 to 170 bpm with median filter False\n",
      "Accuracy metric 1 (w/o double/half tempos): 0.48598130841121495 \n",
      "Accuracy metric 2 (with double/half tempos): 0.8504672897196262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### TEMPO PIPELINE ####\n",
    "# depends on the ODFs computed by the MLP with diffs\n",
    "# Optimal params: 70, 170, False\n",
    "\n",
    "def tempo_pipeline(min_bpm=60, max_bpm=180, median_filter=False):\n",
    "    tempi = []\n",
    "    for i in range(0, len(TEMPO_MLP_ODFS)):\n",
    "        tempo = detect_tempo(TEMPO_MLP_ODFS[i], min_bpm=min_bpm, max_bpm=max_bpm, frame_rate=FPS, median_filter=median_filter)\n",
    "        tempi.append(tempo)\n",
    "\n",
    "    acc_1, acc_2 = evaluate_tempo(tempi, TEMPO_ANNOTATIONS)\n",
    "    print('parameters:', min_bpm, 'to', max_bpm, 'bpm with median filter', median_filter)\n",
    "    print('Accuracy metric 1 (w/o double/half tempos):', acc_1, '\\nAccuracy metric 2 (with double/half tempos):', acc_2)\n",
    "    print('')\n",
    "    return tempi, acc_1, acc_2\n",
    "\n",
    "TEMPI, ACC_1, ACC_2 = tempo_pipeline(min_bpm=70, max_bpm=170, median_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEMPO PARAMETER OPTIMIZATION ####\n",
    "\n",
    "def tempo_parameter_optimization():\n",
    "    min_bpms = [40, 50, 60, 70, 80]\n",
    "    max_bpms = [140, 150, 160, 170, 180, 190, 200, 210, 220]\n",
    "    median_filters = [False, True]\n",
    "    \n",
    "    best_tempi = []\n",
    "    best_acc1 = 0\n",
    "    best_acc2 = 0\n",
    "    best_min_bpm = 0\n",
    "    best_max_bpm = 0\n",
    "    best_median_filter = False\n",
    "    \n",
    "    for i in range(0, len(min_bpms)):\n",
    "        for j in range(0, len(max_bpms)):\n",
    "            for k in range(0, len(median_filters)):\n",
    "                t, a1, a2 = tempo_pipeline(min_bpms[i], max_bpms[j], median_filters[k])\n",
    "                \n",
    "                if a1+a2 > best_acc1 + best_acc2:\n",
    "                    best_acc1 = a1\n",
    "                    best_acc2 = a2\n",
    "                    best_tempi = t\n",
    "                    best_min_bpm = min_bpms[i]\n",
    "                    best_max_bpm = max_bpms[j]\n",
    "                    best_median_filter = median_filters[j]\n",
    "    \n",
    "    return best_tempi, best_acc1, best_acc2, best_min_bpm, best_max_bpm, best_median_filter\n",
    "\n",
    "# UNCOMMENT LINE TO RUN PARAMETER OPTIMIZATION\n",
    "# BEST_TEMPI, BEST_ACC_1, BEST_ACC_2, BEST_MIN_BPM, BEST_MAX_BPM, BEST_MEDIAN_FILTER = tempo_parameter_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3d22e49a6d4a02c46660521a3a48620",
     "grade": false,
     "grade_id": "cell-8437e4317fcdc7c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Summarise your observations/findings in textual form below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdeed959b873e7af41d48e6d53802c41",
     "grade": true,
     "grade_id": "cell-9618baf74e2d84ba",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Tested parameters:\n",
    "min bpm range: 40 - 80 with step size 10\n",
    "max bpm range: 140 - 220 with step size 10\n",
    "with and without a median filter as a preprocessing step for the ODF\n",
    "the ODF itself was taken from the MLP Diff prediction\n",
    "\n",
    "The implemented method suffers greatly from double/half tempo errors, this is indicated by the difference between the 2 accuracy measures. Acc 1 measure varying between 40-50% and Acc 2 measure varying between 80-85% with best results achieved being 48.6% for Acc 1 and 86% for Acc 2 with min bpm 70, max bpm 170 and no median filtering (numbers may slightly vary because of the MLP Diff model retraining).\n",
    "\n",
    "Expanding the bpm range to the maximum of 40-220 did not result in performance increase even though the the minimum and maximum ground truth tempos of the audios are 41 and 208 bpm respectively, which seems counterintuitive at first but then it only makes sense that the algorithm works better when confined to a smaller interval, so a reasonable middle ground must be found when choosing the bpm interval.\n",
    "\n",
    "An optional median filter was added as a preprocessing step to the ODF, which filtered out all values below a \n",
    "moving median, the size of the median window was selected to be the same size as of the moving average window for peak picking. The use of the median filter proved to be inconsequential, it improved the results slightly in some cases and worsened them in others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a6cef29df6b559ee3c5e0da2784e281",
     "grade": false,
     "grade_id": "cell-871a9fd3dbb36fb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Beat tracking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bbe3bccd9461ff65b07fd8ac7c4c00f",
     "grade": false,
     "grade_id": "cell-f0c00d87f531e0c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you are not required to use these predefined constants, but it is recommended\n",
    "BEAT_ANNOTATION_FILES = search_files('data/train', '.beats')\n",
    "BEAT_AUDIO_FILES, BEAT_AUDIO_IDX = find_audio_files(BEAT_ANNOTATION_FILES, AUDIO_FILES)\n",
    "BEAT_AUDIO = [SPECTROGRAMS[i] for i in BEAT_AUDIO_IDX]\n",
    "BEAT_ANNOTATIONS = [madmom.io.load_beats(f) for f in BEAT_ANNOTATION_FILES]\n",
    "\n",
    "assert len(BEAT_ANNOTATION_FILES) == 177\n",
    "assert len(BEAT_AUDIO_FILES) == 177\n",
    "assert len(BEAT_AUDIO) == 177\n",
    "assert len(BEAT_ANNOTATIONS) == 177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "978b7057469673dd676dec30af33b77c",
     "grade": false,
     "grade_id": "cell-5ee8380ba148619c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 5: track the beats based on ODF and periodicity (25 + 5 points)\n",
    "\n",
    "To detect the beats in the ODF, the following procedure should be applied:\n",
    "\n",
    "Step 1: Determine the best possible offset for beat tracking given the tempo\n",
    "        or periodicity determined in task 4 and select the first beat.\n",
    "\n",
    "Step 2: Determine consecutive beats based on the tempo; allow ±10% tempo \n",
    "        deviation between consecutive beats.\n",
    "\n",
    "Step 3: Continue until all beats are tracked.\n",
    "\n",
    "Step 4: Evaluate beat tracking performance (e.g. with `madmom.evaluation.beats` module)\n",
    "        on the dataset. Use `CMLt` and `AMLt` as evaluation metrics, \n",
    "\n",
    "Step 5: (optional) optimise the parameters to get the best performance on the dataset.\n",
    "        Parameters to be optimised: allowed deviation of the tempo, length of the audio\n",
    "        used to determine the tempo.\n",
    "        Replace the default arguments/values in the functions with the optimised parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09e37b1250e8c1f5fab70b4a1b878c79",
     "grade": true,
     "grade_id": "cell-ee4f53636b5ad0f2",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cccb9e229aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def detect_beats(odf, min_bpm=60, max_bpm=180, frame_rate=FPS, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect the beats in an onset detection function (ODF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    odf : numpy array\n",
    "        Onset detection function.\n",
    "    min_bpm : float\n",
    "        Minimum tempo, given in beats per minute (BPM).\n",
    "    max_bpm : float\n",
    "        Maximum tempo, given in beats per minute (BPM).\n",
    "    frame_rate : float\n",
    "        Frame rate of the onset detection function.\n",
    "    kwargs : dict, optional\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beats : numpy array\n",
    "        Detected beats (in seconds).\n",
    "\n",
    "    \"\"\"\n",
    "    # determine tempo from within this function in order to be used\n",
    "    # with a single input (the ODF)\n",
    "    tempo = detect_tempo(odf, min_bpm, max_bpm, frame_rate)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return beats\n",
    "\n",
    "\n",
    "def evaluate_beats(beats, annotations):\n",
    "    \"\"\"\n",
    "    Evaluate detected beats against ground truth annotations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beats : list\n",
    "        List with beats detections for all files.\n",
    "    annotations : list\n",
    "        List with corresponding ground truth annotations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cmlt : float\n",
    "        Averaged CMLt.\n",
    "    amlt : float\n",
    "        Averaged AMLt.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return cmlt, amlt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f02e66207a6f9ad2af88aafa3870608",
     "grade": false,
     "grade_id": "cell-22a15cdbe9be11fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Summarise your observations/findings in textual form below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46f53a26bfdf9cb40cc038a0506c7a7b",
     "grade": true,
     "grade_id": "cell-0be4a3d11e297105",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76651f8aba0ad7edb0fee1d49dc7bf54",
     "grade": false,
     "grade_id": "cell-ec77b27d4692b9ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Task 6: (optional) visualise the results (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "029c4797021dc29471cc4f1de5713876",
     "grade": true,
     "grade_id": "cell-3e3fdd41890c2c57",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f79e85f6f4fe6aee6d248fc1a945fffa",
     "grade": false,
     "grade_id": "cell-0d090757a7d76eb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Chocolate challenge (no points, only chocolate and glory)\n",
    "\n",
    "Put all needed functions defined above in place to be able to detect the tempo and beats in the given audio files.\n",
    "\n",
    "To qualify for the chocolate challenge, please check that running the function below produces\n",
    "two (hopefully empty) detection files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6ee330002ce1c7c485079567aee3b84",
     "grade": false,
     "grade_id": "cell-623ad7e1ec01af78",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f926894420a235d32e97678588f2cbf0",
     "grade": false,
     "grade_id": "cell-6edf4f9a7e57681d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Well done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
